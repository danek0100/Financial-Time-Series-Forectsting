{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# üß† TSAI: –ú–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "\n",
        "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **TSAI** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.\n",
        "\n",
        "## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n",
        "\n",
        "- **–ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–ø–∫–∏ `/data/multivariate_series/`\n",
        "- **–ü–æ—ç—Ç–∞–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏**: —Ä–∞–∑–ª–∏—á–Ω—ã–µ —ç—Ç–∞–ø—ã –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–∏—á–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ progressive feature analysis\n",
        "- **Walk-forward –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ**: —á–µ—Å—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä—è—é—â–∏–º—Å—è –æ–∫–Ω–æ–º\n",
        "- **–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞**: 10 —Ç–æ—á–µ–∫\n",
        "- **–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–∞**: 11 —Ç–æ—á–µ–∫\n",
        "- **Deep Learning –º–æ–¥–µ–ª–∏**: InceptionTime, ResNet, ROCKET, MiniRocket, TST, PatchTST\n",
        "- **–ú–µ—Ç—Ä–∏–∫–∏**: RMSE, MAPE, DA (Directional Accuracy)\n",
        "\n",
        "## üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç—Ç–∞–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "1. **–≠—Ç–∞–ø 1**: –¢–æ–ª—å–∫–æ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è (close)\n",
        "2. **–≠—Ç–∞–ø 2**: + –ê–Ω–æ–º–∞–ª–∏–∏ (anomaly)\n",
        "3. **–≠—Ç–∞–ø 3**: + –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è (weighted_score_with_decay)\n",
        "4. **–≠—Ç–∞–ø 4**: + –ë–∞–∑–æ–≤—ã–µ OHLCV –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "5. **–≠—Ç–∞–ø 5**: + –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "6. **–≠—Ç–∞–ø 6**: + –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (TSFresh)\n",
        "\n",
        "## üß† –ú–æ–¥–µ–ª–∏ TSAI –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "- **InceptionTime**: —Å–≤–µ—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å —Å –º–æ–¥—É–ª—è–º–∏ Inception\n",
        "- **ResNet**: –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "- **ROCKET**: —Ä–∞–Ω–¥–æ–º–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —è–¥—Ä–∞ (–±—ã—Å—Ç—Ä—ã–π)\n",
        "- **MiniRocket**: —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è ROCKET\n",
        "- **TST**: Time Series Transformer\n",
        "- **PatchTST**: –ø–∞—Ç—á-–±–∞–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä\n",
        "\n",
        "## üß† –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è\n",
        "- **Deep Learning –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ**: —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π\n",
        "- **Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è**: —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å —Ä–∞—Å—à–∏—Ä—è—é—â–∏–º—Å—è –æ–∫–Ω–æ–º\n",
        "- **Sequence modeling**: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "- **–ü—Ä–æ–≥–Ω–æ–∑**: 10 —Ç–æ—á–µ–∫ –≤–ø–µ—Ä–µ–¥ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç—ã\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import time\n",
        "import os\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from datetime import datetime\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ TSAI\n",
        "try:\n",
        "    from tsai.all import *\n",
        "    from fastai.losses import MSELossFlat\n",
        "    from tsai.learner import Learner\n",
        "    from tsai.metrics import mae, rmse\n",
        "    import torch\n",
        "    print(\"‚úÖ TSAI –¥–æ—Å—Ç—É–ø–µ–Ω\")\n",
        "except ImportError:\n",
        "    print(\"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º TSAI...\")\n",
        "    import subprocess\n",
        "    import sys\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tsai\", \"torch\", \"fastai==2.7.19\", \"fastcore==1.7.29\", \"--quiet\"])\n",
        "    from tsai.all import *\n",
        "    from fastai.losses import MSELossFlat\n",
        "    from tsai.learner import Learner\n",
        "    from tsai.metrics import mae, rmse\n",
        "    import torch\n",
        "    print(\"‚úÖ TSAI —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
        "print(f\"üî• CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéØ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üíæ –ü–∞–º—è—Ç–∏ GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
        "DATA_PATH = \"../../data/multivariate_series/\"\n",
        "OUTPUT_PATH = \"results/multivariate_tsai/\"\n",
        "FORECAST_HORIZON = 10  # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –Ω–∞ 10 —Ç–æ—á–µ–∫\n",
        "TEST_SIZE = 11  # –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
        "SEQUENCE_LENGTH = 30  # –î–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è TSAI\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤ (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
        "TICKERS = ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
        "\n",
        "# –ù–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
        "STAGE_NAMES = {\n",
        "    1: \"–ë–∞–∑–æ–≤–∞—è —Ü–µ–Ω–∞ (close)\",\n",
        "    2: \"–¶–µ–Ω–∞ + –∞–Ω–æ–º–∞–ª–∏–∏\",\n",
        "    3: \"–¶–µ–Ω–∞ + –∞–Ω–æ–º–∞–ª–∏–∏ + –Ω–æ–≤–æ—Å—Ç–∏\",\n",
        "    4: \"–í—Å—ë –≤—ã—à–µ + OHLCV\",\n",
        "    5: \"–í—Å—ë –≤—ã—à–µ + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
        "    6: \"–í—Å—ë –≤—ã—à–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\"\n",
        "}\n",
        "\n",
        "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ TSAI –º–æ–¥–µ–ª–µ–π (–≤—ã–±–∏—Ä–∞–µ–º –±—ã—Å—Ç—Ä—ã–µ –∏ –Ω–∞–¥–µ–∂–Ω—ã–µ)\n",
        "TSAI_MODELS = {\n",
        "    'InceptionTime': {\n",
        "        'class': InceptionTime,\n",
        "        'kwargs': {\n",
        "            'nb_filters': 32,\n",
        "            'd': 1  # –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "        },\n",
        "        'epochs': 10,\n",
        "        'lr': 1e-3,\n",
        "        'description': '–°–≤–µ—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å —Å –º–æ–¥—É–ª—è–º–∏ Inception'\n",
        "    },\n",
        "    'ResNet': {\n",
        "        'class': ResNet,\n",
        "        'kwargs': {\n",
        "            'd': 1  # –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "        },\n",
        "        'epochs': 10,\n",
        "        'lr': 1e-3,\n",
        "        'description': '–û—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤'\n",
        "    },\n",
        "    'ROCKET': {\n",
        "        'class': ROCKET,\n",
        "        'kwargs': {\n",
        "            'num_kernels': 1000,\n",
        "            'kss': [7, 9, 11],\n",
        "            'device': str(device)\n",
        "        },\n",
        "        'epochs': 5,  # ROCKET –±—ã—Å—Ç—Ä–æ –æ–±—É—á–∞–µ—Ç—Å—è\n",
        "        'lr': 1e-3,\n",
        "        'description': '–†–∞–Ω–¥–æ–º–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —è–¥—Ä–∞'\n",
        "    }\n",
        "}\n",
        "\n",
        "print(f\"–ù–∞—Å—Ç—Ä–æ–π–∫–∏:\")\n",
        "print(f\"- –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {DATA_PATH}\")\n",
        "print(f\"- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_HORIZON}\")\n",
        "print(f\"- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç. –¥–∞–Ω–Ω—ã—Ö: {TEST_SIZE}\")\n",
        "print(f\"- –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {SEQUENCE_LENGTH}\")\n",
        "print(f\"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤: {len(TICKERS)}\")\n",
        "print(f\"- –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_PATH}\")\n",
        "print(f\"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ TSAI –º–æ–¥–µ–ª–µ–π: {len(TSAI_MODELS)}\")\n",
        "print(f\"- –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —ç—Ç–∞–ø–∞–º (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
        "def prepare_features_for_stage(df: pd.DataFrame, stage: int) -> List[str]:\n",
        "    \"\"\"\n",
        "    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
        "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-6)\n",
        "    \n",
        "    Returns:\n",
        "        List[str]: —Å–ø–∏—Å–æ–∫ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "    \"\"\"\n",
        "    available_features = set(df.columns)\n",
        "    \n",
        "    # –≠—Ç–∞–ø 1: –¢–æ–ª—å–∫–æ —Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è\n",
        "    if stage == 1:\n",
        "        return ['close'] if 'close' in available_features else []\n",
        "    \n",
        "    # –≠—Ç–∞–ø 2: –¶–µ–Ω–∞ + –∞–Ω–æ–º–∞–ª–∏–∏\n",
        "    elif stage == 2:\n",
        "        features = ['close']\n",
        "        if 'anomaly' in available_features:\n",
        "            features.append('anomaly')\n",
        "        return [f for f in features if f in available_features]\n",
        "    \n",
        "    # –≠—Ç–∞–ø 3: –¶–µ–Ω–∞ + –∞–Ω–æ–º–∞–ª–∏–∏ + –Ω–æ–≤–æ—Å—Ç–∏\n",
        "    elif stage == 3:\n",
        "        features = ['close']\n",
        "        if 'anomaly' in available_features:\n",
        "            features.append('anomaly')\n",
        "        if 'weighted_score_with_decay' in available_features:\n",
        "            features.append('weighted_score_with_decay')\n",
        "        return [f for f in features if f in available_features]\n",
        "    \n",
        "    # –≠—Ç–∞–ø 4: –í—Å—ë –≤—ã—à–µ + OHLCV\n",
        "    elif stage == 4:\n",
        "        features = ['close']\n",
        "        if 'anomaly' in available_features:\n",
        "            features.append('anomaly')\n",
        "        if 'weighted_score_with_decay' in available_features:\n",
        "            features.append('weighted_score_with_decay')\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º OHLCV\n",
        "        ohlcv_features = ['open', 'high', 'low', 'volume']\n",
        "        for feat in ohlcv_features:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        return [f for f in features if f in available_features]\n",
        "    \n",
        "    # –≠—Ç–∞–ø 5: –í—Å—ë –≤—ã—à–µ + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "    elif stage == 5:\n",
        "        features = ['close']\n",
        "        if 'anomaly' in available_features:\n",
        "            features.append('anomaly')\n",
        "        if 'weighted_score_with_decay' in available_features:\n",
        "            features.append('weighted_score_with_decay')\n",
        "        \n",
        "        # OHLCV\n",
        "        ohlcv_features = ['open', 'high', 'low', 'volume']\n",
        "        for feat in ohlcv_features:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "        tech_features = ['EMA_14', 'RSI_14', 'MACD', 'return', 'ATR_14', 'VWAP']\n",
        "        for feat in tech_features:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        return [f for f in features if f in available_features]\n",
        "    \n",
        "    # –≠—Ç–∞–ø 6: –í—Å—ë –≤—ã—à–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "    elif stage == 6:\n",
        "        features = ['close']\n",
        "        if 'anomaly' in available_features:\n",
        "            features.append('anomaly')\n",
        "        if 'weighted_score_with_decay' in available_features:\n",
        "            features.append('weighted_score_with_decay')\n",
        "        \n",
        "        # OHLCV\n",
        "        ohlcv_features = ['open', 'high', 'low', 'volume']\n",
        "        for feat in ohlcv_features:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "        tech_features = ['EMA_14', 'RSI_14', 'MACD', 'return', 'ATR_14', 'VWAP']\n",
        "        for feat in tech_features:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—ã–±–æ—Ä–æ—á–Ω–æ TSFresh)\n",
        "        tsfresh_stats = [\n",
        "            'value__mean',\n",
        "            'value__maximum',\n",
        "            'value__minimum',\n",
        "            'value__standard_deviation',\n",
        "            'value__partial_autocorrelation__lag_3',\n",
        "            'value__autocorrelation__lag_5',\n",
        "            'value__longest_strike_above_mean'\n",
        "        ]\n",
        "        for feat in tsfresh_stats:\n",
        "            if feat in available_features:\n",
        "                features.append(feat)\n",
        "        \n",
        "        return [f for f in features if f in available_features]\n",
        "    \n",
        "    else:\n",
        "        return ['close'] if 'close' in available_features else []\n",
        "\n",
        "print(\"–§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–∑–¥–∞–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è TSAI\n",
        "def calculate_directional_accuracy(actual: np.ndarray, predicted: np.ndarray) -> float:\n",
        "    \"\"\"–í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (DA)\"\"\"\n",
        "    if len(actual) < 2 or len(predicted) < 2:\n",
        "        return 0.0\n",
        "    \n",
        "    actual_direction = np.diff(actual) > 0\n",
        "    predicted_direction = np.diff(predicted) > 0\n",
        "    \n",
        "    return np.mean(actual_direction == predicted_direction) * 100\n",
        "\n",
        "def load_ticker_data(ticker: str) -> Optional[pd.DataFrame]:\n",
        "    \"\"\"–ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–∏–∫–µ—Ä–∞\"\"\"\n",
        "    file_path = f\"{DATA_PATH}{ticker}_multivariate.csv\"\n",
        "    \n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"‚ö†Ô∏è –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {file_path}\")\n",
        "        return None\n",
        "    \n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ timestamp –∫–æ–ª–æ–Ω–∫–∏\n",
        "        if 'timestamp' in df.columns:\n",
        "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "            df = df.set_index('timestamp')\n",
        "        \n",
        "        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤—Ä–µ–º–µ–Ω–∏\n",
        "        df = df.sort_index()\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "        required_cols = ['close']\n",
        "        if not all(col in df.columns for col in required_cols):\n",
        "            print(f\"‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ {ticker}\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω {ticker}: {len(df)} —Ç–æ—á–µ–∫, {len(df.columns)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def create_sequences(data, seq_len=30, horizon=1):\n",
        "    \"\"\"\n",
        "    –°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π TSAI\n",
        "    \n",
        "    Args:\n",
        "        data: –º–∞—Å—Å–∏–≤ –∑–Ω–∞—á–µ–Ω–∏–π –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞\n",
        "        seq_len: –¥–ª–∏–Ω–∞ –≤—Ö–æ–¥–Ω–æ–π –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏  \n",
        "        horizon: –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "    \n",
        "    Returns:\n",
        "        X, y: –≤—Ö–æ–¥–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∏ —Ü–µ–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - seq_len - horizon + 1):\n",
        "        X.append(data[i:(i + seq_len)])\n",
        "        y.append(data[i + seq_len:i + seq_len + horizon])\n",
        "    \n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "print(\"–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Å–æ–∑–¥–∞–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ—Ü–µ–Ω–∫–∏ TSAI –º–æ–¥–µ–ª–µ–π\n",
        "def evaluate_tsai_model(df: pd.DataFrame, ticker: str, stage: int, \n",
        "                        model_name: str, model_config: Dict) -> Optional[Dict]:\n",
        "    \"\"\"\n",
        "    –û–±—É—á–∞–µ—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç TSAI –º–æ–¥–µ–ª—å –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Ç–∏–∫–µ—Ä–∞ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞\n",
        "        ticker: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞\n",
        "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞\n",
        "        model_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ TSAI\n",
        "        model_config: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–∏\n",
        "        \n",
        "    Returns:\n",
        "        Dict —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
        "        feature_columns = prepare_features_for_stage(df, stage)\n",
        "        \n",
        "        if len(feature_columns) == 0:\n",
        "            print(f\"  ‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        \n",
        "        if len(available_features) == 0:\n",
        "            print(f\"  ‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "        df_clean = df[available_features].dropna()\n",
        "        \n",
        "        if len(df_clean) < TEST_SIZE + SEQUENCE_LENGTH + 10:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä\n",
        "            print(f\"  ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        print(f\"  üß† –°–æ–∑–¥–∞–Ω–∏–µ TSAI –º–æ–¥–µ–ª–∏ ({model_name})...\")\n",
        "        print(f\"  üìä –ü—Ä–∏–∑–Ω–∞–∫–∏ —ç—Ç–∞–ø–∞ {stage}: {', '.join(available_features)}\")\n",
        "        \n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (—Ü–µ–Ω–∞ –∑–∞–∫—Ä—ã—Ç–∏—è)\n",
        "        target_series = df_clean['close'].values.astype(float)\n",
        "        \n",
        "        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö\n",
        "        scaler = StandardScaler()\n",
        "        target_scaled = scaler.fit_transform(target_series.reshape(-1, 1)).flatten()\n",
        "        \n",
        "        # Walk-forward –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ\n",
        "        predictions = []\n",
        "        \n",
        "        # –ù–∞—á–∏–Ω–∞–µ–º —Å –ø–æ—Å–ª–µ–¥–Ω–∏—Ö TEST_SIZE —Ç–æ—á–µ–∫\n",
        "        start_idx = len(target_scaled) - TEST_SIZE\n",
        "        \n",
        "        print(f\"  üéØ Walk-forward –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ TSAI: {FORECAST_HORIZON} —à–∞–≥–æ–≤\")\n",
        "        \n",
        "        step_times = []\n",
        "        model = None\n",
        "        \n",
        "        for i in range(FORECAST_HORIZON):\n",
        "            step_start = time.time()\n",
        "            \n",
        "            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–∫—É—â—É—é –ø–æ–∑–∏—Ü–∏—é\n",
        "            current_pos = start_idx + i\n",
        "            \n",
        "            # –ë–µ—Ä–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–æ current_pos\n",
        "            hist_data = target_scaled[:current_pos]\n",
        "            \n",
        "            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö\n",
        "            if len(hist_data) < SEQUENCE_LENGTH + 10:\n",
        "                print(f\"    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —à–∞–≥–∞ {i+1}\")\n",
        "                predictions.append(np.nan)\n",
        "                step_times.append(time.time() - step_start)\n",
        "                continue\n",
        "            \n",
        "            # –ï—Å–ª–∏ –ø–µ—Ä–≤—ã–π —à–∞–≥ - –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "            if i == 0:\n",
        "                print(f\"    üöÄ –û–±—É—á–µ–Ω–∏–µ TSAI –º–æ–¥–µ–ª–∏ –Ω–∞ {len(hist_data)} —Ç–æ—á–∫–∞—Ö...\")\n",
        "                training_start = time.time()\n",
        "                \n",
        "                # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "                X_train, y_train = create_sequences(hist_data, SEQUENCE_LENGTH, 1)\n",
        "                \n",
        "                if len(X_train) < 10:\n",
        "                    print(f\"    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
        "                    return None\n",
        "                \n",
        "                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è TSAI\n",
        "                X_train = X_train.reshape(len(X_train), 1, SEQUENCE_LENGTH)  # (samples, vars, seq_len)\n",
        "                y_train = y_train.flatten()\n",
        "                \n",
        "                # –°–æ–∑–¥–∞–µ–º —Ä–∞–∑–±–∏–≤–∫—É train/validation\n",
        "                train_size = int(0.8 * len(X_train))\n",
        "                idxs = np.arange(len(X_train))\n",
        "                splits = ([idxs[:train_size]], [idxs[train_size:]])\n",
        "                \n",
        "                # –°–æ–∑–¥–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã TSAI\n",
        "                dsets = TSDatasets(X_train, y_train, splits=splits, tfms=None, inplace=True)\n",
        "                dls = TSDataLoaders.from_dsets(dsets.train, dsets.valid, bs=[32, 128])\n",
        "                \n",
        "                # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "                model_class = model_config['class']\n",
        "                model = model_class(dls.vars, dls.c, dls.len, **model_config['kwargs'])\n",
        "                \n",
        "                # –°–æ–∑–¥–∞–µ–º learner\n",
        "                learn = Learner(dls, model, loss_func=MSELossFlat(), metrics=[mae, rmse])\n",
        "                \n",
        "                # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "                with learn.no_bar(), learn.no_logging():\n",
        "                    learn.fit_one_cycle(model_config['epochs'], lr_max=model_config['lr'])\n",
        "                \n",
        "                training_time = time.time() - training_start\n",
        "                print(f\"    ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∑–∞ {training_time:.2f}—Å\")\n",
        "            \n",
        "            try:\n",
        "                # –°–æ–∑–¥–∞–µ–º –≤—Ö–æ–¥–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "                if len(hist_data) >= SEQUENCE_LENGTH:\n",
        "                    seq_input = hist_data[-SEQUENCE_LENGTH:].reshape(1, 1, SEQUENCE_LENGTH)\n",
        "                    \n",
        "                    # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
        "                    with torch.no_grad():\n",
        "                        pred_scaled = model(torch.tensor(seq_input, dtype=torch.float32).to(device)).cpu().numpy().ravel()[0]\n",
        "                    \n",
        "                    # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ü–µ–Ω—É\n",
        "                    pred_price = scaler.inverse_transform([[pred_scaled]])[0, 0]\n",
        "                    predictions.append(pred_price)\n",
        "                    \n",
        "                    step_time = time.time() - step_start\n",
        "                    step_times.append(step_time)\n",
        "                    \n",
        "                    print(f\"    üìà –®–∞–≥ {i+1}: –ø—Ä–æ–≥–Ω–æ–∑={pred_price:.4f}, –≤—Ä–µ–º—è={step_time:.3f}—Å\")\n",
        "                else:\n",
        "                    print(f\"    ‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –Ω–∞ —à–∞–≥–µ {i+1}\")\n",
        "                    predictions.append(np.nan)\n",
        "                    step_times.append(time.time() - step_start)\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ —à–∞–≥–µ {i+1}: {e}\")\n",
        "                predictions.append(np.nan)\n",
        "                step_times.append(time.time() - step_start)\n",
        "                continue\n",
        "        \n",
        "        # –§–∏–ª—å—Ç—Ä—É–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        predictions = np.array(predictions)\n",
        "        valid_mask = ~np.isnan(predictions)\n",
        "        \n",
        "        if np.sum(valid_mask) == 0:\n",
        "            print(f\"  ‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è {ticker}\")\n",
        "            return None\n",
        "        \n",
        "        predictions_clean = predictions[valid_mask]\n",
        "        \n",
        "        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        actual_values = target_series[start_idx:start_idx + len(predictions)]\n",
        "        actual_clean = actual_values[valid_mask]\n",
        "        \n",
        "        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
        "        mae = mean_absolute_error(actual_clean, predictions_clean)\n",
        "        rmse = mean_squared_error(actual_clean, predictions_clean, squared=False)\n",
        "        mape = mean_absolute_percentage_error(actual_clean, predictions_clean) * 100\n",
        "        da = calculate_directional_accuracy(actual_clean, predictions_clean)\n",
        "        \n",
        "        avg_time = np.mean(step_times)\n",
        "        \n",
        "        print(f\"  üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {ticker} (—ç—Ç–∞–ø {stage}, {model_name}):\")\n",
        "        print(f\"    MAE: {mae:.4f}\")\n",
        "        print(f\"    RMSE: {rmse:.4f}\")\n",
        "        print(f\"    MAPE: {mape:.2f}%\")\n",
        "        print(f\"    DA: {da:.2f}%\")\n",
        "        print(f\"    –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.3f}—Å\")\n",
        "        \n",
        "        return {\n",
        "            'ticker': ticker,\n",
        "            'stage': stage,\n",
        "            'model_name': model_name,\n",
        "            'predictions': predictions_clean,\n",
        "            'actual': actual_clean,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'mape': mape,\n",
        "            'da': da,\n",
        "            'time': avg_time,\n",
        "            'features': available_features\n",
        "        }\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ü–µ–Ω–∫–µ {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"–§—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ TSAI –º–æ–¥–µ–ª–∏ —Å–æ–∑–¥–∞–Ω–∞\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
        "print(\"üöÄ –ó–ê–ü–£–°–ö –ú–ù–û–ì–û–ú–ï–†–ù–û–ì–û TSAI DEEP LEARNING –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–ê\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "results = []\n",
        "\n",
        "# –í—ã–±–∏—Ä–∞–µ–º —ç—Ç–∞–ø—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
        "test_stages = [1, 6]  # –ë–∞–∑–æ–≤–∞—è —Ü–µ–Ω–∞ –∏ –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "print(f\"üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç—Ç–∞–ø—ã: {test_stages}\")\n",
        "print(f\"üéØ –¢–∏–∫–µ—Ä—ã: {TICKERS}\")\n",
        "print(f\"üß† –ú–æ–¥–µ–ª–∏ TSAI: {list(TSAI_MODELS.keys())}\")\n",
        "print(f\"‚è±Ô∏è –≠—Ç–æ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å –≤—Ä–µ–º—è (deep learning —Ç—Ä–µ–±—É–µ—Ç –æ–±—É—á–µ–Ω–∏—è)...\")\n",
        "\n",
        "# –°—á–µ—Ç—á–∏–∫ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞\n",
        "total_experiments = len(TICKERS) * len(test_stages) * len(TSAI_MODELS)\n",
        "current_experiment = 0\n",
        "\n",
        "for ticker in TICKERS:\n",
        "    print(f\"\\\\n{'='*80}\")\n",
        "    print(f\"üìà –û–ë–†–ê–ë–û–¢–ö–ê –¢–ò–ö–ï–†–ê: {ticker}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    \n",
        "    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Ç–∏–∫–µ—Ä–∞\n",
        "    df = load_ticker_data(ticker)\n",
        "    \n",
        "    if df is None:\n",
        "        print(f\"‚è≠Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º {ticker} - –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö\")\n",
        "        continue\n",
        "    \n",
        "    # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —ç—Ç–∞–ø\n",
        "    for stage in test_stages:\n",
        "        print(f\"\\\\nüî¨ –≠–¢–ê–ü {stage}: {STAGE_NAMES[stage]}\")\n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é –º–æ–¥–µ–ª—å TSAI\n",
        "        for model_name, model_config in TSAI_MODELS.items():\n",
        "            current_experiment += 1\n",
        "            progress = (current_experiment / total_experiments) * 100\n",
        "            \n",
        "            print(f\"\\\\nüìä –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç {current_experiment}/{total_experiments} ({progress:.1f}%)\")\n",
        "            print(f\"üß† –ú–æ–¥–µ–ª—å: {model_name} - {model_config['description']}\")\n",
        "            \n",
        "            # –ó–∞–ø—É—Å–∫–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏\n",
        "            experiment_start = time.time()\n",
        "            \n",
        "            result = evaluate_tsai_model(\n",
        "                df=df,\n",
        "                ticker=ticker,\n",
        "                stage=stage,\n",
        "                model_name=model_name,\n",
        "                model_config=model_config\n",
        "            )\n",
        "            \n",
        "            experiment_time = time.time() - experiment_start\n",
        "            \n",
        "            if result is not None:\n",
        "                # –î–æ–±–∞–≤–ª—è–µ–º –≤—Ä–µ–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞\n",
        "                result['experiment_time'] = experiment_time\n",
        "                results.append(result)\n",
        "                \n",
        "                print(f\"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {experiment_time:.1f}—Å\")\n",
        "            else:\n",
        "                print(f\"‚ùå –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –Ω–µ —É–¥–∞–ª—Å—è\")\n",
        "\n",
        "print(f\"\\\\n{'='*80}\")\n",
        "print(f\"üéâ –í–°–ï –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ó–ê–í–ï–†–®–ï–ù–´!\")\n",
        "print(f\"üìä –£—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {len(results)} –∏–∑ {total_experiments}\")\n",
        "print(f\"{'='*80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "if results:\n",
        "    print(\"üìä –ê–ù–ê–õ–ò–ó –†–ï–ó–£–õ–¨–¢–ê–¢–û–í TSAI DEEP LEARNING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º DataFrame —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
        "    df_results = pd.DataFrame(results)\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É\n",
        "    print(f\"üìà –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:\")\n",
        "    print(f\"  –í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(df_results)}\")\n",
        "    print(f\"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤: {df_results['ticker'].nunique()}\")\n",
        "    print(f\"  –≠—Ç–∞–ø–æ–≤: {sorted(df_results['stage'].unique())}\")\n",
        "    print(f\"  –ú–æ–¥–µ–ª–µ–π: {sorted(df_results['model_name'].unique())}\")\n",
        "    \n",
        "    # –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏\n",
        "    print(f\"\\\\nüìä –°—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏:\")\n",
        "    print(f\"  RMSE: {df_results['rmse'].mean():.4f} (¬±{df_results['rmse'].std():.4f})\")\n",
        "    print(f\"  MAPE: {df_results['mape'].mean():.2f}% (¬±{df_results['mape'].std():.2f}%)\")\n",
        "    print(f\"  DA: {df_results['da'].mean():.2f}% (¬±{df_results['da'].std():.2f}%)\")\n",
        "    print(f\"  –í—Ä–µ–º—è: {df_results['time'].mean():.3f}—Å (¬±{df_results['time'].std():.3f}—Å)\")\n",
        "    \n",
        "    # –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ\n",
        "    print(f\"\\\\nüèÜ –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
        "    \n",
        "    # –õ—É—á—à–∏–π RMSE\n",
        "    best_rmse = df_results.loc[df_results['rmse'].idxmin()]\n",
        "    print(f\"  –õ—É—á—à–∏–π RMSE: {best_rmse['rmse']:.4f} ({best_rmse['ticker']}, {best_rmse['model_name']}, —ç—Ç–∞–ø {best_rmse['stage']})\")\n",
        "    \n",
        "    # –õ—É—á—à–∏–π MAPE\n",
        "    best_mape = df_results.loc[df_results['mape'].idxmin()]\n",
        "    print(f\"  –õ—É—á—à–∏–π MAPE: {best_mape['mape']:.2f}% ({best_mape['ticker']}, {best_mape['model_name']}, —ç—Ç–∞–ø {best_mape['stage']})\")\n",
        "    \n",
        "    # –õ—É—á—à–∏–π DA\n",
        "    best_da = df_results.loc[df_results['da'].idxmax()]\n",
        "    print(f\"  –õ—É—á—à–∏–π DA: {best_da['da']:.2f}% ({best_da['ticker']}, {best_da['model_name']}, —ç—Ç–∞–ø {best_da['stage']})\")\n",
        "    \n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π\n",
        "    print(f\"\\\\nüß† –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π TSAI:\")\n",
        "    model_comparison = df_results.groupby('model_name').agg({\n",
        "        'rmse': ['mean', 'std'],\n",
        "        'mape': ['mean', 'std'],\n",
        "        'da': ['mean', 'std'],\n",
        "        'time': ['mean', 'std'],\n",
        "        'experiment_time': ['mean', 'std']\n",
        "    }).round(4)\n",
        "    \n",
        "    print(model_comparison)\n",
        "    \n",
        "    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤\n",
        "    if len(df_results['stage'].unique()) > 1:\n",
        "        print(f\"\\\\nüìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —ç—Ç–∞–ø–æ–≤:\")\n",
        "        stage_comparison = df_results.groupby('stage').agg({\n",
        "            'rmse': ['mean', 'std'],\n",
        "            'mape': ['mean', 'std'],\n",
        "            'da': ['mean', 'std']\n",
        "        }).round(4)\n",
        "        \n",
        "        print(stage_comparison)\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    results_file = f\"{OUTPUT_PATH}multivariate_tsai_results_{timestamp}.csv\"\n",
        "    \n",
        "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ–º –º–∞—Å—Å–∏–≤—ã)\n",
        "    df_save = df_results.drop(['predictions', 'actual'], axis=1, errors='ignore')\n",
        "    df_save.to_csv(results_file, index=False)\n",
        "    \n",
        "    print(f\"\\\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_file}\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "if results:\n",
        "    print(\"üìä –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –†–ï–ó–£–õ–¨–¢–ê–¢–û–í TSAI DEEP LEARNING\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "    fig.suptitle('üß† –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å TSAI Deep Learning', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # 1. RMSE –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "    model_rmse = df_results.groupby('model_name')['rmse'].mean().sort_values()\n",
        "    axes[0,0].bar(range(len(model_rmse)), model_rmse.values, color='lightcoral')\n",
        "    axes[0,0].set_title('–°—Ä–µ–¥–Ω–∏–π RMSE –ø–æ –º–æ–¥–µ–ª—è–º TSAI', fontweight='bold')\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].set_xticks(range(len(model_rmse)))\n",
        "    axes[0,0].set_xticklabels(model_rmse.index, rotation=45)\n",
        "    axes[0,0].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 2. MAPE –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "    model_mape = df_results.groupby('model_name')['mape'].mean().sort_values()\n",
        "    axes[0,1].bar(range(len(model_mape)), model_mape.values, color='lightblue')\n",
        "    axes[0,1].set_title('–°—Ä–µ–¥–Ω–∏–π MAPE –ø–æ –º–æ–¥–µ–ª—è–º TSAI', fontweight='bold')\n",
        "    axes[0,1].set_ylabel('MAPE (%)')\n",
        "    axes[0,1].set_xticks(range(len(model_mape)))\n",
        "    axes[0,1].set_xticklabels(model_mape.index, rotation=45)\n",
        "    axes[0,1].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 3. DA –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "    model_da = df_results.groupby('model_name')['da'].mean().sort_values(ascending=False)\n",
        "    axes[0,2].bar(range(len(model_da)), model_da.values, color='lightgreen')\n",
        "    axes[0,2].set_title('–°—Ä–µ–¥–Ω–∏–π DA –ø–æ –º–æ–¥–µ–ª—è–º TSAI', fontweight='bold')\n",
        "    axes[0,2].set_ylabel('DA (%)')\n",
        "    axes[0,2].set_xticks(range(len(model_da)))\n",
        "    axes[0,2].set_xticklabels(model_da.index, rotation=45)\n",
        "    axes[0,2].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    # 4. RMSE –ø–æ —ç—Ç–∞–ø–∞–º\n",
        "    if df_results['stage'].nunique() > 1:\n",
        "        stage_rmse = df_results.groupby('stage')['rmse'].mean()\n",
        "        axes[1,0].plot(stage_rmse.index, stage_rmse.values, marker='o', linewidth=2, markersize=8)\n",
        "        axes[1,0].set_title('RMSE –ø–æ —ç—Ç–∞–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
        "        axes[1,0].set_xlabel('–≠—Ç–∞–ø')\n",
        "        axes[1,0].set_ylabel('RMSE')\n",
        "        axes[1,0].grid(True, alpha=0.3)\n",
        "        axes[1,0].set_xticks(stage_rmse.index)\n",
        "    else:\n",
        "        axes[1,0].text(0.5, 0.5, '–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —ç—Ç–∞–ø\\\\n–≤ –¥–∞–Ω–Ω—ã—Ö', ha='center', va='center', transform=axes[1,0].transAxes)\n",
        "        axes[1,0].set_title('RMSE –ø–æ —ç—Ç–∞–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
        "    \n",
        "    # 5. MAPE –ø–æ —ç—Ç–∞–ø–∞–º\n",
        "    if df_results['stage'].nunique() > 1:\n",
        "        stage_mape = df_results.groupby('stage')['mape'].mean()\n",
        "        axes[1,1].plot(stage_mape.index, stage_mape.values, marker='s', linewidth=2, markersize=8, color='orange')\n",
        "        axes[1,1].set_title('MAPE –ø–æ —ç—Ç–∞–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
        "        axes[1,1].set_xlabel('–≠—Ç–∞–ø')\n",
        "        axes[1,1].set_ylabel('MAPE (%)')\n",
        "        axes[1,1].grid(True, alpha=0.3)\n",
        "        axes[1,1].set_xticks(stage_mape.index)\n",
        "    else:\n",
        "        axes[1,1].text(0.5, 0.5, '–¢–æ–ª—å–∫–æ –æ–¥–∏–Ω —ç—Ç–∞–ø\\\\n–≤ –¥–∞–Ω–Ω—ã—Ö', ha='center', va='center', transform=axes[1,1].transAxes)\n",
        "        axes[1,1].set_title('MAPE –ø–æ —ç—Ç–∞–ø–∞–º –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
        "    \n",
        "    # 6. –í—Ä–µ–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ø–æ –º–æ–¥–µ–ª—è–º\n",
        "    model_time = df_results.groupby('model_name')['experiment_time'].mean() / 60  # –≤ –º–∏–Ω—É—Ç–∞—Ö\n",
        "    axes[1,2].bar(range(len(model_time)), model_time.values, color='purple', alpha=0.7)\n",
        "    axes[1,2].set_title('–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–º–∏–Ω)', fontweight='bold')\n",
        "    axes[1,2].set_ylabel('–í—Ä–µ–º—è (–º–∏–Ω)')\n",
        "    axes[1,2].set_xticks(range(len(model_time)))\n",
        "    axes[1,2].set_xticklabels(model_time.index, rotation=45)\n",
        "    axes[1,2].grid(axis='y', alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    plot_file = f\"{OUTPUT_PATH}multivariate_tsai_comparison_{timestamp}.png\"\n",
        "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
        "    print(f\"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_file}\")\n",
        "    \n",
        "    plt.show()\n",
        "    \n",
        "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è - heatmap –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ –º–æ–¥–µ–ª—è–º\n",
        "    if len(df_results) > 0:\n",
        "        print(\"\\\\nüìà –î–µ—Ç–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ —Ç–∏–∫–µ—Ä–∞–º:\")\n",
        "        \n",
        "        plt.figure(figsize=(16, 10))\n",
        "        \n",
        "        # Heatmap RMSE –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ –º–æ–¥–µ–ª—è–º\n",
        "        pivot_rmse = df_results.pivot_table(values='rmse', index='ticker', columns='model_name', aggfunc='mean')\n",
        "        \n",
        "        plt.subplot(2, 2, 1)\n",
        "        sns.heatmap(pivot_rmse, annot=True, fmt='.3f', cmap='Reds', cbar_kws={'label': 'RMSE'})\n",
        "        plt.title('RMSE –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ TSAI –º–æ–¥–µ–ª—è–º', fontweight='bold')\n",
        "        plt.ylabel('–¢–∏–∫–µ—Ä—ã')\n",
        "        plt.xlabel('TSAI –º–æ–¥–µ–ª–∏')\n",
        "        \n",
        "        # Heatmap MAPE –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ –º–æ–¥–µ–ª—è–º\n",
        "        pivot_mape = df_results.pivot_table(values='mape', index='ticker', columns='model_name', aggfunc='mean')\n",
        "        \n",
        "        plt.subplot(2, 2, 2)\n",
        "        sns.heatmap(pivot_mape, annot=True, fmt='.1f', cmap='Blues', cbar_kws={'label': 'MAPE (%)'})\n",
        "        plt.title('MAPE –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ TSAI –º–æ–¥–µ–ª—è–º', fontweight='bold')\n",
        "        plt.ylabel('–¢–∏–∫–µ—Ä—ã')\n",
        "        plt.xlabel('TSAI –º–æ–¥–µ–ª–∏')\n",
        "        \n",
        "        # Heatmap DA –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ –º–æ–¥–µ–ª—è–º\n",
        "        pivot_da = df_results.pivot_table(values='da', index='ticker', columns='model_name', aggfunc='mean')\n",
        "        \n",
        "        plt.subplot(2, 2, 3)\n",
        "        sns.heatmap(pivot_da, annot=True, fmt='.1f', cmap='Greens', cbar_kws={'label': 'DA (%)'})\n",
        "        plt.title('DA –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ TSAI –º–æ–¥–µ–ª—è–º', fontweight='bold')\n",
        "        plt.ylabel('–¢–∏–∫–µ—Ä—ã')\n",
        "        plt.xlabel('TSAI –º–æ–¥–µ–ª–∏')\n",
        "        \n",
        "        # Scatter plot RMSE vs DA\n",
        "        plt.subplot(2, 2, 4)\n",
        "        for model in df_results['model_name'].unique():\n",
        "            model_data = df_results[df_results['model_name'] == model]\n",
        "            plt.scatter(model_data['rmse'], model_data['da'], label=model, alpha=0.7, s=60)\n",
        "        \n",
        "        plt.xlabel('RMSE')\n",
        "        plt.ylabel('DA (%)')\n",
        "        plt.title('–°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ RMSE –∏ DA –¥–ª—è TSAI', fontweight='bold')\n",
        "        plt.legend()\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        \n",
        "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é\n",
        "        detailed_plot_file = f\"{OUTPUT_PATH}tsai_detailed_comparison_{timestamp}.png\"\n",
        "        plt.savefig(detailed_plot_file, dpi=300, bbox_inches='tight')\n",
        "        print(f\"üìä –î–µ—Ç–∞–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {detailed_plot_file}\")\n",
        "        \n",
        "        plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ –ø–æ –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–º—É TSAI Deep Learning\n",
        "\n",
        "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **TSAI** - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.\n",
        "\n",
        "### üîë –ö–ª—é—á–µ–≤—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:\n",
        "\n",
        "1. **üß† Deep Learning –ø–æ–¥—Ö–æ–¥**: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "2. **üéØ –¢–æ—á–Ω–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ multivariate –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏**: –¢–µ –∂–µ —Ñ–∏—á–∏, —ç—Ç–∞–ø—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "3. **üìä Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è**: TEST_SIZE=11, FORECAST_HORIZON=10\n",
        "4. **üåü Sequence modeling**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –¥–ª–∏–Ω–æ–π 30 —Ç–æ—á–µ–∫\n",
        "5. **‚ö° GPU –ø–æ–¥–¥–µ—Ä–∂–∫–∞**: –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CUDA –∫–æ–≥–¥–∞ –¥–æ—Å—Ç—É–ø–Ω–æ\n",
        "\n",
        "### üèóÔ∏è –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ TSAI:\n",
        "\n",
        "- **InceptionTime**: –°–≤–µ—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å —Å –º–æ–¥—É–ª—è–º–∏ Inception –¥–ª—è –≤—ã—è–≤–ª–µ–Ω–∏—è –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —Ä–∞–∑–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∞\n",
        "- **ResNet**: –û—Å—Ç–∞—Ç–æ—á–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π –±–µ–∑ –∑–∞—Ç—É—Ö–∞–Ω–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤  \n",
        "- **ROCKET**: –†–∞–Ω–¥–æ–º–Ω—ã–µ —Å–≤–µ—Ä—Ç–æ—á–Ω—ã–µ —è–¥—Ä–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "- **Sequence Processing**: –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –¥–ª–∏–Ω—ã\n",
        "\n",
        "### üìà –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã –¥—Ä—É–≥–∏–º multivariate –±–ª–æ–∫–Ω–æ—Ç–∞–º):\n",
        "\n",
        "- **TEST_SIZE**: 11 —Ç–æ—á–µ–∫\n",
        "- **FORECAST_HORIZON**: 10 —Ç–æ—á–µ–∫  \n",
        "- **SEQUENCE_LENGTH**: 30 —Ç–æ—á–µ–∫ (–≤—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å)\n",
        "- **–¢–∏–∫–µ—Ä—ã**: ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
        "- **–≠—Ç–∞–ø—ã**: [1, 6] (–±–∞–∑–æ–≤–∞—è —Ü–µ–Ω–∞ –∏ –ø–æ–ª–Ω—ã–π –Ω–∞–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤)\n",
        "\n",
        "### üéØ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç—Ç–∞–ø–æ–≤ (—Ç–æ—á–Ω–æ –∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö):\n",
        "\n",
        "1. **–≠—Ç–∞–ø 1**: –¢–æ–ª—å–∫–æ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è (close)\n",
        "2. **–≠—Ç–∞–ø 2**: + –ê–Ω–æ–º–∞–ª–∏–∏ (anomaly)\n",
        "3. **–≠—Ç–∞–ø 3**: + –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è (weighted_score_with_decay)\n",
        "4. **–≠—Ç–∞–ø 4**: + OHLCV –¥–∞–Ω–Ω—ã–µ (open, high, low, volume)\n",
        "5. **–≠—Ç–∞–ø 5**: + –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (EMA_14, RSI_14, MACD, return, ATR_14, VWAP)\n",
        "6. **–≠—Ç–∞–ø 6**: + –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ TSFresh (value__mean, value__maximum, –∏ –¥—Ä.)\n",
        "\n",
        "### üî¨ –ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ TSAI Deep Learning –ø–æ–¥—Ö–æ–¥–∞:\n",
        "\n",
        "1. **üéØ –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è**: –ú–æ–¥–µ–ª–∏ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω—ã –¥–ª—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "2. **üîç –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤**: –ù–µ —Ç—Ä–µ–±—É–µ—Ç —Ä—É—á–Ω–æ–π –∏–Ω–∂–µ–Ω–µ—Ä–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "3. **üìà –ù–µ–ª–∏–Ω–µ–π–Ω—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏**: –°–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞—Ç—å —Å–ª–æ–∂–Ω—ã–µ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∏\n",
        "4. **‚ö° FastAI –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è (one-cycle learning)\n",
        "5. **üîÑ Transfer Learning –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª**: –í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–Ω–æ—Å–∞ –∑–Ω–∞–Ω–∏–π –º–µ–∂–¥—É —Ä—è–¥–∞–º–∏\n",
        "\n",
        "### ‚ö†Ô∏è –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Å–æ–æ–±—Ä–∞–∂–µ–Ω–∏—è:\n",
        "\n",
        "1. **üíª –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è**: –¢—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤ —á–µ–º –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –º–µ—Ç–æ–¥—ã\n",
        "2. **üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö**: –ù—É–∂–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π\n",
        "3. **üéõÔ∏è –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã**: –¢—Ä–µ–±—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è\n",
        "4. **üîÆ –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å**: –°–ª–æ–∂–Ω–µ–µ –ø–æ–Ω—è—Ç—å –ª–æ–≥–∏–∫—É –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª—å—é\n",
        "\n",
        "### üöÄ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:\n",
        "\n",
        "#### –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏:\n",
        "- **InceptionTime**: –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞—á\n",
        "- **ResNet**: –ö–æ–≥–¥–∞ –Ω—É–∂–Ω–∞ —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è –≥–ª—É–±–æ–∫–∏—Ö —Å–µ—Ç–µ–π\n",
        "- **ROCKET**: –î–ª—è –±—ã—Å—Ç—Ä—ã—Ö —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –∏ –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–∏–∏\n",
        "\n",
        "#### –ù–∞—Å—Ç—Ä–æ–π–∫–∏:\n",
        "- **Sequence Length (30)**: –û–ø—Ç–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–∞–Ω—Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–π\n",
        "- **Epochs (5-10)**: –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
        "- **Learning Rate (1e-3)**: –°—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ\n",
        "- **Normalization**: –û–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
        "\n",
        "### üîÑ –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ multivariate –ø–æ–¥—Ö–æ–¥–∞–º–∏:\n",
        "\n",
        "| –ê—Å–ø–µ–∫—Ç | ML_DARTS | ML_CHRONOS | **ML_TSAI** |\n",
        "|--------|----------|------------|-------------|\n",
        "| **–ü–æ–¥—Ö–æ–¥** | –ö–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ ML | Foundation Model | **Deep Learning** |\n",
        "| **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞** | –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã | Transformer | **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ NN** |\n",
        "| **–û–±—É—á–µ–Ω–∏–µ** | –ë—ã—Å—Ç—Ä–æ–µ | Zero-shot | **–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è** |\n",
        "| **–ì–∏–±–∫–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è | **–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è** |\n",
        "| **–ù–µ–ª–∏–Ω–µ–π–Ω–æ—Å—Ç—å** | –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è | –í—ã—Å–æ–∫–∞—è | **–û—á–µ–Ω—å –≤—ã—Å–æ–∫–∞—è** |\n",
        "| **–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è | –ù–∏–∑–∫–∞—è | **–ù–∏–∑–∫–∞—è** |\n",
        "| **GPU —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è** | –ù–µ—Ç | –ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ | **–ñ–µ–ª–∞—Ç–µ–ª—å–Ω–æ** |\n",
        "\n",
        "### üèÜ –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å TSAI:\n",
        "\n",
        "‚úÖ **–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:**\n",
        "- –°–ª–æ–∂–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Å –Ω–µ–ª–∏–Ω–µ–π–Ω—ã–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏\n",
        "- –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö (>1000 —Ç–æ—á–µ–∫)\n",
        "- –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å GPU –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –æ–±—É—á–µ–Ω–∏—è\n",
        "- –ó–∞–¥–∞—á–∏, –≥–¥–µ —Ç–æ—á–Ω–æ—Å—Ç—å –≤–∞–∂–Ω–µ–µ –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏\n",
        "- –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ –ø—Ä–æ–µ–∫—Ç—ã\n",
        "\n",
        "‚ùå **–ù–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:**\n",
        "- –ü—Ä–æ—Å—Ç—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Å –æ—á–µ–≤–∏–¥–Ω—ã–º–∏ —Ç—Ä–µ–Ω–¥–∞–º–∏\n",
        "- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã\n",
        "- –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ –±—ã—Å—Ç—Ä–æ–º—É –ø–æ–ª—É—á–µ–Ω–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
        "- –ö—Ä–∏—Ç–∏—á–Ω–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ–º–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏\n",
        "- –ú–∞–ª–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (<500 —Ç–æ—á–µ–∫)\n",
        "\n",
        "### üåü –£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ TSAI:\n",
        "\n",
        "- **–ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**: –õ–µ–≥–∫–∞—è –∑–∞–º–µ–Ω–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –º–æ–¥–µ–ª–∏\n",
        "- **–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏**: Attention, skip connections, batch normalization\n",
        "- **FastAI —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞**: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è –∏ —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏–∏\n",
        "- **–ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π —Ñ–æ–∫—É—Å**: –ù–æ–≤–µ–π—à–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –∏–∑ –Ω–∞—É—á–Ω—ã—Ö –ø—É–±–ª–∏–∫–∞—Ü–∏–π\n",
        "\n",
        "**TSAI –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –º–æ—â–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –≥–ª—É–±–æ–∫–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –∫ –∑–∞–¥–∞—á–∞–º –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤, –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –Ω–µ–ª–∏–Ω–µ–π–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤!** üß†‚ö°\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
