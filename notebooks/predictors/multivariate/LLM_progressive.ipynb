{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ c –ø–æ–º–æ—â—å—é LLM.\n",
    "\n",
    "## –≠—Ç–∞–ø—ã –∞–Ω–∞–ª–∏–∑–∞:\n",
    "1. **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å** - –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—è–¥—É `close` (univariate)\n",
    "2. **+ –ê–Ω–æ–º–∞–ª–∏–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π\n",
    "3. **+ –ù–æ–≤–æ—Å—Ç–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π  \n",
    "4. **+ –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —è–ø–æ–Ω—Å–∫–∏—Ö —Å–≤–µ—á–µ–π\n",
    "5. **+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
    "6. **+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
    "7. **+ PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∂–∞—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "## –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏:\n",
    "- **RMSE** - —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞\n",
    "- **MAPE** - —Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞  \n",
    "- **DA** - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (Directional Accuracy)\n",
    "\n",
    "## –ú–æ–¥–µ–ª—å:\n",
    "- **ChatGPT-4o-mini** –∏–∑ OPENROUTER \n",
    "- **–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞**: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–Ω–µ–π\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# –ü—É—Ç—å –∫ pip –≤ –∞–∫—Ç–∏–≤–Ω–æ–º —è–¥—Ä–µ\n",
    "pip_path = os.path.join(sys.prefix, \"bin\", \"pip\")\n",
    "\n",
    "subprocess.check_call([pip_path, \"install\", \"requests\", \"langchain\", \"langchain-gigachat\", \"matplotlib\", \"scikit-learn\", \"pandas\", \"numpy\", \"seaborn\", \"gigachat\", \"openai\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain_gigachat import GigaChat\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "\n",
    "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è API –∫–ª—é—á–µ–π\n",
    "API_KEYS = {\n",
    "    'openrouter': \"\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞:\n",
      "- –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: ../../data/multivariate_series/\n",
      "- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: ./progressive_analysis/\n",
      "- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: 10 –¥–Ω–µ–π\n",
      "- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: 11 –¥–Ω–µ–π\n",
      "- –¢–∏–∫–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω AFLT: (2415, 918)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω LKOH: (2415, 915)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω MOEX: (2415, 918)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω NVTK: (2415, 916)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω PIKK: (2415, 911)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω SBER: (2415, 909)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω VKCO: (1241, 910)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω VTBR: (1764, 905)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω X5: (1547, 913)\n",
      "–ó–∞–≥—Ä—É–∂–µ–Ω YDEX: (2415, 918)\n",
      "\n",
      "–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ 10 —Ç–∏–∫–µ—Ä–æ–≤\n"
     ]
    }
   ],
   "source": [
    "# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
    "INPUT_PATH = '../../data/multivariate_series/'\n",
    "OUTPUT_PATH = './progressive_analysis/'\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤\n",
    "tickers = ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
    "\n",
    "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞\n",
    "FORECAST_HORIZON = 10  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
    "TEST_SIZE = 11         # –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (–±–æ–ª—å—à–µ —á–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞)\n",
    "\n",
    "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞:\")\n",
    "print(f\"- –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {INPUT_PATH}\")\n",
    "print(f\"- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {OUTPUT_PATH}\")\n",
    "print(f\"- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_HORIZON} –¥–Ω–µ–π\")\n",
    "print(f\"- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {TEST_SIZE} –¥–Ω–µ–π\")\n",
    "print(f\"- –¢–∏–∫–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {tickers}\")\n",
    "\n",
    "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "data = {}\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        file_path = f\"{INPUT_PATH}{ticker}_multivariate.csv\"\n",
    "        # 1) –ß–∏—Ç–∞–µ–º CSV, –ø–∞—Ä—Å–∏–º timestamp\n",
    "        df = pd.read_csv(file_path, parse_dates=['timestamp'])\n",
    "        # 2) –£–±–∏—Ä–∞–µ–º timezone (–¥–µ–ª–∞–µ–º tz-naive)\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], utc=True).dt.tz_localize(None)\n",
    "        # 3) –ò–Ω–¥–µ–∫—Å–∏—Ä—É–µ–º –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "        # 4) –†–µ–∏ÃÜ–Ω–∂–∏—Ä—É–µ–º –ø–æ —Ä–µ–∞–ª—å–Ω—ã–º –±–∏–∑–Ω–µ—Å-–¥–Ω—è–º\n",
    "        bd_index = pd.date_range(df.index.min(), df.index.max(), freq='B')\n",
    "        df = df.reindex(bd_index)\n",
    "        # 5) –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ for all columns\n",
    "        df = df.ffill().bfill()\n",
    "        # 6) –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å\n",
    "        df.index.name = 'timestamp'\n",
    "        \n",
    "        data[ticker] = df\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω {ticker}: {df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {ticker}: {e}\")\n",
    "\n",
    "print(f\"\\n–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Ç–∏–∫–µ—Ä–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_directional_accuracy(actual, predicted):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (Directional Accuracy)\n",
    "    \n",
    "    Args:\n",
    "        actual: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        predicted: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "    \n",
    "    Returns:\n",
    "        DA: —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–æ—Ç 0 –¥–æ 1)\n",
    "    \"\"\"\n",
    "    if len(actual) <= 1 or len(predicted) <= 1:\n",
    "        return np.nan\n",
    "    \n",
    "    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
    "    actual_direction = np.diff(actual) > 0\n",
    "    \n",
    "    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
    "    predicted_direction = np.diff(predicted) > 0\n",
    "    \n",
    "    # –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "    da = np.mean(actual_direction == predicted_direction)\n",
    "    \n",
    "    return da\n",
    "\n",
    "def prepare_features_for_stage(df, stage):\n",
    "    \"\"\"\n",
    "    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –∞–Ω–∞–ª–∏–∑–∞\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
    "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-7)\n",
    "    \n",
    "    Returns:\n",
    "        feature_columns: —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
    "    \"\"\"\n",
    "    \n",
    "    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤—Å–µ–≥–¥–∞ –∏—Å–∫–ª—é—á–∞–µ–º)\n",
    "    base_exclude = ['date', 'daily_headlines', 'return']\n",
    "    \n",
    "    if stage == 1:\n",
    "        # –≠—Ç–∞–ø 1: —Ç–æ–ª—å–∫–æ close (univariate)\n",
    "        return ['close']\n",
    "    \n",
    "    elif stage == 2:\n",
    "        # –≠—Ç–∞–ø 2: + –∞–Ω–æ–º–∞–ª–∏–∏\n",
    "        features = ['close', 'anomaly']\n",
    "        return features\n",
    "    \n",
    "    elif stage == 3:\n",
    "        # –≠—Ç–∞–ø 3: + –Ω–æ–≤–æ—Å—Ç–∏ –æ—Ü–µ–Ω–∫–æ–π\n",
    "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
    "        return features\n",
    "\n",
    "    elif stage == 4:\n",
    "        # –≠—Ç–∞–ø 4: + –Ω–æ–≤–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–æ–º\n",
    "        features = ['close', 'anomaly', 'daily_headlines']\n",
    "        return features\n",
    "    \n",
    "    \n",
    "    elif stage == 5:\n",
    "        # –≠—Ç–∞–ø 5: + –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π\n",
    "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–∏\n",
    "        candels_cols = [col for col in df.columns if col in ['open', 'high', 'low', 'volume']]\n",
    "        features.extend(candels_cols)\n",
    "\n",
    "        return features\n",
    "    \n",
    "    elif stage == 6:\n",
    "        # –≠—Ç–∞–ø 6: + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–∏\n",
    "        candels_cols = [col for col in df.columns if col in ['open', 'high', 'low', 'volume']]\n",
    "        features.extend(candels_cols)\n",
    "        \n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "        tech_indicators = ['return', 'SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
    "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
    "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
    "        features.extend(tech_cols)\n",
    "        return features\n",
    "    \n",
    "    elif stage == 7:\n",
    "        # –≠—Ç–∞–ø 7: + TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–∏\n",
    "        candels_cols = [col for col in df.columns if col in ['open', 'high', 'low', 'volume']]\n",
    "        features.extend(candels_cols)\n",
    "        \n",
    "        tech_indicators = ['returb', 'SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
    "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
    "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
    "        features.extend(tech_cols)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "        tsfresh_cols = ['value__cwt_coefficients__coeff_14__w10_withs_(2, 5, 10, 20)', 'value__minimun', 'value__maximin', 'value__mean']\n",
    "        features.extend(tsfresh_cols)\n",
    "        return features\n",
    "\n",
    "    elif stage == 8:\n",
    "        # –≠—Ç–∞–ø 8: + –ö–∞—Ä—Ç–∏–Ω–∫–∞\n",
    "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º —Å–≤–µ—á–∏\n",
    "        candels_cols = [col for col in df.columns if col in ['open', 'high', 'low', 'volume']]\n",
    "        features.extend(candels_cols)\n",
    "        \n",
    "        tech_indicators = ['return', 'SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
    "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
    "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
    "        features.extend(tech_cols)\n",
    "\n",
    "        # –î–æ–±–∞–≤–ª—è–µ–º TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "        tsfresh_cols = [col for col in df.columns if 'value__' in col]\n",
    "        features.extend(tsfresh_cols)\n",
    "\n",
    "        #TODO: –ö–∞—Ä—Ç–∏–Ω–∫–∞ –≤ base64 (–≥—Ä–∞—Ñ–∏–∫ + –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ç–µ—Ö.–∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤, –∫–∞–∫ –¥–ª—è —Ç—Ä–µ–¥–µ—Ä–∞)\n",
    "        return features\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —ç—Ç–∞–ø: {stage}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ç—á–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã!\n"
     ]
    }
   ],
   "source": [
    "def compute_patch_stats(patch):\n",
    "    \"\"\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫ –¥–ª—è –ø–∞—Ç—á–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞\"\"\"\n",
    "    ex = float(np.mean(patch))\n",
    "    dx = float(np.std(patch))\n",
    "    try:\n",
    "        mode_val = float(statistics.mode([round(x, 2) for x in patch]))\n",
    "    except statistics.StatisticsError:\n",
    "        mode_val = None\n",
    "    pct = (patch[-1] - patch[0]) / patch[0] * 100 if patch[0] != 0 else 0\n",
    "    return ex, dx, mode_val, pct\n",
    "\n",
    "def make_patch_messages(window_data, stage, patch_size=5):\n",
    "    \"\"\"\n",
    "    –†–∞–∑–±–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ–∫–Ω–æ –Ω–∞ –Ω–µ–ø–µ—Ä–µ–∫—Ä—ã–≤–∞—é—â–∏–µ—Å—è –ø–∞—Ç—á–∏ patch_size\n",
    "    –∏ –≤–µ—Ä–Ω—É—Ç—å list system-—Å–æ–æ–±—â–µ–Ω–∏–π –ø–æ –∫–∞–∂–¥–æ–º—É –ø–∞—Ç—á—É.\n",
    "    \"\"\"\n",
    "    msgs = []\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –¥–∞–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞\n",
    "    features = prepare_features_for_stage(window_data, stage)\n",
    "    available_features = [f for f in features if f in window_data.columns]\n",
    "    \n",
    "    n_patches = len(window_data) // patch_size\n",
    "    if n_patches == 0:\n",
    "        n_patches = 1\n",
    "        patch_size = len(window_data)\n",
    "    \n",
    "    for i in range(n_patches):\n",
    "        start_idx = i * patch_size\n",
    "        end_idx = min((i + 1) * patch_size, len(window_data))\n",
    "        patch_data = window_data.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –ø–∞—Ç—á–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤\n",
    "        characteristics = []\n",
    "        \n",
    "        # –í—Å–µ–≥–¥–∞ –¥–æ–±–∞–≤–ª—è–µ–º —Ü–µ–Ω—ã\n",
    "        if 'close' in available_features:\n",
    "            prices = [f\"{val:.2f}\" for val in patch_data['close'].values]\n",
    "            characteristics.append(f\"Prices: {', '.join(prices)}\")\n",
    "        \n",
    "        if stage >= 2 and 'anomaly' in available_features:\n",
    "            # –≠—Ç–∞–ø 2+: –¥–æ–±–∞–≤–ª—è–µ–º –∞–Ω–æ–º–∞–ª–∏–∏\n",
    "            anomalies = [str(int(val)) if not pd.isna(val) else \"0\" for val in patch_data['anomaly'].values]\n",
    "            characteristics.append(f\"Anomaly (0-no, 1-yes): {', '.join(anomalies)}\")\n",
    "        \n",
    "        if stage >= 3 and 'weighted_score_with_decay' in available_features:\n",
    "            # –≠—Ç–∞–ø 3+: –¥–æ–±–∞–≤–ª—è–µ–º –Ω–æ–≤–æ—Å—Ç–Ω—É—é –æ—Ü–µ–Ω–∫—É\n",
    "            scores = [f\"{val*100:.0f}\" if not pd.isna(val) else \"0\" for val in patch_data['weighted_score_with_decay'].values]\n",
    "            characteristics.append(f\"News sentiment (-100 to +100): {', '.join(scores)}\")\n",
    "        \n",
    "        if stage >= 4 and 'daily_headlines' in available_features:\n",
    "            # –≠—Ç–∞–ø 4+: –¥–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–π –≤ –ø–∞—Ç—á–µ)\n",
    "            headlines = patch_data['daily_headlines'].dropna()\n",
    "            if len(headlines) > 0:\n",
    "                last_headline = str(headlines.iloc[-1])[:100] + (\"...\" if len(str(headlines.iloc[-1])) > 100 else \"\")\n",
    "                characteristics.append(f\"Latest news: {last_headline}\")\n",
    "        \n",
    "        if stage >= 5:\n",
    "            # –≠—Ç–∞–ø 5+: –¥–æ–±–∞–≤–ª—è–µ–º OHLV –¥–∞–Ω–Ω—ã–µ\n",
    "            if 'open' in available_features:\n",
    "                opens = [f\"{val:.2f}\" for val in patch_data['open'].values]\n",
    "                characteristics.append(f\"Open prices: {', '.join(opens)}\")\n",
    "            \n",
    "            if 'high' in available_features:\n",
    "                highs = [f\"{val:.2f}\" for val in patch_data['high'].values]\n",
    "                characteristics.append(f\"High prices: {', '.join(highs)}\")\n",
    "            \n",
    "            if 'low' in available_features:\n",
    "                lows = [f\"{val:.2f}\" for val in patch_data['low'].values]\n",
    "                characteristics.append(f\"Low prices: {', '.join(lows)}\")\n",
    "            \n",
    "            if 'volume' in available_features:\n",
    "                volumes = [f\"{val:.0f}\" if not pd.isna(val) else \"0\" for val in patch_data['volume'].values]\n",
    "                characteristics.append(f\"Volumes: {', '.join(volumes)}\")\n",
    "        \n",
    "        if stage >= 6:\n",
    "            # –≠—Ç–∞–ø 6+: –¥–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (–≤—ã–±–æ—Ä–æ—á–Ω–æ, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–≥—Ä—É–∂–∞—Ç—å)\n",
    "            if 'RSI_14' in available_features:\n",
    "                rsi_vals = [f\"{val:.1f}\" if not pd.isna(val) else \"50\" for val in patch_data['RSI_14'].values]\n",
    "                characteristics.append(f\"RSI: {', '.join(rsi_vals)}\")\n",
    "            \n",
    "            if 'MACD' in available_features:\n",
    "                macd_vals = [f\"{val:.3f}\" if not pd.isna(val) else \"0\" for val in patch_data['MACD'].values]\n",
    "                characteristics.append(f\"MACD: {', '.join(macd_vals)}\")\n",
    "            \n",
    "            if 'return' in available_features:\n",
    "                returns = [f\"{val*100:.1f}%\" if not pd.isna(val) else \"0%\" for val in patch_data['return'].values]\n",
    "                characteristics.append(f\"Returns: {', '.join(returns)}\")\n",
    "        \n",
    "        if stage >= 7:\n",
    "            # –≠—Ç–∞–ø 7+: –¥–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–≤—ã–±–æ—Ä–æ—á–Ω–æ)\n",
    "            tsfresh_features = [col for col in available_features if 'value__' in col]\n",
    "            if tsfresh_features:\n",
    "                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –¥–æ—Å—Ç—É–ø–Ω—ã–π TSFresh –ø—Ä–∏–∑–Ω–∞–∫\n",
    "                feature = tsfresh_features[0]\n",
    "                vals = [f\"{val:.3f}\" if not pd.isna(val) else \"0\" for val in patch_data[feature].values]\n",
    "                feature_name = feature.replace('value__', '').replace('_', ' ')[:20]\n",
    "                characteristics.append(f\"Stat feature: {', '.join(vals)}\")\n",
    "        \n",
    "        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ –≤ –æ–¥–Ω–æ —Å–æ–æ–±—â–µ–Ω–∏–µ\n",
    "        content = f\"Patch {i+1}: \" + \" | \".join(characteristics)\n",
    "        \n",
    "        msgs.append({\"role\": \"system\", \"content\": content})\n",
    "    \n",
    "    return msgs\n",
    "\n",
    "def parse_llm_response(response_text):\n",
    "    \"\"\"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∏–∑ –æ—Ç–≤–µ—Ç–∞ LLM\"\"\"\n",
    "    try:\n",
    "        # –ü–æ–∏—Å–∫ –ø–µ—Ä–≤–æ–≥–æ —á–∏—Å–ª–∞ –≤ —Ç–µ–∫—Å—Ç–µ\n",
    "        match = re.search(r\"[-+]?\\d*\\.?\\d+\", str(response_text).strip())\n",
    "        if match:\n",
    "            return float(match.group(0))\n",
    "        else:\n",
    "            return np.nan\n",
    "    except (ValueError, AttributeError):\n",
    "        return np.nan\n",
    "\n",
    "print(\"–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø–∞—Ç—á–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ö–ª–∞—Å—Å OpenRouterLLMPredictor –æ–ø—Ä–µ–¥–µ–ª–µ–Ω!\n"
     ]
    }
   ],
   "source": [
    "class OpenRouterLLMPredictor:\n",
    "    \"\"\"–ö–ª–∞—Å—Å –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —á–µ—Ä–µ–∑ OpenRouter API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model_name=\"openai/gpt-4o-mini\", max_retries=3, drop_threshold=0.20):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://github.com/\", \n",
    "            \"X-Title\": \"Financial Time Series Forecasting\"\n",
    "        }\n",
    "        self.max_retries = max_retries\n",
    "        self.drop_threshold = drop_threshold\n",
    "    \n",
    "    def predict(self, window_data, stage, window_size=10):\n",
    "        \"\"\"–ü—Ä–æ–≥–Ω–æ–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞\"\"\"\n",
    "        \n",
    "        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç\n",
    "        prompt = create_llm_prompt(window_data, stage, window_size)\n",
    "        if prompt is None:\n",
    "            return np.nan\n",
    "        \n",
    "        prev_price = window_data['close'].iloc[-1]\n",
    "        attempt = 0\n",
    "        \n",
    "        while attempt < self.max_retries:\n",
    "            attempt += 1\n",
    "            \n",
    "            try:\n",
    "                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è\n",
    "                messages = [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"You are a financial time series forecaster. \"\n",
    "                            \"When asked to predict, return exactly one numeric value \"\n",
    "                            \"and nothing else‚Äîno explanations, no units, no commentary.\"\n",
    "                        )\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": prompt\n",
    "                    }\n",
    "                ]\n",
    "                \n",
    "                # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –ø—Ä–æ–±–ª–µ–º–µ\n",
    "                if attempt > 1:\n",
    "                    messages.insert(1, {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"Note: Previous prediction was unrealistic. \"\n",
    "                            \"Please reconsider market trends and provide a more plausible prediction. \"\n",
    "                            \"Return only the next price as a single number.\"\n",
    "                        )\n",
    "                    })\n",
    "                \n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=self.headers,\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"messages\": messages,\n",
    "                        \"max_tokens\": 50,\n",
    "                        \"temperature\": 0.1\n",
    "                    },\n",
    "                    timeout=30\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç\n",
    "                response_data = response.json()\n",
    "                content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                pred = parse_llm_response(content)\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "                if np.isnan(pred):\n",
    "                    print(f\"    –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ '{content}'\")\n",
    "                    if attempt >= self.max_retries:\n",
    "                        return prev_price\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–∏–ª—å–Ω–æ–µ –∑–∞–Ω–∏–∂–µ–Ω–∏–µ/–∑–∞–≤—ã—à–µ–Ω–∏–µ\n",
    "                if pred < prev_price * (1 - self.drop_threshold) or pred > prev_price * (1 + self.drop_threshold):\n",
    "                    print(f\"    –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –∞–Ω–æ–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ {pred:.2f} (–ø—Ä–µ–¥—ã–¥—É—â–∞—è —Ü–µ–Ω–∞ {prev_price:.2f})\")\n",
    "                    if attempt < self.max_retries:\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                        if pred < prev_price * (1 - self.drop_threshold):\n",
    "                            return prev_price * (1 - self.drop_threshold)\n",
    "                        else:\n",
    "                            return prev_price * (1 + self.drop_threshold)\n",
    "                \n",
    "                return pred\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    –û—à–∏–±–∫–∞ –ø–æ–ø—ã—Ç–∫–∏ {attempt}: {e}\")\n",
    "                if attempt >= self.max_retries:\n",
    "                    return prev_price\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return prev_price\n",
    "\n",
    "print(\"–ö–ª–∞—Å—Å OpenRouterLLMPredictor –æ–ø—Ä–µ–¥–µ–ª–µ–Ω!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§—É–Ω–∫—Ü–∏—è evaluate_model_for_ticker_llm –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_for_ticker_llm(df, ticker, stage, predictor, window_size=20):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç LLM –º–æ–¥–µ–ª—å –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —ç—Ç–∞–ø–µ\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \"—Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\" –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞\n",
    "        ticker: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞\n",
    "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-7)\n",
    "        predictor: —ç–∫–∑–µ–º–ø–ª—è—Ä OpenRouterLLMPredictor\n",
    "        window_size: —Ä–∞–∑–º–µ—Ä –æ–∫–Ω–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "    \n",
    "    Returns:\n",
    "        results: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
    "        feature_columns = prepare_features_for_stage(df, stage)\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "        available_features = [col for col in feature_columns if col in df.columns]\n",
    "        \n",
    "        if len(available_features) == 0:\n",
    "            print(f\"  - –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–∫—Ä–æ–º–µ daily_headlines)\n",
    "        numeric_features = [f for f in available_features if f != 'daily_headlines']\n",
    "        df_clean = df[available_features].copy()\n",
    "        \n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "        for feature in numeric_features:\n",
    "            if feature in df_clean.columns:\n",
    "                df_clean[feature] = df_clean[feature].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        if len(df_clean) < TEST_SIZE + window_size:\n",
    "            print(f\"  - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
    "        predictions = []\n",
    "        actual_values = []\n",
    "        \n",
    "        start_idx = len(df_clean) - TEST_SIZE\n",
    "        \n",
    "        for i in range(FORECAST_HORIZON):\n",
    "            current_idx = start_idx + i\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–∫–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "            window_start = max(0, current_idx - window_size)\n",
    "            window_data = df_clean.iloc[window_start:current_idx]\n",
    "            \n",
    "            if len(window_data) < 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ\n",
    "                break\n",
    "            \n",
    "            # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "            pred = predictor.predict(window_data, stage, window_size=min(10, len(window_data)))\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "            if current_idx < len(df_clean):\n",
    "                actual = df_clean.iloc[current_idx]['close']\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                actual_values.append(actual)\n",
    "                \n",
    "                print(f\"    –î–µ–Ω—å {i+1}: –ø—Ä–æ–≥–Ω–æ–∑={pred:.2f}, —Ñ–∞–∫—Ç={actual:.2f}, –æ—à–∏–±–∫–∞={abs(pred-actual)/actual*100:.1f}%\")\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(f\"  - –ü—É—Å—Ç—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã\n",
    "        predicted_values = np.array(predictions)\n",
    "        actual_values = np.array(actual_values)\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        valid_mask = ~(np.isnan(predicted_values) | np.isnan(actual_values))\n",
    "        predicted_values = predicted_values[valid_mask]\n",
    "        actual_values = actual_values[valid_mask]\n",
    "        \n",
    "        if len(predicted_values) == 0:\n",
    "            print(f\"  - –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "        rmse_value = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "        mape_value = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
    "        da_value = calculate_directional_accuracy(actual_values, predicted_values)\n",
    "        \n",
    "        results = {\n",
    "            'ticker': ticker,\n",
    "            'stage': stage,\n",
    "            'rmse': rmse_value,\n",
    "            'mape': mape_value,\n",
    "            'da': da_value,\n",
    "            'feature_count': len(available_features),\n",
    "            'predictions_count': len(predicted_values)\n",
    "        }\n",
    "        \n",
    "        print(f\"  - {ticker}: RMSE={rmse_value:.4f}, MAPE={mape_value:.2f}%, DA={da_value:.3f}, Features={len(available_features)}, Predictions={len(predicted_values)}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - –û—à–∏–±–∫–∞ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"–§—É–Ω–∫—Ü–∏—è evaluate_model_for_ticker_llm –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–ê–¢–ß–ï–ô\n",
      "==================================================\n",
      "üìä –¢–∏–∫–µ—Ä: SBER, –≠—Ç–∞–ø: 1\n",
      "üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['close']\n",
      "\\nüìù –ü—Ä–∏–º–µ—Ä –ø–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞: 5):\n",
      "================================================================================\n",
      "–ü–∞—Ç—á 1:\n",
      "Patch 1: Prices: 292.46, 299.81, 296.50, 297.99, 300.80\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 2:\n",
      "Patch 2: Prices: 304.80, 300.01, 308.12, 312.25, 310.00\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 3:\n",
      "Patch 3: Prices: 310.16, 316.69, 314.23, 309.41, 307.80\n",
      "----------------------------------------\n",
      "================================================================================\n",
      "\\n--------------------------------------------------------------------------------\\n\n",
      "üìä –¢–∏–∫–µ—Ä: SBER, –≠—Ç–∞–ø: 3\n",
      "üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['close', 'anomaly', 'weighted_score_with_decay']\n",
      "\\nüìù –ü—Ä–∏–º–µ—Ä –ø–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞: 5):\n",
      "================================================================================\n",
      "–ü–∞—Ç—á 1:\n",
      "Patch 1: Prices: 292.46, 299.81, 296.50, 297.99, 300.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 0, 0, 0, -39\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 2:\n",
      "Patch 2: Prices: 304.80, 300.01, 308.12, 312.25, 310.00 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): -46, -72, 0, 71, 0\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 3:\n",
      "Patch 3: Prices: 310.16, 316.69, 314.23, 309.41, 307.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 94, 121, 39, 0\n",
      "----------------------------------------\n",
      "================================================================================\n",
      "\\n--------------------------------------------------------------------------------\\n\n",
      "üìä –¢–∏–∫–µ—Ä: SBER, –≠—Ç–∞–ø: 6\n",
      "üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['close', 'anomaly', 'weighted_score_with_decay', 'open', 'high', 'low', 'volume', 'return', 'SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', 'MACD_diff', 'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
      "\\nüìù –ü—Ä–∏–º–µ—Ä –ø–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞: 5):\n",
      "================================================================================\n",
      "–ü–∞—Ç—á 1:\n",
      "Patch 1: Prices: 292.46, 299.81, 296.50, 297.99, 300.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 0, 0, 0, -39 | Open prices: 294.09, 293.00, 301.49, 296.51, 298.00 | High prices: 296.75, 300.48, 302.40, 300.45, 303.30 | Low prices: 288.27, 292.80, 295.13, 295.60, 296.06 | Volumes: 6286284, 5041821, 3476895, 2425871, 3010350 | RSI: 42.1, 48.5, 45.7, 47.1, 49.8 | MACD: -6.616, -5.743, -3.902, -3.548, -3.007 | Returns: -0.3%, 2.5%, -1.7%, 0.5%, 0.9%\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 2:\n",
      "Patch 2: Prices: 304.80, 300.01, 308.12, 312.25, 310.00 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): -46, -72, 0, 71, 0 | Open prices: 300.89, 302.70, 302.67, 308.50, 313.00 | High prices: 305.07, 303.85, 308.70, 315.00, 314.98 | Low prices: 300.41, 296.25, 301.76, 306.55, 305.55 | Volumes: 3757980, 4614792, 5126013, 6448201, 5885119 | RSI: 53.4, 48.9, 55.7, 58.7, 56.5 | MACD: -2.229, -1.977, -1.109, -0.088, 0.534 | Returns: 1.3%, -1.6%, 2.7%, 1.3%, -0.7%\n",
      "----------------------------------------\n",
      "–ü–∞—Ç—á 3:\n",
      "Patch 3: Prices: 310.16, 316.69, 314.23, 309.41, 307.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 94, 121, 39, 0 | Open prices: 310.77, 310.85, 318.36, 314.99, 309.28 | High prices: 312.80, 317.87, 320.00, 315.45, 309.95 | Low prices: 309.09, 310.71, 313.52, 308.10, 303.12 | Volumes: 2412000, 5176357, 5231579, 3024999, 4126629 | RSI: 56.6, 61.6, 57.8, 52.4, 50.8 | MACD: 1.028, 1.924, 3.404, 3.076, 2.656 | Returns: 0.1%, 2.1%, -1.2%, -1.5%, -0.5%\n",
      "----------------------------------------\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'role': 'system',\n",
       "  'content': 'Patch 1: Prices: 292.46, 299.81, 296.50, 297.99, 300.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 0, 0, 0, -39 | Open prices: 294.09, 293.00, 301.49, 296.51, 298.00 | High prices: 296.75, 300.48, 302.40, 300.45, 303.30 | Low prices: 288.27, 292.80, 295.13, 295.60, 296.06 | Volumes: 6286284, 5041821, 3476895, 2425871, 3010350 | RSI: 42.1, 48.5, 45.7, 47.1, 49.8 | MACD: -6.616, -5.743, -3.902, -3.548, -3.007 | Returns: -0.3%, 2.5%, -1.7%, 0.5%, 0.9%'},\n",
       " {'role': 'system',\n",
       "  'content': 'Patch 2: Prices: 304.80, 300.01, 308.12, 312.25, 310.00 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): -46, -72, 0, 71, 0 | Open prices: 300.89, 302.70, 302.67, 308.50, 313.00 | High prices: 305.07, 303.85, 308.70, 315.00, 314.98 | Low prices: 300.41, 296.25, 301.76, 306.55, 305.55 | Volumes: 3757980, 4614792, 5126013, 6448201, 5885119 | RSI: 53.4, 48.9, 55.7, 58.7, 56.5 | MACD: -2.229, -1.977, -1.109, -0.088, 0.534 | Returns: 1.3%, -1.6%, 2.7%, 1.3%, -0.7%'},\n",
       " {'role': 'system',\n",
       "  'content': 'Patch 3: Prices: 310.16, 316.69, 314.23, 309.41, 307.80 | Anomaly (0-no, 1-yes): 0, 0, 0, 0, 0 | News sentiment (-100 to +100): 0, 94, 121, 39, 0 | Open prices: 310.77, 310.85, 318.36, 314.99, 309.28 | High prices: 312.80, 317.87, 320.00, 315.45, 309.95 | Low prices: 309.09, 310.71, 313.52, 308.10, 303.12 | Volumes: 2412000, 5176357, 5231579, 3024999, 4126629 | RSI: 56.6, 61.6, 57.8, 52.4, 50.8 | MACD: 1.028, 1.924, 3.404, 3.076, 2.656 | Returns: 0.1%, 2.1%, -1.2%, -1.5%, -0.5%'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–∞—Ç—á–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞\n",
    "def test_patch_example(ticker='SBER', stage=3, patch_size=5):\n",
    "    \"\"\"–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ø—Ä–∏–º–µ—Ä –ø–∞—Ç—á–µ–π –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\"\"\"\n",
    "    if ticker not in data:\n",
    "        print(f\"–¢–∏–∫–µ—Ä {ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "        return\n",
    "    \n",
    "    df = data[ticker]\n",
    "    feature_columns = prepare_features_for_stage(df, stage)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    print(f\"üìä –¢–∏–∫–µ—Ä: {ticker}, –≠—Ç–∞–ø: {stage}\")\n",
    "    print(f\"üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {available_features}\")\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö (–∫—Ä–æ–º–µ daily_headlines)\n",
    "    numeric_features = [f for f in available_features if f != 'daily_headlines']\n",
    "    df_clean = df[available_features].copy()\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "    for feature in numeric_features:\n",
    "        if feature in df_clean.columns:\n",
    "            df_clean[feature] = df_clean[feature].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ 15 –¥–Ω–µ–π –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
    "    window_data = df_clean.tail(15)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ç—á–∏\n",
    "    patches = make_patch_messages(window_data, stage, patch_size)\n",
    "    \n",
    "    print(f\"\\\\nüìù –ü—Ä–∏–º–µ—Ä –ø–∞—Ç—á–µ–π (—Ä–∞–∑–º–µ—Ä –ø–∞—Ç—á–∞: {patch_size}):\")\n",
    "    print(\"=\" * 80)\n",
    "    for i, patch in enumerate(patches):\n",
    "        print(f\"–ü–∞—Ç—á {i+1}:\")\n",
    "        print(patch['content'])\n",
    "        print(\"-\" * 40)\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return patches\n",
    "\n",
    "# –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ —ç—Ç–∞–ø—ã\n",
    "print(\"üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–ê–¢–ß–ï–ô\")\n",
    "print(\"=\"*50)\n",
    "test_patch_example('SBER', 1, 5)\n",
    "print(\"\\\\n\" + \"-\"*80 + \"\\\\n\")\n",
    "test_patch_example('SBER', 3, 5)\n",
    "print(\"\\\\n\" + \"-\"*80 + \"\\\\n\")\n",
    "test_patch_example('SBER', 6, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –û–î–ò–ù–û–ß–ù–û–ì–û –ü–†–û–ì–ù–û–ó–ê\n",
      "==================================================\n",
      "üìä –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è SBER, –≠—Ç–∞–ø: 1\n",
      "üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['close']\n",
      "üí∞ –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Ü–µ–Ω–∞: 309.41\n",
      "üí∞ –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞: 307.80\n",
      "üìà –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: -0.52%\n",
      "\\nü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM...\n",
      "üéØ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è —Ü–µ–Ω–∞: 312.50\n",
      "üìä –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: +1.00%\n",
      "‚ùå –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: 1.53%\n",
      "üß≠ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ–∞–∫—Ç=–ø–∞–¥–µ–Ω–∏–µ, –ø—Ä–æ–≥–Ω–æ–∑=—Ä–æ—Å—Ç (‚ùå)\n",
      "\\n--------------------------------------------------\\n\n",
      "üìä –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è SBER, –≠—Ç–∞–ø: 7\n",
      "üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: ['close', 'anomaly', 'weighted_score_with_decay', 'open', 'high', 'low', 'volume', 'SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', 'MACD_diff', 'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP', 'value__mean']\n",
      "üí∞ –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Ü–µ–Ω–∞: 309.41\n",
      "üí∞ –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞: 307.80\n",
      "üìà –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: -0.52%\n",
      "\\nü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM...\n",
      "üéØ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è —Ü–µ–Ω–∞: 311.50\n",
      "üìä –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: +0.68%\n",
      "‚ùå –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: 1.20%\n",
      "üß≠ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ–∞–∫—Ç=–ø–∞–¥–µ–Ω–∏–µ, –ø—Ä–æ–≥–Ω–æ–∑=—Ä–æ—Å—Ç (‚ùå)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "311.5"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# –¢–µ—Å—Ç –æ–¥–∏–Ω–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
    "def test_single_prediction(ticker='SBER', stage=3, patch=3):\n",
    "    \"\"\"–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏\"\"\"\n",
    "    if ticker not in data:\n",
    "        print(f\"–¢–∏–∫–µ—Ä {ticker} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "        return\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä\n",
    "    test_predictor = OpenRouterPredictor(\n",
    "        api_key=API_KEYS['openrouter'],\n",
    "        model_name=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "        max_retries=1,\n",
    "        drop_threshold=0.15\n",
    "    )\n",
    "    \n",
    "    df = data[ticker]\n",
    "    feature_columns = prepare_features_for_stage(df, stage)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    print(f\"üìä –¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {ticker}, –≠—Ç–∞–ø: {stage}\")\n",
    "    print(f\"üîß –î–æ—Å—Ç—É–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {available_features}\")\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "    numeric_features = [f for f in available_features if f != 'daily_headlines']\n",
    "    df_clean = df[available_features].copy()\n",
    "    \n",
    "    # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "    for feature in numeric_features:\n",
    "        if feature in df_clean.columns:\n",
    "            df_clean[feature] = df_clean[feature].fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # –ë–µ—Ä–µ–º –æ–∫–Ω–æ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–Ω–µ–π, –∏—Å–∫–ª—é—á–∞—è –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å)\n",
    "    window_data = df_clean.iloc[-21:-1]  # 20 –¥–Ω–µ–π\n",
    "    actual_price = df_clean.iloc[-1]['close']  # –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –¥–Ω—è\n",
    "    prev_price = window_data.iloc[-1]['close']  # –¶–µ–Ω–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–Ω—è\n",
    "    \n",
    "    print(f\"üí∞ –ü—Ä–µ–¥—ã–¥—É—â–∞—è —Ü–µ–Ω–∞: {prev_price:.2f}\")\n",
    "    print(f\"üí∞ –§–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ü–µ–Ω–∞: {actual_price:.2f}\")\n",
    "    print(f\"üìà –§–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {((actual_price - prev_price) / prev_price * 100):+.2f}%\")\n",
    "    \n",
    "    # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "    print(\"\\\\nü§ñ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ LLM...\")\n",
    "    predicted_price = test_predictor.predict(window_data, stage, 1)\n",
    "    \n",
    "    if not np.isnan(predicted_price):\n",
    "        error_pct = abs(predicted_price - actual_price) / actual_price * 100\n",
    "        predicted_change = (predicted_price - prev_price) / prev_price * 100\n",
    "        \n",
    "        print(f\"üéØ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–∞—è —Ü–µ–Ω–∞: {predicted_price:.2f}\")\n",
    "        print(f\"üìä –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ: {predicted_change:+.2f}%\") \n",
    "        print(f\"‚ùå –ê–±—Å–æ–ª—é—Ç–Ω–∞—è –æ—à–∏–±–∫–∞: {error_pct:.2f}%\")\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
    "        actual_direction = \"—Ä–æ—Å—Ç\" if actual_price > prev_price else \"–ø–∞–¥–µ–Ω–∏–µ\"\n",
    "        predicted_direction = \"—Ä–æ—Å—Ç\" if predicted_price > prev_price else \"–ø–∞–¥–µ–Ω–∏–µ\"\n",
    "        direction_correct = actual_direction == predicted_direction\n",
    "        \n",
    "        print(f\"üß≠ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: —Ñ–∞–∫—Ç={actual_direction}, –ø—Ä–æ–≥–Ω–æ–∑={predicted_direction} ({'‚úÖ' if direction_correct else '‚ùå'})\")\n",
    "    else:\n",
    "        print(\"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑\")\n",
    "    \n",
    "    return predicted_price\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "print(\"üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø –û–î–ò–ù–û–ß–ù–û–ì–û –ü–†–û–ì–ù–û–ó–ê\")\n",
    "print(\"=\"*50)\n",
    "test_single_prediction('SBER', 1)\n",
    "print(\"\\\\n\" + \"-\"*50 + \"\\\\n\")\n",
    "test_single_prediction('SBER', 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_llm_ticker(df, ticker, stage, predictor, window_size=20, patch_size=5):\n",
    "    \"\"\"\n",
    "    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç LLM –º–æ–¥–µ–ª—å –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —ç—Ç–∞–ø–µ\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \"—Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\" —Å –ø–∞—Ç—á–∞–º–∏\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
    "        feature_columns = prepare_features_for_stage(df, stage)\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "        available_features = [col for col in feature_columns if col in df.columns]\n",
    "        \n",
    "        if len(available_features) == 0:\n",
    "            print(f\"  - –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "        numeric_features = [f for f in available_features if f != 'daily_headlines']\n",
    "        df_clean = df[available_features].copy()\n",
    "        \n",
    "        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "        for feature in numeric_features:\n",
    "            if feature in df_clean.columns:\n",
    "                df_clean[feature] = df_clean[feature].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        if len(df_clean) < TEST_SIZE + window_size:\n",
    "            print(f\"  - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
    "        predictions = []\n",
    "        actual_values = []\n",
    "        \n",
    "        start_idx = len(df_clean) - TEST_SIZE\n",
    "        \n",
    "        for i in range(FORECAST_HORIZON):\n",
    "            current_idx = start_idx + i\n",
    "            \n",
    "            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ–∫–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "            window_start = max(0, current_idx - window_size)\n",
    "            window_data = df_clean.iloc[window_start:current_idx]\n",
    "            \n",
    "            if len(window_data) < 5:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –æ–∫–Ω–æ\n",
    "                break\n",
    "            \n",
    "            # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "            pred = predictor.predict(window_data, stage, patch_size=patch_size)\n",
    "            \n",
    "            # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "            if current_idx < len(df_clean):\n",
    "                actual = df_clean.iloc[current_idx]['close']\n",
    "                \n",
    "                predictions.append(pred)\n",
    "                actual_values.append(actual)\n",
    "                \n",
    "                print(f\"    –î–µ–Ω—å {i+1}: –ø—Ä–æ–≥–Ω–æ–∑={pred:.2f}, —Ñ–∞–∫—Ç={actual:.2f}, –æ—à–∏–±–∫–∞={abs(pred-actual)/actual*100:.1f}%\")\n",
    "            else:\n",
    "                break\n",
    "        \n",
    "        if len(predictions) == 0:\n",
    "            print(f\"  - –ü—É—Å—Ç—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ numpy –º–∞—Å—Å–∏–≤—ã\n",
    "        predicted_values = np.array(predictions)\n",
    "        actual_values = np.array(actual_values)\n",
    "        \n",
    "        # –£–±–∏—Ä–∞–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        valid_mask = ~(np.isnan(predicted_values) | np.isnan(actual_values))\n",
    "        predicted_values = predicted_values[valid_mask]\n",
    "        actual_values = actual_values[valid_mask]\n",
    "        \n",
    "        if len(predicted_values) == 0:\n",
    "            print(f\"  - –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "            return None\n",
    "        \n",
    "        # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "        rmse_value = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "        mape_value = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
    "        da_value = calculate_directional_accuracy(actual_values, predicted_values)\n",
    "        \n",
    "        results = {\n",
    "            'ticker': ticker,\n",
    "            'stage': stage,\n",
    "            'rmse': rmse_value,\n",
    "            'mape': mape_value,\n",
    "            'da': da_value,\n",
    "            'feature_count': len(available_features),\n",
    "            'predictions_count': len(predicted_values)\n",
    "        }\n",
    "        \n",
    "        print(f\"  - {ticker}: RMSE={rmse_value:.4f}, MAPE={mape_value:.2f}%, DA={da_value:.3f}, Features={len(available_features)}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - –û—à–∏–±–∫–∞ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"–§—É–Ω–∫—Ü–∏—è evaluate_llm_ticker –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üöÄ –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –ø–æ –∑–∞–ø—É—Å–∫—É –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "\n",
    "### –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞:\n",
    "1. **–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ API –∫–ª—é—á OpenRouter –Ω–∞—Å—Ç—Ä–æ–µ–Ω** –≤ —è—á–µ–π–∫–µ —Å `API_KEYS`\n",
    "2. **–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã** - –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã multivariate —Ñ–∞–π–ª—ã –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤\n",
    "3. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –ø—Ä–æ–º–ø—Ç—ã** - –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É —Å `test_prompt_example()` —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å, –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —ç—Ç–∞–ø–æ–≤\n",
    "\n",
    "### –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ:\n",
    "```python\n",
    "# –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ –ø–µ—Ä–µ–¥ –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º\n",
    "test_single_prediction('SBER', 1)  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
    "test_single_prediction('SBER', 3)  # –° –Ω–æ–≤–æ—Å—Ç—è–º–∏\n",
    "```\n",
    "\n",
    "### –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n",
    "- –ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Ü–∏–∫–ª–æ–º –∞–Ω–∞–ª–∏–∑–∞\n",
    "- **–í–Ω–∏–º–∞–Ω–∏–µ**: –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–∂–µ—Ç –∑–∞–Ω—è—Ç—å **2-3 —á–∞—Å–∞** (7 —ç—Ç–∞–ø–æ–≤ √ó 10 —Ç–∏–∫–µ—Ä–æ–≤ √ó 10 –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ √ó 2 —Å–µ–∫—É–Ω–¥—ã = ~2300 –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API)\n",
    "- –î–ª—è —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –≤—Ä–µ–º–µ–Ω–∏ –º–æ–∂–Ω–æ:\n",
    "  - –£–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤ –≤ —Å–ø–∏—Å–∫–µ `tickers`\n",
    "  - –£–º–µ–Ω—å—à–∏—Ç—å `FORECAST_HORIZON` (—Å–µ–π—á–∞—Å 10 –¥–Ω–µ–π)\n",
    "  - –£–≤–µ–ª–∏—á–∏—Ç—å –ø–∞—É–∑—É `time.sleep()` –µ—Å–ª–∏ –ø–æ–ª—É—á–∞–µ—Ç–µ –æ—à–∏–±–∫–∏ rate limit\n",
    "\n",
    "### –≠—Ç–∞–ø—ã –∞–Ω–∞–ª–∏–∑–∞:\n",
    "1. **–≠—Ç–∞–ø 1**: –¢–æ–ª—å–∫–æ —Ü–µ–Ω—ã –∑–∞–∫—Ä—ã—Ç–∏—è (baseline)\n",
    "2. **–≠—Ç–∞–ø 2**: + –ò–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã –∞–Ω–æ–º–∞–ª–∏–π\n",
    "3. **–≠—Ç–∞–ø 3**: + –ù–æ–≤–æ—Å—Ç–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (—á–∏—Å–ª–æ–≤–æ–π —Å–∫–æ—Ä)\n",
    "4. **–≠—Ç–∞–ø 4**: + –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–µ–π (–∑–∞–≥–æ–ª–æ–≤–∫–∏)\n",
    "5. **–≠—Ç–∞–ø 5**: + OHLV –¥–∞–Ω–Ω—ã–µ (—Å–≤–µ—á–∏)\n",
    "6. **–≠—Ç–∞–ø 6**: + –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã (SMA, RSI, MACD –∏ —Ç.–¥.)\n",
    "7. **–≠—Ç–∞–ø 7**: + TSFresh —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "\n",
    "### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "- –ë—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫—É `./progressive_analysis/`\n",
    "- –ì—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "- CSV —Ñ–∞–π–ª—ã —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏\n",
    "- –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å –ø–∞—Ç—á–∞–º–∏\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤\n",
    "stage_names = {\n",
    "    1: \"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (close)\",\n",
    "    2: \"+ –ê–Ω–æ–º–∞–ª–∏–∏\", \n",
    "    3: \"+ –ù–æ–≤–æ—Å—Ç–∏ (–æ—Ü–µ–Ω–∫–∞)\",\n",
    "    4: \"+ –ù–æ–≤–æ—Å—Ç–∏ (—Ç–µ–∫—Å—Ç)\",\n",
    "    5: \"+ –°–≤–µ—á–∏ (OHLV)\",\n",
    "    6: \"+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
    "    7: \"+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\"\n",
    "}\n",
    "\n",
    "# –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "all_results = []\n",
    "stage_summaries = []\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å LLM (–ø–∞—Ç—á–∏)\\\\n\")\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —ç—Ç–∞–ø–∞–º\n",
    "for stage in range(1, 8):  # 7 —ç—Ç–∞–ø–æ–≤\n",
    "    print(f\"üìä –≠–¢–ê–ü {stage}: {stage_names[stage]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä –¥–ª—è —ç—Ç–æ–≥–æ —ç—Ç–∞–ø–∞\n",
    "    predictor = OpenRouterPredictor(\n",
    "        api_key=API_KEYS['openrouter'],\n",
    "        model_name=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "        max_retries=2,\n",
    "        drop_threshold=0.15\n",
    "    )\n",
    "    \n",
    "    stage_results = []\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–∫–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ\n",
    "    for ticker in tickers:\n",
    "        if ticker in data:\n",
    "            print(f\"\\\\n  –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {ticker}...\")\n",
    "            result = evaluate_llm_ticker(data[ticker], ticker, stage, predictor, window_size=20, patch_size=5)\n",
    "            if result is not None:\n",
    "                all_results.append(result)\n",
    "                stage_results.append(result)\n",
    "            \n",
    "            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–∞–ø—É\n",
    "    if stage_results:\n",
    "        avg_rmse = np.mean([r['rmse'] for r in stage_results])\n",
    "        avg_mape = np.mean([r['mape'] for r in stage_results])\n",
    "        avg_da = np.mean([r['da'] for r in stage_results if not np.isnan(r['da'])])\n",
    "        avg_features = np.mean([r['feature_count'] for r in stage_results])\n",
    "        \n",
    "        stage_summary = {\n",
    "            'stage': stage,\n",
    "            'stage_name': stage_names[stage],\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mape': avg_mape,\n",
    "            'avg_da': avg_da,\n",
    "            'avg_features': avg_features,\n",
    "            'ticker_count': len(stage_results)\n",
    "        }\n",
    "        \n",
    "        stage_summaries.append(stage_summary)\n",
    "        \n",
    "        print(f\"\\\\nüìà –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ {stage}:\")\n",
    "        print(f\"   RMSE: {avg_rmse:.4f}\")\n",
    "        print(f\"   MAPE: {avg_mape:.2f}%\")\n",
    "        print(f\"   DA: {avg_da:.3f}\")\n",
    "        print(f\"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {avg_features:.1f}\")\n",
    "        print(f\"   –£—Å–ø–µ—à–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤: {len(stage_results)}/{len(tickers)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {stage}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(stage_summaries)} —ç—Ç–∞–ø–æ–≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üöÄ –ö–∞–∫ –∑–∞–ø—É—Å—Ç–∏—Ç—å –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "\n",
    "### 1. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "\n",
    "**–°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –ø–∞—Ç—á–∏:**\n",
    "```python\n",
    "# –ó–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É —Å test_patch_example() —á—Ç–æ–±—ã –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –∫–∞–∫ –≤—ã–≥–ª—è–¥—è—Ç –ø–∞—Ç—á–∏\n",
    "```\n",
    "\n",
    "**–ó–∞—Ç–µ–º –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ –æ–¥–∏–Ω–æ—á–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑:**\n",
    "```python\n",
    "# –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —è—á–µ–π–∫–µ test_single_prediction()\n",
    "test_single_prediction('SBER', 1)  # –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å\n",
    "test_single_prediction('SBER', 3)  # –° –Ω–æ–≤–æ—Å—Ç—è–º–∏  \n",
    "```\n",
    "\n",
    "### 2. –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞\n",
    "\n",
    "**–î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —è—á–µ–π–∫—É —Å –æ—Å–Ω–æ–≤–Ω—ã–º —Ü–∏–∫–ª–æ–º**\n",
    "\n",
    "**‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ:**\n",
    "- –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–π–º–µ—Ç **2-3 —á–∞—Å–∞** \n",
    "- –ë—É–¥–µ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–æ ~1400 –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ API (7 —ç—Ç–∞–ø–æ–≤ √ó 10 —Ç–∏–∫–µ—Ä–æ–≤ √ó 10 –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ √ó 2 —Å–µ–∫—É–Ω–¥—ã)\n",
    "- –°—Ç–æ–∏–º–æ—Å—Ç—å: –ø—Ä–∏–º–µ—Ä–Ω–æ $5-10 –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∞—Ä–∏—Ñ–æ–≤ OpenRouter\n",
    "\n",
    "**üîß –î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è:**\n",
    "```python\n",
    "# –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤:\n",
    "tickers = ['SBER', 'MOEX', 'LKOH']  # –¢–æ–ª—å–∫–æ 3 —Ç–∏–∫–µ—Ä–∞\n",
    "\n",
    "# –ò–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞:\n",
    "FORECAST_HORIZON = 5  # –í–º–µ—Å—Ç–æ 10 –¥–Ω–µ–π\n",
    "\n",
    "# –ò–ª–∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π—Ç–µ —Ç–æ–ª—å–∫–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç—Ç–∞–ø–æ–≤:\n",
    "for stage in range(1, 4):  # –¢–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 3 —ç—Ç–∞–ø–∞\n",
    "```\n",
    "\n",
    "### 3. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "\n",
    "–ü–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞:\n",
    "- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—Å—è –≤ `./progressive_analysis/`\n",
    "- –í—ã —É–≤–∏–¥–∏—Ç–µ –∫–∞–∫ –∫–∞–∂–¥—ã–π —Ç–∏–ø –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "- –ì—Ä–∞—Ñ–∏–∫–∏ –ø–æ–∫–∞–∂—É—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–µ RMSE, MAPE –∏ DA –ø–æ —ç—Ç–∞–ø–∞–º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å OpenRouterPredictor –æ–ø—Ä–µ–¥–µ–ª–µ–Ω!\n"
     ]
    }
   ],
   "source": [
    "class OpenRouterPredictor:\n",
    "    \"\"\"–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è LLM –º–æ–¥–µ–ª–µ–π —á–µ—Ä–µ–∑ OpenRouter API\"\"\"\n",
    "    \n",
    "    def __init__(self, api_key, model_name, max_retries=1, drop_threshold=0.20):\n",
    "        self.api_key = api_key\n",
    "        self.model_name = model_name\n",
    "        self.api_url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "        self.headers = {\n",
    "            \"Authorization\": f\"Bearer {api_key}\",\n",
    "            \"Content-Type\": \"application/json\",\n",
    "            \"HTTP-Referer\": \"https://github.com/\", \n",
    "            \"X-Title\": \"Financial Time Series Forecasting\"\n",
    "        }\n",
    "        self.max_retries = max_retries\n",
    "        self.drop_threshold = drop_threshold\n",
    "    \n",
    "    def predict(self, window_data, stage, patch_size=5):\n",
    "        \"\"\"–ü—Ä–æ–≥–Ω–æ–∑ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ–∫–Ω–∞\"\"\"\n",
    "        prev_price = window_data['close'].iloc[-1]\n",
    "        attempt = 0\n",
    "        \n",
    "        while attempt < self.max_retries:\n",
    "            attempt += 1\n",
    "            \n",
    "            try:\n",
    "                # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –ø–∞—Ç—á–∞–º\n",
    "                msgs = make_patch_messages(window_data, stage, patch_size)\n",
    "                \n",
    "                # –î–æ–±–∞–≤–ª—è–µ–º system prompt\n",
    "                msgs.insert(0, {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a financial time series forecaster. \"\n",
    "                        \"When asked to predict, return exactly one numeric value \"\n",
    "                        \"and nothing else‚Äîno explanations, no units, no commentary.\"\n",
    "                    )\n",
    "                })\n",
    "                \n",
    "                # –ï—Å–ª–∏ –Ω–µ –ø–µ—Ä–≤–∞—è –ø–æ–ø—ã—Ç–∫–∞ - –¥–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ –ø—Ä–æ–±–ª–µ–º–µ\n",
    "                if attempt > 1:\n",
    "                    msgs.insert(1, {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": (\n",
    "                            \"Note: Previous prediction was unrealistic. \"\n",
    "                            \"Please reconsider market trends and provide a more plausible prediction. \"\n",
    "                            \"Return only the next price as a single number.\"\n",
    "                        )\n",
    "                    })\n",
    "                \n",
    "                # User prompt\n",
    "                msgs.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Based on these financial characteristics, predict the next closing price value. Output only the next closing price as a number.\"\n",
    "                })\n",
    "                \n",
    "                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å\n",
    "                response = requests.post(\n",
    "                    self.api_url,\n",
    "                    headers=self.headers,\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"messages\": msgs,\n",
    "                        \"max_tokens\": 50,\n",
    "                        \"temperature\": 0.1\n",
    "                    },\n",
    "                    timeout=30\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                # –ü–∞—Ä—Å–∏–º –æ—Ç–≤–µ—Ç\n",
    "                response_data = response.json()\n",
    "                content = response_data[\"choices\"][0][\"message\"][\"content\"]\n",
    "                pred = parse_llm_response(content)\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∞–Ω–æ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "                if np.isnan(pred):\n",
    "                    print(f\"    –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –Ω–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —á–∏—Å–ª–æ –∏–∑ '{content}'\")\n",
    "                    if attempt >= self.max_retries:\n",
    "                        return prev_price\n",
    "                    time.sleep(1)\n",
    "                    continue\n",
    "                \n",
    "                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–∏–ª—å–Ω–æ–µ –∑–∞–Ω–∏–∂–µ–Ω–∏–µ/–∑–∞–≤—ã—à–µ–Ω–∏–µ\n",
    "                if pred < prev_price * (1 - self.drop_threshold) or pred > prev_price * (1 + self.drop_threshold):\n",
    "                    print(f\"    –ü–æ–ø—ã—Ç–∫–∞ {attempt}: –∞–Ω–æ–º–∞–ª—å–Ω—ã–π –ø—Ä–æ–≥–Ω–æ–∑ {pred:.2f} (–ø—Ä–µ–¥—ã–¥—É—â–∞—è —Ü–µ–Ω–∞ {prev_price:.2f})\")\n",
    "                    if attempt < self.max_retries:\n",
    "                        time.sleep(1)\n",
    "                        continue\n",
    "                    else:\n",
    "                        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "                        if pred < prev_price * (1 - self.drop_threshold):\n",
    "                            return prev_price * (1 - self.drop_threshold)\n",
    "                        else:\n",
    "                            return prev_price * (1 + self.drop_threshold)\n",
    "                \n",
    "                return pred\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    –û—à–∏–±–∫–∞ –ø–æ–ø—ã—Ç–∫–∏ {attempt}: {e}\")\n",
    "                if attempt >= self.max_retries:\n",
    "                    return prev_price\n",
    "                time.sleep(1)\n",
    "        \n",
    "        return prev_price\n",
    "\n",
    "print(\"–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å OpenRouterPredictor –æ–ø—Ä–µ–¥–µ–ª–µ–Ω!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_ticker_llm():\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_for_ticker(df, ticker, stage):\n",
    "    \"\"\"\n",
    "    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –µ—ë –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —ç—Ç–∞–ø–µ\n",
    "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \"—Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\" –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞\n",
    "        ticker: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞\n",
    "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-7)\n",
    "    \n",
    "    Returns:\n",
    "        results: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    \"\"\"\n",
    "    \n",
    "    #try:\n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
    "    feature_columns = prepare_features_for_stage(df, stage)\n",
    "    \n",
    "    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
    "    available_features = [col for col in feature_columns if col in df.columns]\n",
    "    \n",
    "    if len(available_features) == 0:\n",
    "        print(f\"  - –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "        return None\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
    "    df_clean = df[available_features].dropna()\n",
    "    \n",
    "    if len(df_clean) < TEST_SIZE + 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
    "        print(f\"  - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "        return None\n",
    "    \n",
    "    # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è DARTS\n",
    "    if stage == 1:\n",
    "        # Univariate –º–æ–¥–µ–ª—å –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ extending window\n",
    "        ts = TimeSeries.from_dataframe(df_clean, time_col=None, value_cols='close')\n",
    "        current_ts = ts[:-TEST_SIZE]   # –Ω–∞—á–∞–ª—å–Ω–æ–µ –æ–±—É—á–∞—é—â–µ–µ –æ–∫–Ω–æ\n",
    "        predictions = []\n",
    "\n",
    "        for i in range(FORECAST_HORIZON):\n",
    "            # 1) –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ–º current_ts\n",
    "            model = RandomForest(lags=14, random_state=42)\n",
    "            model.fit(current_ts)\n",
    "\n",
    "            # 2) –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 —à–∞–≥ –≤–ø–µ—Ä–µ–¥\n",
    "            pred = model.predict(n=1)\n",
    "            y_pred = pred.values().flatten()[0]\n",
    "            predictions.append(y_pred)\n",
    "\n",
    "            # 3) –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–∫–Ω–æ\n",
    "            next_time = ts.time_index[len(current_ts)]\n",
    "            y_true = ts.values()[len(current_ts)][0]\n",
    "            s = pd.Series([y_true], index=[next_time])\n",
    "            actual_ts = TimeSeries.from_series(\n",
    "                s,\n",
    "                fill_missing_dates=False,\n",
    "                freq=ts.freq\n",
    "            )\n",
    "            current_ts = current_ts.append(actual_ts)\n",
    "\n",
    "        feature_importance = {}\n",
    "        \n",
    "    else:\n",
    "        # Multivariate –º–æ–¥–µ–ª—å\n",
    "        target_col = 'close'\n",
    "        past_covariates_cols = [col for col in available_features if col != target_col]\n",
    "        \n",
    "        if len(past_covariates_cols) == 0:\n",
    "            # Fallback –∫ univariate –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–≤–∞—Ä–∏–∞—Ç\n",
    "            ts = TimeSeries.from_dataframe(df_clean, time_col=None, value_cols=target_col)\n",
    " \n",
    "            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
    "            predictions = []\n",
    "            current_ts = ts[:-TEST_SIZE]\n",
    "            \n",
    "            for i in range(FORECAST_HORIZON):\n",
    "                # 1) –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ–º current_ts\n",
    "                model = RandomForest(lags=14, random_state=42)\n",
    "                model.fit(current_ts)\n",
    "    \n",
    "                # 2) –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 —à–∞–≥ –≤–ø–µ—Ä–µ–¥\n",
    "                pred = model.predict(n=1)\n",
    "                y_pred = pred.values().flatten()[0]\n",
    "                predictions.append(y_pred)\n",
    "    \n",
    "                # 3) –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –æ–∫–Ω–æ\n",
    "                next_time = ts.time_index[len(current_ts)]\n",
    "                y_true = ts.values()[len(current_ts)][0]\n",
    "                s = pd.Series([y_true], index=[next_time])\n",
    "                actual_ts = TimeSeries.from_series(\n",
    "                    s,\n",
    "                    fill_missing_dates=False,\n",
    "                    freq=ts.freq\n",
    "                )\n",
    "                current_ts = current_ts.append(actual_ts)\n",
    "            \n",
    "            feature_importance = {}\n",
    "            \n",
    "        else:\n",
    "            # –°–æ–∑–¥–∞–µ–º TimeSeries –¥–ª—è —Ü–µ–ª–∏ –∏ –∫–æ–≤–∞—Ä–∏–∞—Ç\n",
    "            target_ts = TimeSeries.from_dataframe(df_clean, time_col=None, value_cols=target_col)\n",
    "            past_covariates_ts = TimeSeries.from_dataframe(df_clean, time_col=None, value_cols=past_covariates_cols)\n",
    "\n",
    "            # —á—Ç–æ–±—ã –≤ –∫–æ–Ω—Ü–µ –º–æ–∂–Ω–æ –±—ã–ª–æ —á–∏—Ç–∞—Ç—å actual_values –∏–∑ ts\n",
    "            ts = target_ts\n",
    "            \n",
    "            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
    "            predictions = []\n",
    "            current_target = target_ts[:-TEST_SIZE]\n",
    "            current_covariates = past_covariates_ts[:-TEST_SIZE]\n",
    "            \n",
    "            for i in range(FORECAST_HORIZON):\n",
    "                # 1) –ü–µ—Ä–µ–æ–±—É—á–∞–µ–º multivariate –º–æ–¥–µ–ª—å\n",
    "                model = RandomForest(lags=14, lags_past_covariates=7, random_state=42)\n",
    "                model.fit(series=current_target, past_covariates=current_covariates)\n",
    "    \n",
    "                # 2) –ü—Ä–æ–≥–Ω–æ–∑\n",
    "                pred = model.predict(n=1, past_covariates=current_covariates)\n",
    "                y_pred = pred.values().flatten()[0]\n",
    "                predictions.append(y_pred)\n",
    "    \n",
    "                # 3) –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞\n",
    "                next_t = target_ts.time_index[len(current_target)]\n",
    "                y_true = target_ts.values()[len(current_target)][0]\n",
    "                s_y = pd.Series([y_true], index=[next_t])\n",
    "                actual_y_ts = TimeSeries.from_series(\n",
    "                    s_y, fill_missing_dates=False, freq=target_ts.freq\n",
    "                )\n",
    "                current_target = current_target.append(actual_y_ts)\n",
    "    \n",
    "                # 4) –î–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ–≤–∞—Ä–∏–∞—Ç—ã\n",
    "                next_t = past_covariates_ts.time_index[len(current_covariates)]\n",
    "                x_true = past_covariates_ts.values()[len(current_covariates)].flatten()\n",
    "                df_x = pd.DataFrame([x_true], index=[next_t], columns=past_covariates_cols)\n",
    "                actual_x_ts = TimeSeries.from_dataframe(\n",
    "                    df_x, time_col=None, value_cols=past_covariates_cols,\n",
    "                    fill_missing_dates=False, freq=past_covariates_ts.freq\n",
    "                )\n",
    "                current_covariates = current_covariates.append(actual_x_ts)\n",
    "            \n",
    "            # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "            if hasattr(model.model, 'feature_importances_'):\n",
    "                importance_values = model.model.feature_importances_\n",
    "                # –°–æ–∑–¥–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (lags + past_covariates)\n",
    "                feature_names = []\n",
    "                for lag in range(1, 15):  # lags=14\n",
    "                    feature_names.append(f'{target_col}_lag_{lag}')\n",
    "                for lag in range(1, 8):   # lags_past_covariates=7\n",
    "                    for col in past_covariates_cols:\n",
    "                        feature_names.append(f'{col}_lag_{lag}')\n",
    "                \n",
    "                feature_importance = dict(zip(feature_names[:len(importance_values)], importance_values))\n",
    "            else:\n",
    "                feature_importance = {}\n",
    "    \n",
    "    # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    actual_values = ts[-TEST_SIZE:-TEST_SIZE+FORECAST_HORIZON].values().flatten()\n",
    "    predicted_values = np.array(predictions)\n",
    "    \n",
    "    # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç\n",
    "    min_length = min(len(actual_values), len(predicted_values))\n",
    "    actual_values = actual_values[:min_length]\n",
    "    predicted_values = predicted_values[:min_length]\n",
    "    \n",
    "    if min_length == 0:\n",
    "        print(f\"  - –ü—É—Å—Ç—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
    "        return None\n",
    "    \n",
    "    # RMSE\n",
    "    rmse_value = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
    "    \n",
    "    # MAPE\n",
    "    mape_value = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
    "    \n",
    "    # DA (Directional Accuracy)\n",
    "    da_value = calculate_directional_accuracy(actual_values, predicted_values)\n",
    "    \n",
    "    results = {\n",
    "        'ticker': ticker,\n",
    "        'stage': stage,\n",
    "        'rmse': rmse_value,\n",
    "        'mape': mape_value,\n",
    "        'da': da_value,\n",
    "        'feature_count': len(available_features),\n",
    "        'feature_importance': feature_importance\n",
    "    }\n",
    "    \n",
    "    print(f\"  - {ticker}: RMSE={rmse_value:.4f}, MAPE={mape_value:.2f}%, DA={da_value:.3f}, Features={len(available_features)}\")\n",
    "    \n",
    "    return results\n",
    "        \n",
    "    # except Exception as e:\n",
    "    #     print(f\"  - –û—à–∏–±–∫–∞ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}: {str(e)}\")\n",
    "    #     return None\n",
    "\n",
    "print(\"–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ —ç—Ç–∞–ø–∞–º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –ø—Ä–µ–¥–∏–∫—Ç–æ—Ä\n",
    "predictor = OpenRouterLLMPredictor(\n",
    "    api_key=API_KEYS['openrouter'],\n",
    "    model_name=\"openai/gpt-4o-mini-2024-07-18\",\n",
    "    max_retries=2,\n",
    "    drop_threshold=0.15\n",
    ")\n",
    "\n",
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤\n",
    "stage_names = {\n",
    "    1: \"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (close)\",\n",
    "    2: \"+ –ê–Ω–æ–º–∞–ª–∏–∏\", \n",
    "    3: \"+ –ù–æ–≤–æ—Å—Ç–∏ (–æ—Ü–µ–Ω–∫–∞)\",\n",
    "    4: \"+ –ù–æ–≤–æ—Å—Ç–∏ (—Ç–µ–∫—Å—Ç)\",\n",
    "    5: \"+ –°–≤–µ—á–∏ (OHLV)\",\n",
    "    6: \"+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
    "    7: \"+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\"\n",
    "}\n",
    "\n",
    "# –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "all_results = []\n",
    "stage_summaries = []\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ LLM\\\\n\")\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —ç—Ç–∞–ø–∞–º\n",
    "for stage in range(1, 8):  # 7 —ç—Ç–∞–ø–æ–≤\n",
    "    print(f\"üìä –≠–¢–ê–ü {stage}: {stage_names[stage]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    stage_results = []\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–∫–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ\n",
    "    for ticker in tickers:\n",
    "        if ticker in data:\n",
    "            print(f\"\\\\n  –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {ticker}...\")\n",
    "            result = evaluate_model_for_ticker_llm(data[ticker], ticker, stage, predictor)\n",
    "            if result is not None:\n",
    "                all_results.append(result)\n",
    "                stage_results.append(result)\n",
    "            \n",
    "            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –∫ API\n",
    "            time.sleep(2)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–∞–ø—É\n",
    "    if stage_results:\n",
    "        avg_rmse = np.mean([r['rmse'] for r in stage_results])\n",
    "        avg_mape = np.mean([r['mape'] for r in stage_results])\n",
    "        avg_da = np.mean([r['da'] for r in stage_results if not np.isnan(r['da'])])\n",
    "        avg_features = np.mean([r['feature_count'] for r in stage_results])\n",
    "        \n",
    "        stage_summary = {\n",
    "            'stage': stage,\n",
    "            'stage_name': stage_names[stage],\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mape': avg_mape,\n",
    "            'avg_da': avg_da,\n",
    "            'avg_features': avg_features,\n",
    "            'ticker_count': len(stage_results)\n",
    "        }\n",
    "        \n",
    "        stage_summaries.append(stage_summary)\n",
    "        \n",
    "        print(f\"\\\\nüìà –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ {stage}:\")\n",
    "        print(f\"   RMSE: {avg_rmse:.4f}\")\n",
    "        print(f\"   MAPE: {avg_mape:.2f}%\")\n",
    "        print(f\"   DA: {avg_da:.3f}\")\n",
    "        print(f\"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {avg_features:.1f}\")\n",
    "        print(f\"   –£—Å–ø–µ—à–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤: {len(stage_results)}/{len(tickers)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {stage}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(stage_summaries)} —ç—Ç–∞–ø–æ–≤\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
      "\n",
      "üìä –≠–¢–ê–ü 1: –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (close)\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤\n",
    "stage_names = {\n",
    "    1: \"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (close)\",\n",
    "    2: \"+ –ê–Ω–æ–º–∞–ª–∏–∏\", \n",
    "    3: \"+ –ù–æ–≤–æ—Å—Ç–∏\",\n",
    "    4: \"+ –°–≤–µ—á–∏\",\n",
    "    5: \"+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
    "    6: \"+ PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\",\n",
    "    7: \"+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\", \n",
    "    8: \"+ –ö–∞—Ä—Ç–∏–Ω–∫–∞\",\n",
    "}\n",
    "\n",
    "# –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "all_results = []\n",
    "stage_summaries = []\n",
    "\n",
    "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
    "\n",
    "## TODO: adapt to LLM\n",
    "\n",
    "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —ç—Ç–∞–ø–∞–º\n",
    "for stage in range(1, 9):\n",
    "    print(f\"üìä –≠–¢–ê–ü {stage}: {stage_names[stage]}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    stage_results = []\n",
    "    \n",
    "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–∫–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ\n",
    "    for ticker in tickers:\n",
    "        if ticker in data:\n",
    "            result = evaluate_model_for_ticker_llm(data[ticker], ticker, stage)\n",
    "            if result is not None:\n",
    "                all_results.append(result)\n",
    "                stage_results.append(result)\n",
    "    \n",
    "    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–∞–ø—É\n",
    "    if stage_results:\n",
    "        avg_rmse = np.mean([r['rmse'] for r in stage_results])\n",
    "        avg_mape = np.mean([r['mape'] for r in stage_results])\n",
    "        avg_da = np.mean([r['da'] for r in stage_results if not np.isnan(r['da'])])\n",
    "        avg_features = np.mean([r['feature_count'] for r in stage_results])\n",
    "        \n",
    "        stage_summary = {\n",
    "            'stage': stage,\n",
    "            'stage_name': stage_names[stage],\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'avg_mape': avg_mape,\n",
    "            'avg_da': avg_da,\n",
    "            'avg_features': avg_features,\n",
    "            'ticker_count': len(stage_results)\n",
    "        }\n",
    "        \n",
    "        stage_summaries.append(stage_summary)\n",
    "        \n",
    "        print(f\"\\nüìà –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ {stage}:\")\n",
    "        print(f\"   RMSE: {avg_rmse:.4f}\")\n",
    "        print(f\"   MAPE: {avg_mape:.2f}%\")\n",
    "        print(f\"   DA: {avg_da:.3f}\")\n",
    "        print(f\"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {avg_features:.1f}\")\n",
    "        print(f\"   –£—Å–ø–µ—à–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤: {len(stage_results)}/{len(tickers)}\")\n",
    "    else:\n",
    "        print(f\"‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {stage}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
    "\n",
    "print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(stage_summaries)} —ç—Ç–∞–ø–æ–≤\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "results_df = pd.DataFrame(stage_summaries)\n",
    "\n",
    "if not results_df.empty:\n",
    "    print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    display_df = results_df.copy()\n",
    "    display_df['RMSE'] = display_df['avg_rmse'].apply(lambda x: f\"{x:.4f}\")\n",
    "    display_df['MAPE (%)'] = display_df['avg_mape'].apply(lambda x: f\"{x:.2f}\")\n",
    "    display_df['DA'] = display_df['avg_da'].apply(lambda x: f\"{x:.3f}\")\n",
    "    display_df['–ü—Ä–∏–∑–Ω–∞–∫–æ–≤'] = display_df['avg_features'].apply(lambda x: f\"{x:.0f}\")\n",
    "    display_df['–¢–∏–∫–µ—Ä–æ–≤'] = display_df['ticker_count'].apply(lambda x: f\"{x}\")\n",
    "    \n",
    "    # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "    final_table = display_df[['stage', 'stage_name', 'RMSE', 'MAPE (%)', 'DA', '–ü—Ä–∏–∑–Ω–∞–∫–æ–≤', '–¢–∏–∫–µ—Ä–æ–≤']].copy()\n",
    "    final_table.columns = ['–≠—Ç–∞–ø', '–û–ø–∏—Å–∞–Ω–∏–µ', 'RMSE', 'MAPE (%)', 'DA', '–ü—Ä–∏–∑–Ω–∞–∫–æ–≤', '–¢–∏–∫–µ—Ä–æ–≤']\n",
    "    \n",
    "    print(final_table.to_string(index=False))\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
    "    results_df.to_csv(f\"{OUTPUT_PATH}progressive_analysis_summary.csv\", index=False)\n",
    "    final_table.to_csv(f\"{OUTPUT_PATH}progressive_analysis_formatted.csv\", index=False)\n",
    "    \n",
    "    print(f\"\\\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_PATH}\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
    "    if len(results_df) > 1:\n",
    "        base_rmse = results_df.iloc[0]['avg_rmse']\n",
    "        base_mape = results_df.iloc[0]['avg_mape'] \n",
    "        base_da = results_df.iloc[0]['avg_da']\n",
    "        \n",
    "        print(\"\\\\nüìà –ò–ó–ú–ï–ù–ï–ù–ò–Ø –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–û –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò:\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for i, row in results_df.iterrows():\n",
    "            if i == 0:\n",
    "                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
    "                \n",
    "            rmse_change = ((row['avg_rmse'] - base_rmse) / base_rmse) * 100\n",
    "            mape_change = ((row['avg_mape'] - base_mape) / base_mape) * 100\n",
    "            da_change = ((row['avg_da'] - base_da) / base_da) * 100\n",
    "            \n",
    "            print(f\"–≠—Ç–∞–ø {row['stage']} - {row['stage_name']}:\")\n",
    "            print(f\"  RMSE: {rmse_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if rmse_change < 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "            print(f\"  MAPE: {mape_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if mape_change < 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "            print(f\"  DA: {da_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if da_change > 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "            print()\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results_df.empty:\n",
    "    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('–ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 1: RMSE\n",
    "    axes[0,0].plot(results_df['stage'], results_df['avg_rmse'], 'o-', linewidth=2, markersize=8, color='red')\n",
    "    axes[0,0].set_title('RMSE (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞)', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[0,0].set_ylabel('RMSE')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    axes[0,0].set_xticks(results_df['stage'])\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 2: MAPE\n",
    "    axes[0,1].plot(results_df['stage'], results_df['avg_mape'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "    axes[0,1].set_title('MAPE (—Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞)', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[0,1].set_ylabel('MAPE (%)')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    axes[0,1].set_xticks(results_df['stage'])\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 3: DA\n",
    "    axes[1,0].plot(results_df['stage'], results_df['avg_da'], 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[1,0].set_title('DA (—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[1,0].set_ylabel('DA')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    axes[1,0].set_xticks(results_df['stage'])\n",
    "    axes[1,0].set_ylim(0, 1)\n",
    "    \n",
    "    # –ì—Ä–∞—Ñ–∏–∫ 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "    axes[1,1].plot(results_df['stage'], results_df['avg_features'], 'o-', linewidth=2, markersize=8, color='purple')\n",
    "    axes[1,1].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[1,1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    axes[1,1].set_xticks(results_df['stage'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_PATH}metrics_progression.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º heatmap —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    \n",
    "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è heatmap\n",
    "    heatmap_data = results_df[['stage', 'avg_rmse', 'avg_mape', 'avg_da']].copy()\n",
    "    \n",
    "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (min-max scaling)\n",
    "    for col in ['avg_rmse', 'avg_mape', 'avg_da']:\n",
    "        min_val = heatmap_data[col].min()\n",
    "        max_val = heatmap_data[col].max()\n",
    "        if max_val > min_val:\n",
    "            heatmap_data[col] = (heatmap_data[col] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # –î–ª—è RMSE –∏ MAPE - –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)\n",
    "    heatmap_data['avg_rmse'] = 1 - heatmap_data['avg_rmse']\n",
    "    heatmap_data['avg_mape'] = 1 - heatmap_data['avg_mape']\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º heatmap\n",
    "    heatmap_matrix = heatmap_data[['avg_rmse', 'avg_mape', 'avg_da']].T\n",
    "    heatmap_matrix.columns = [f\"–≠—Ç–∞–ø {i}\" for i in results_df['stage']]\n",
    "    heatmap_matrix.index = ['RMSE (–Ω–æ—Ä–º.)', 'MAPE (–Ω–æ—Ä–º.)', 'DA']\n",
    "    \n",
    "    sns.heatmap(heatmap_matrix, annot=True, cmap='RdYlGn', center=0.5, \n",
    "                cbar_kws={'label': '–ö–∞—á–µ—Å—Ç–≤–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)'}, ax=ax)\n",
    "    ax.set_title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –ø–æ —ç—Ç–∞–ø–∞–º\\\\n(–∑–µ–ª–µ–Ω—ã–π = –ª—É—á—à–µ, –∫—Ä–∞—Å–Ω—ã–π = —Ö—É–∂–µ)', \n",
    "                 fontweight='bold', fontsize=14)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_PATH}quality_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üéØ –†–ï–ó–Æ–ú–ï –ü–†–û–ì–†–ï–°–°–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "if results_df.empty:\n",
    "    print(\"‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
    "else:\n",
    "    print(f\"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results_df)} —ç—Ç–∞–ø–æ–≤ –¥–ª—è {len(tickers)} —Ç–∏–∫–µ—Ä–æ–≤\")\n",
    "    print(f\"üìä –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "    \n",
    "    # –ù–∞–π–¥–µ–º –ª—É—á—à–∏–π —ç—Ç–∞–ø –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ\n",
    "    best_rmse_stage = results_df.loc[results_df['avg_rmse'].idxmin()]\n",
    "    best_mape_stage = results_df.loc[results_df['avg_mape'].idxmin()]\n",
    "    best_da_stage = results_df.loc[results_df['avg_da'].idxmax()]\n",
    "    \n",
    "    print(\"\\\\nüèÜ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
    "    print(f\"   –õ—É—á—à–∏–π RMSE: –≠—Ç–∞–ø {best_rmse_stage['stage']} ({best_rmse_stage['stage_name']}) - {best_rmse_stage['avg_rmse']:.4f}\")\n",
    "    print(f\"   –õ—É—á—à–∏–π MAPE: –≠—Ç–∞–ø {best_mape_stage['stage']} ({best_mape_stage['stage_name']}) - {best_mape_stage['avg_mape']:.2f}%\")\n",
    "    print(f\"   –õ—É—á—à–∏–π DA: –≠—Ç–∞–ø {best_da_stage['stage']} ({best_da_stage['stage_name']}) - {best_da_stage['avg_da']:.3f}\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
    "    base_metrics = results_df.iloc[0]\n",
    "    final_metrics = results_df.iloc[-1]\n",
    "    \n",
    "    rmse_change = ((final_metrics['avg_rmse'] - base_metrics['avg_rmse']) / base_metrics['avg_rmse']) * 100\n",
    "    mape_change = ((final_metrics['avg_mape'] - base_metrics['avg_mape']) / base_metrics['avg_mape']) * 100\n",
    "    da_change = ((final_metrics['avg_da'] - base_metrics['avg_da']) / base_metrics['avg_da']) * 100\n",
    "    \n",
    "    print(\"\\\\nüìà –û–ë–©–ï–ï –£–õ–£–ß–®–ï–ù–ò–ï (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø vs –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å):\")\n",
    "    print(f\"   RMSE: {rmse_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if rmse_change < 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "    print(f\"   MAPE: {mape_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if mape_change < 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "    print(f\"   DA: {da_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if da_change > 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
    "    \n",
    "    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
    "    print(\"\\\\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
    "    \n",
    "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —ç—Ç–∞–ø—ã\n",
    "    rmse_improvements = []\n",
    "    mape_improvements = []\n",
    "    da_improvements = []\n",
    "    \n",
    "    for i in range(1, len(results_df)):\n",
    "        prev_metrics = results_df.iloc[i-1]\n",
    "        curr_metrics = results_df.iloc[i]\n",
    "        \n",
    "        rmse_change = ((curr_metrics['avg_rmse'] - prev_metrics['avg_rmse']) / prev_metrics['avg_rmse']) * 100\n",
    "        mape_change = ((curr_metrics['avg_mape'] - prev_metrics['avg_mape']) / prev_metrics['avg_mape']) * 100\n",
    "        da_change = ((curr_metrics['avg_da'] - prev_metrics['avg_da']) / prev_metrics['avg_da']) * 100\n",
    "        \n",
    "        rmse_improvements.append((curr_metrics['stage'], rmse_change))\n",
    "        mape_improvements.append((curr_metrics['stage'], mape_change))\n",
    "        da_improvements.append((curr_metrics['stage'], da_change))\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º —ç—Ç–∞–ø—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏\n",
    "    best_rmse_improvement = min(rmse_improvements, key=lambda x: x[1])\n",
    "    best_mape_improvement = min(mape_improvements, key=lambda x: x[1])\n",
    "    best_da_improvement = max(da_improvements, key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"   1. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ RMSE –¥–∞–ª —ç—Ç–∞–ø {best_rmse_improvement[0]} ({best_rmse_improvement[1]:+.1f}%)\")\n",
    "    print(f\"   2. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ MAPE –¥–∞–ª —ç—Ç–∞–ø {best_mape_improvement[0]} ({best_mape_improvement[1]:+.1f}%)\")\n",
    "    print(f\"   3. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ DA –¥–∞–ª —ç—Ç–∞–ø {best_da_improvement[0]} ({best_da_improvement[1]:+.1f}%)\")\n",
    "    \n",
    "    # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
    "    print(\"\\\\n‚öñÔ∏è –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò (–∫–∞—á–µ—Å—Ç–≤–æ vs —Å–ª–æ–∂–Ω–æ—Å—Ç—å):\")\n",
    "    for _, row in results_df.iterrows():\n",
    "        efficiency_score = (1 - row['avg_rmse']/base_metrics['avg_rmse']) / (row['avg_features']/base_metrics['avg_features'])\n",
    "        print(f\"   –≠—Ç–∞–ø {row['stage']}: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = {efficiency_score:.3f} (–∫–∞—á–µ—Å—Ç–≤–æ/—Å–ª–æ–∂–Ω–æ—Å—Ç—å)\")\n",
    "\n",
    "print(\"\\\\nüéâ –ü–†–û–ì–†–ï–°–°–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!\")\n",
    "print(f\"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {OUTPUT_PATH}\")\n",
    "print(\"\\\\n–§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "print(\"   - progressive_analysis_summary.csv - —Å–≤–æ–¥–∫–∞ –ø–æ —ç—Ç–∞–ø–∞–º\")\n",
    "print(\"   - progressive_analysis_formatted.csv - —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\")\n",
    "print(\"   - feature_importance_summary.csv - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
    "print(\"   - metrics_progression.png - –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\")\n",
    "print(\"   - quality_heatmap.png - —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
    "print(\"   - feature_importance_stage_N.png - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç—Ç–∞–ø–∞\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llms)",
   "language": "python",
   "name": "llms"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
