{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# üìà VAR: Vector Autoregression –¥–ª—è –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "\n",
    "–≠—Ç–æ—Ç –±–ª–æ–∫–Ω–æ—Ç —Ä–µ–∞–ª–∏–∑—É–µ—Ç –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **Vector Autoregression (VAR)** –º–æ–¥–µ–ª–∏ - –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–∞–Ω–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.\n",
    "\n",
    "## üéØ –û—Å–Ω–æ–≤–Ω—ã–µ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏\n",
    "\n",
    "- **–ú–Ω–æ–≥–æ–º–µ—Ä–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ**: –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –ø–∞–ø–∫–∏ `/data/multivariate_series/`\n",
    "- **–ü–æ—ç—Ç–∞–ø–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏**: —Ä–∞–∑–ª–∏—á–Ω—ã–µ —ç—Ç–∞–ø—ã –¥–æ–±–∞–≤–ª–µ–Ω–∏—è —Ñ–∏—á–µ–π —Å–æ–≥–ª–∞—Å–Ω–æ progressive feature analysis\n",
    "- **Walk-forward –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ**: —á–µ—Å—Ç–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä—è—é—â–∏–º—Å—è –æ–∫–Ω–æ–º\n",
    "- **–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞**: 10 —Ç–æ—á–µ–∫\n",
    "- **–†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–∞**: 11 —Ç–æ—á–µ–∫\n",
    "- **VAR –º–æ–¥–µ–ª—å**: Vector Autoregression —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ª–∞–≥–æ–≤\n",
    "- **–ú–µ—Ç—Ä–∏–∫–∏**: RMSE, MAPE, DA (Directional Accuracy)\n",
    "\n",
    "## üìä –°—Ç—Ä—É–∫—Ç—É—Ä–∞ —ç—Ç–∞–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "1. **–≠—Ç–∞–ø 1**: close + volume (–±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏)\n",
    "2. **–≠—Ç–∞–ø 2**: + anomaly\n",
    "3. **–≠—Ç–∞–ø 3**: + weighted_score_with_decay (–Ω–æ–≤–æ—Å—Ç–∏)\n",
    "4. **–≠—Ç–∞–ø 4**: + OHLCV –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "5. **–≠—Ç–∞–ø 5**: + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "6. **–≠—Ç–∞–ø 6**: + —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (TSFresh)\n",
    "\n",
    "## üî¨ –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è VAR\n",
    "- **–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å**: –ø—Ä–æ–≤–µ—Ä–∫–∞ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä—è–¥–æ–≤\n",
    "- **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ª–∞–≥–æ–≤**: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–∞–≥–æ–≤\n",
    "- **–ö–æ–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**: –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã—Ö –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π\n",
    "- **Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è**: —Å—Ç—Ä–æ–≥–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±—É–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "- **Impulse Response**: –∞–Ω–∞–ª–∏–∑ —Ä–µ–∞–∫—Ü–∏–∏ –Ω–∞ —à–æ–∫–∏\n",
    "- **–ü—Ä–æ–≥–Ω–æ–∑**: 10 —Ç–æ—á–µ–∫ –≤–ø–µ—Ä–µ–¥ –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã\n",
      "üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\n",
      "üìà –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å VAR –º–æ–¥–µ–ª—è–º–∏!\n"
     ]
    }
   ],
   "source": [
    "# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏ –∏–º–ø–æ—Ä—Ç—ã\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è VAR\n",
    "try:\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "    from statsmodels.tsa.stattools import adfuller, kpss\n",
    "    from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    print(\"‚úÖ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã\")\n",
    "except ImportError:\n",
    "    print(\"üì¶ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \n",
    "                          \"statsmodels\", \"scipy\", \"--quiet\"])\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    from statsmodels.tsa.vector_ar.var_model import VAR\n",
    "    from statsmodels.tsa.stattools import adfuller, kpss\n",
    "    from statsmodels.tsa.vector_ar.vecm import coint_johansen\n",
    "    from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "    print(\"‚úÖ –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã\")\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# –£—Å—Ç–∞–Ω–æ–≤–∏–º —á–∏—Ç–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –≤—ã–≤–æ–¥–∞ —á–∏—Å–µ–ª\n",
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "\n",
    "print(\"üìö –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
    "print(\"üìà –ì–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å VAR –º–æ–¥–µ–ª—è–º–∏!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ù–∞—Å—Ç—Ä–æ–π–∫–∏:\n",
      "- –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: ../../data/multivariate_series/\n",
      "- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: 10\n",
      "- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç. –¥–∞–Ω–Ω—ã—Ö: 11\n",
      "- –ú–∏–Ω. —Ä–∞–∑–º–µ—Ä –æ–±—É—á. –¥–∞–Ω–Ω—ã—Ö: 100\n",
      "- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤: 10\n",
      "- –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: results/multivariate_var/\n"
     ]
    }
   ],
   "source": [
    "# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
    "DATA_PATH = \"../../data/multivariate_series/\"\n",
    "OUTPUT_PATH = \"results/multivariate_var/\"\n",
    "FORECAST_HORIZON = 10  # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º –Ω–∞ 10 —Ç–æ—á–µ–∫\n",
    "TEST_SIZE = 11  # –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –Ω–∞–±–æ—Ä–∞\n",
    "MIN_TRAIN_SIZE = 100  # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏\n",
    "\n",
    "# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "Path(OUTPUT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤ (–∫–∞–∫ –≤ –¥—Ä—É–≥–∏—Ö multivariate –±–ª–æ–∫–Ω–æ—Ç–∞—Ö)\n",
    "TICKERS = ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
    "\n",
    "# –ù–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
    "STAGE_NAMES = {\n",
    "    1: \"–ë–∞–∑–æ–≤—ã–µ (close + volume)\",\n",
    "    2: \"–ë–∞–∑–æ–≤—ã–µ + –∞–Ω–æ–º–∞–ª–∏–∏\", \n",
    "    3: \"–ë–∞–∑–æ–≤—ã–µ + –∞–Ω–æ–º–∞–ª–∏–∏ + –Ω–æ–≤–æ—Å—Ç–∏\",\n",
    "    4: \"–í—ã—à–µ + OHLCV\",\n",
    "    5: \"–í—ã—à–µ + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
    "    6: \"–í—ã—à–µ + —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏\"\n",
    "}\n",
    "\n",
    "# –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞\n",
    "FEATURE_STAGES = {\n",
    "    1: ['close', 'volume'],\n",
    "    2: ['close', 'volume', 'anomaly'],\n",
    "    3: ['close', 'volume', 'anomaly', 'weighted_score_with_decay'],\n",
    "    4: ['close', 'volume', 'anomaly', 'weighted_score_with_decay', 'open', 'high', 'low'],\n",
    "    5: ['close', 'volume', 'anomaly', 'weighted_score_with_decay', 'open', 'high', 'low',\n",
    "        'return', 'SMA_14', 'RSI_14', 'MACD', 'BB_hband', 'BB_lband', 'ATR_14'],\n",
    "    6: ['close', 'volume', 'anomaly', 'weighted_score_with_decay', 'open', 'high', 'low',\n",
    "        'return', 'SMA_14', 'RSI_14', 'MACD', 'BB_hband', 'BB_lband', 'ATR_14']  # + TSFresh –±—É–¥–µ—Ç –¥–æ–±–∞–≤–ª–µ–Ω–æ –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏\n",
    "}\n",
    "\n",
    "print(f\"–ù–∞—Å—Ç—Ä–æ–π–∫–∏:\")\n",
    "print(f\"- –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º: {DATA_PATH}\")\n",
    "print(f\"- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_HORIZON}\")\n",
    "print(f\"- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç. –¥–∞–Ω–Ω—ã—Ö: {TEST_SIZE}\")\n",
    "print(f\"- –ú–∏–Ω. —Ä–∞–∑–º–µ—Ä –æ–±—É—á. –¥–∞–Ω–Ω—ã—Ö: {MIN_TRAIN_SIZE}\")\n",
    "print(f\"- –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–∏–∫–µ—Ä–æ–≤: {len(TICKERS)}\")\n",
    "print(f\"- –ü–∞–ø–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å—é\n",
    "def prepare_features_for_stage(df, stage):\n",
    "    \"\"\"\n",
    "    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞\n",
    "    \"\"\"\n",
    "    base_features = FEATURE_STAGES[stage]\n",
    "    \n",
    "    # –î–ª—è 6 —ç—Ç–∞–ø–∞ –¥–æ–±–∞–≤–ª—è–µ–º TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    if stage == 6:\n",
    "        tsfresh_features = [col for col in df.columns if col.startswith('value__')]\n",
    "        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 20 TSFresh –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —É–ø—Ä–æ—â–µ–Ω–∏—è\n",
    "        tsfresh_features = tsfresh_features[:20]\n",
    "        base_features = base_features + tsfresh_features\n",
    "    \n",
    "    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "    available_features = [col for col in base_features if col in df.columns]\n",
    "    \n",
    "    return available_features\n",
    "\n",
    "def check_stationarity(series, alpha=0.05):\n",
    "    \"\"\"\n",
    "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ –∏—Å–ø–æ–ª—å–∑—É—è ADF —Ç–µ—Å—Ç\n",
    "    \"\"\"\n",
    "    try:\n",
    "        result = adfuller(series.dropna())\n",
    "        is_stationary = result[1] <= alpha\n",
    "        return {\n",
    "            'is_stationary': is_stationary, \n",
    "            'adf_stat': result[0],\n",
    "            'p_value': result[1],\n",
    "            'critical_values': result[4]\n",
    "        }\n",
    "    except:\n",
    "        return {'is_stationary': False, 'adf_stat': None, 'p_value': 1.0, 'critical_values': None}\n",
    "\n",
    "def make_stationary(df, method='diff'):\n",
    "    \"\"\"\n",
    "    –î–µ–ª–∞–µ—Ç –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏\n",
    "    \"\"\"\n",
    "    df_stationary = df.copy()\n",
    "    stationarity_info = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç—å\n",
    "        stat_result = check_stationarity(df[col])\n",
    "        \n",
    "        if not stat_result['is_stationary']:\n",
    "            if method == 'diff':\n",
    "                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ä–∞–∑–Ω–æ—Å—Ç—å\n",
    "                df_stationary[col] = df[col].diff()\n",
    "            elif method == 'log_diff':\n",
    "                # –õ–æ–≥–∞—Ä–∏—Ñ–º–∏—á–µ—Å–∫–∞—è —Ä–∞–∑–Ω–æ—Å—Ç—å (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π)\n",
    "                if (df[col] > 0).all():\n",
    "                    df_stationary[col] = np.log(df[col]).diff()\n",
    "                else:\n",
    "                    df_stationary[col] = df[col].diff()\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏\n",
    "        final_stat = check_stationarity(df_stationary[col])\n",
    "        stationarity_info[col] = {\n",
    "            'original_stationary': stat_result['is_stationary'],\n",
    "            'final_stationary': final_stat['is_stationary'],\n",
    "            'transformation': 'none' if stat_result['is_stationary'] else method\n",
    "        }\n",
    "    \n",
    "    # –£–¥–∞–ª—è–µ–º –ø–µ—Ä–≤—É—é —Å—Ç—Ä–æ–∫—É (NaN –ø–æ—Å–ª–µ –¥–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏—Ä–æ–≤–∞–Ω–∏—è)\n",
    "    df_stationary = df_stationary.dropna()\n",
    "    \n",
    "    return df_stationary, stationarity_info\n",
    "\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–ø—Ä–æ—Ü–µ–Ω—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–π)\n",
    "    \"\"\"\n",
    "    if len(y_true) <= 1:\n",
    "        return 0.0\n",
    "    \n",
    "    y_true_diff = np.diff(y_true)\n",
    "    y_pred_diff = np.diff(y_pred)\n",
    "    \n",
    "    correct_direction = np.sum(np.sign(y_true_diff) == np.sign(y_pred_diff))\n",
    "    total_predictions = len(y_true_diff)\n",
    "    \n",
    "    return correct_direction / total_predictions if total_predictions > 0 else 0.0\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≥–æ—Ç–æ–≤—ã!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAR –º–æ–¥–µ–ª—å —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π\n",
    "class VARForecaster:\n",
    "    def __init__(self, max_lags=10, ic='aic'):\n",
    "        self.max_lags = max_lags\n",
    "        self.ic = ic  # –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π –¥–ª—è –≤—ã–±–æ—Ä–∞ –ª–∞–≥–æ–≤\n",
    "        self.model = None\n",
    "        self.fitted_model = None\n",
    "        self.lag_order = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def find_optimal_lags(self, data):\n",
    "        \"\"\"\n",
    "        –ù–∞—Ö–æ–¥–∏—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–∞–≥–æ–≤\n",
    "        \"\"\"\n",
    "        try:\n",
    "            model = VAR(data)\n",
    "            lag_order_results = model.select_order(maxlags=self.max_lags)\n",
    "            \n",
    "            if self.ic == 'aic':\n",
    "                optimal_lag = lag_order_results.aic\n",
    "            elif self.ic == 'bic':\n",
    "                optimal_lag = lag_order_results.bic\n",
    "            elif self.ic == 'hqic':\n",
    "                optimal_lag = lag_order_results.hqic\n",
    "            else:\n",
    "                optimal_lag = lag_order_results.aic\n",
    "                \n",
    "            return min(optimal_lag, self.max_lags) if optimal_lag is not None else 1\n",
    "        except:\n",
    "            return 1\n",
    "    \n",
    "    def fit(self, data):\n",
    "        \"\"\"\n",
    "        –û–±—É—á–∞–µ—Ç VAR –º–æ–¥–µ–ª—å\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.feature_names = data.columns.tolist()\n",
    "            \n",
    "            # –ù–∞—Ö–æ–¥–∏–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ –ª–∞–≥–∏\n",
    "            self.lag_order = self.find_optimal_lags(data)\n",
    "            \n",
    "            # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "            self.model = VAR(data)\n",
    "            self.fitted_model = self.model.fit(self.lag_order)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ VAR: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def forecast(self, steps=1, last_obs=None):\n",
    "        \"\"\"\n",
    "        –î–µ–ª–∞–µ—Ç –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ steps —à–∞–≥–æ–≤ –≤–ø–µ—Ä–µ–¥\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if self.fitted_model is None:\n",
    "                return None\n",
    "                \n",
    "            if last_obs is None:\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è –∏–∑ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "                forecast = self.fitted_model.forecast(\n",
    "                    self.fitted_model.y, \n",
    "                    steps=steps\n",
    "                )\n",
    "            else:\n",
    "                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–∞–±–ª—é–¥–µ–Ω–∏—è\n",
    "                forecast = self.fitted_model.forecast(\n",
    "                    last_obs[-self.lag_order:], \n",
    "                    steps=steps\n",
    "                )\n",
    "            \n",
    "            return pd.DataFrame(forecast, columns=self.feature_names)\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_model_summary(self):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–≤–æ–¥–∫—É –º–æ–¥–µ–ª–∏\n",
    "        \"\"\"\n",
    "        if self.fitted_model is not None:\n",
    "            return self.fitted_model.summary()\n",
    "        return None\n",
    "    \n",
    "    def get_residuals(self):\n",
    "        \"\"\"\n",
    "        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Å—Ç–∞—Ç–∫–∏ –º–æ–¥–µ–ª–∏\n",
    "        \"\"\"\n",
    "        if self.fitted_model is not None:\n",
    "            return self.fitted_model.resid\n",
    "        return None\n",
    "\n",
    "print(\"‚úÖ VAR –∫–ª–∞—Å—Å –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_and_prepare_data(ticker, stage):\n",
    "    \"\"\"\n",
    "    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –∏ —ç—Ç–∞–ø–∞\n",
    "    \"\"\"\n",
    "    file_path = Path(DATA_PATH) / f\"{ticker}_multivariate.csv\"\n",
    "    \n",
    "    if not file_path.exists():\n",
    "        print(f\"‚ö†Ô∏è –§–∞–π–ª {file_path} –Ω–µ –Ω–∞–π–¥–µ–Ω\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º timestamp –≤ datetime\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        df = df.set_index('timestamp').sort_index()\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
    "        features = prepare_features_for_stage(df, stage)\n",
    "        \n",
    "        if len(features) == 0:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {stage} —É —Ç–∏–∫–µ—Ä–∞ {ticker}\")\n",
    "            return None, None\n",
    "        \n",
    "        # –û—Ç–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏\n",
    "        df_features = df[features].copy()\n",
    "        \n",
    "        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN\n",
    "        df_features = df_features.dropna()\n",
    "        \n",
    "        if df_features.empty:\n",
    "            print(f\"‚ö†Ô∏è –ü—É—Å—Ç–æ–π DataFrame –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–ª—è {ticker}, —ç—Ç–∞–ø {stage}\")\n",
    "            return None, None\n",
    "        \n",
    "        print(f\"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker}, —ç—Ç–∞–ø {stage}: {df_features.shape}\")\n",
    "        print(f\"   –ü—Ä–∏–∑–Ω–∞–∫–∏: {features[:5]}{'...' if len(features) > 5 else ''}\")\n",
    "        \n",
    "        return df_features, features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {ticker}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# –§—É–Ω–∫—Ü–∏—è –¥–ª—è walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏–∏\n",
    "def walk_forward_validation(df, forecaster, target_col='close'):\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è VAR –º–æ–¥–µ–ª–∏\n",
    "    \"\"\"\n",
    "    n_samples = len(df)\n",
    "    train_end = n_samples - TEST_SIZE\n",
    "    \n",
    "    if train_end < MIN_TRAIN_SIZE:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {n_samples} < {MIN_TRAIN_SIZE + TEST_SIZE}\")\n",
    "        return None\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "    for i in range(TEST_SIZE):\n",
    "        current_train_end = train_end + i\n",
    "        \n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "        train_data = df.iloc[:current_train_end]\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        if target_col not in train_data.columns:\n",
    "            print(f\"‚ö†Ô∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö\")\n",
    "            return None\n",
    "        \n",
    "        # –î–µ–ª–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏\n",
    "        train_stationary, stationarity_info = make_stationary(train_data)\n",
    "        \n",
    "        if train_stationary.empty or len(train_stationary) < 10:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏: {len(train_stationary)}\")\n",
    "            continue\n",
    "        \n",
    "        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "        var_model = VARForecaster(max_lags=min(5, len(train_stationary)//10))\n",
    "        success = var_model.fit(train_stationary)\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —à–∞–≥–µ {i}\")\n",
    "            continue\n",
    "        \n",
    "        # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "        forecast_df = var_model.forecast(steps=FORECAST_HORIZON)\n",
    "        \n",
    "        if forecast_df is None or target_col not in forecast_df.columns:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —à–∞–≥–µ {i}\")\n",
    "            continue\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
    "        pred_values = forecast_df[target_col].values\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        actual_start = current_train_end\n",
    "        actual_end = min(actual_start + FORECAST_HORIZON, len(df))\n",
    "        actual_values = df[target_col].iloc[actual_start:actual_end].values\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        min_len = min(len(pred_values), len(actual_values))\n",
    "        if min_len > 0:\n",
    "            predictions.extend(pred_values[:min_len])\n",
    "            actuals.extend(actual_values[:min_len])\n",
    "            dates.extend(df.index[actual_start:actual_start + min_len])\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'predictions': np.array(predictions),\n",
    "        'actuals': np.array(actuals),\n",
    "        'dates': dates\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ –§—É–Ω–∫—Ü–∏–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–ø—É—Å–∫–∞ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "def run_var_experiments():\n",
    "    \"\"\"\n",
    "    –ó–∞–ø—É—Å–∫–∞–µ—Ç —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã VAR –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ –∏ —ç—Ç–∞–ø–æ–≤\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å VAR –º–æ–¥–µ–ª—å—é...\")\n",
    "    print(f\"–¢–∏–∫–µ—Ä—ã: {TICKERS}\")\n",
    "    print(f\"–≠—Ç–∞–ø—ã: {list(STAGE_NAMES.keys())}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for ticker in TICKERS:\n",
    "        print(f\"\\nüìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–∏–∫–µ—Ä: {ticker}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for stage in STAGE_NAMES.keys():\n",
    "            print(f\"\\nüîÑ –≠—Ç–∞–ø {stage}: {STAGE_NAMES[stage]}\")\n",
    "            \n",
    "            try:\n",
    "                # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "                df, features = load_and_prepare_data(ticker, stage)\n",
    "                \n",
    "                if df is None:\n",
    "                    continue\n",
    "                \n",
    "                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–∞–ª–∏–¥–∞—Ü–∏—é\n",
    "                validation_results = walk_forward_validation(df, None, target_col='close')\n",
    "                \n",
    "                if validation_results is None:\n",
    "                    print(f\"‚ùå –í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ —É–¥–∞–ª–∞—Å—å –¥–ª—è {ticker}, —ç—Ç–∞–ø {stage}\")\n",
    "                    continue\n",
    "                \n",
    "                # –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏\n",
    "                predictions = validation_results['predictions']\n",
    "                actuals = validation_results['actuals']\n",
    "                \n",
    "                # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "                rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "                mae = mean_absolute_error(actuals, predictions)\n",
    "                mape = mean_absolute_percentage_error(actuals, predictions) * 100\n",
    "                da = directional_accuracy(actuals, predictions) * 100\n",
    "                \n",
    "                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n",
    "                mse = mean_squared_error(actuals, predictions)\n",
    "                correlation = np.corrcoef(actuals, predictions)[0, 1] if len(actuals) > 1 else 0\n",
    "                \n",
    "                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
    "                result = {\n",
    "                    'ticker': ticker,\n",
    "                    'stage': stage,\n",
    "                    'stage_name': STAGE_NAMES[stage],\n",
    "                    'n_features': len(features),\n",
    "                    'n_samples': len(df),\n",
    "                    'n_predictions': len(predictions),\n",
    "                    'rmse': rmse,\n",
    "                    'mae': mae,\n",
    "                    'mape': mape,\n",
    "                    'mse': mse,\n",
    "                    'da': da,\n",
    "                    'correlation': correlation,\n",
    "                    'features': features[:10]  # –ü–µ—Ä–≤—ã–µ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏\n",
    "                }\n",
    "                \n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è {ticker}, —ç—Ç–∞–ø {stage}:\")\n",
    "                print(f\"   RMSE: {rmse:.4f}\")\n",
    "                print(f\"   MAE: {mae:.4f}\")\n",
    "                print(f\"   MAPE: {mape:.2f}%\")\n",
    "                print(f\"   DA: {da:.2f}%\")\n",
    "                print(f\"   –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è: {correlation:.4f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {ticker}, —ç—Ç–∞–ø {stage}: {e}\")\n",
    "                continue\n",
    "    \n",
    "    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    if not results_df.empty:\n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        output_file = Path(OUTPUT_PATH) / f\"multivariate_var_results_{timestamp}.csv\"\n",
    "        results_df.to_csv(output_file, index=False)\n",
    "        print(f\"\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_file}\")\n",
    "        \n",
    "        # –í—ã–≤–æ–¥–∏–º —Å–≤–æ–¥–∫—É\n",
    "        print(f\"\\nüìà –°–≤–æ–¥–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "        print(f\"–í—Å–µ–≥–æ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤: {len(results_df)}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π RMSE: {results_df['rmse'].mean():.4f}\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π MAPE: {results_df['mape'].mean():.2f}%\")\n",
    "        print(f\"–°—Ä–µ–¥–Ω–∏–π DA: {results_df['da'].mean():.2f}%\")\n",
    "        \n",
    "        return results_df\n",
    "    else:\n",
    "        print(\"‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
    "        return None\n",
    "\n",
    "print(\"üéØ –§—É–Ω–∫—Ü–∏—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ –≥–æ—Ç–æ–≤–∞ –∫ –∑–∞–ø—É—Å–∫—É!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "def plot_var_results(results_df):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ VAR —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if results_df is None or results_df.empty:\n",
    "        print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('üìà –†–µ–∑—É–ª—å—Ç–∞—Ç—ã VAR –º–æ–¥–µ–ª–∏ –ø–æ —ç—Ç–∞–ø–∞–º –∏ —Ç–∏–∫–µ—Ä–∞–º', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. RMSE –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "    stage_rmse = results_df.groupby('stage')['rmse'].mean().reset_index()\n",
    "    axes[0, 0].bar(stage_rmse['stage'], stage_rmse['rmse'], color='skyblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('–°—Ä–µ–¥–Ω–∏–π RMSE –ø–æ —ç—Ç–∞–ø–∞–º')\n",
    "    axes[0, 0].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[0, 0].set_ylabel('RMSE')\n",
    "    for i, v in enumerate(stage_rmse['rmse']):\n",
    "        axes[0, 0].text(i+1, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 2. MAPE –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "    stage_mape = results_df.groupby('stage')['mape'].mean().reset_index()\n",
    "    axes[0, 1].bar(stage_mape['stage'], stage_mape['mape'], color='lightcoral', alpha=0.7)\n",
    "    axes[0, 1].set_title('–°—Ä–µ–¥–Ω–∏–π MAPE –ø–æ —ç—Ç–∞–ø–∞–º')\n",
    "    axes[0, 1].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[0, 1].set_ylabel('MAPE (%)')\n",
    "    for i, v in enumerate(stage_mape['mape']):\n",
    "        axes[0, 1].text(i+1, v + 0.5, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 3. DA –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "    stage_da = results_df.groupby('stage')['da'].mean().reset_index()\n",
    "    axes[1, 0].bar(stage_da['stage'], stage_da['da'], color='lightgreen', alpha=0.7)\n",
    "    axes[1, 0].set_title('–°—Ä–µ–¥–Ω—è—è Directional Accuracy –ø–æ —ç—Ç–∞–ø–∞–º')\n",
    "    axes[1, 0].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[1, 0].set_ylabel('DA (%)')\n",
    "    axes[1, 0].axhline(y=50, color='red', linestyle='--', alpha=0.5, label='–°–ª—É—á–∞–π–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å')\n",
    "    axes[1, 0].legend()\n",
    "    for i, v in enumerate(stage_da['da']):\n",
    "        axes[1, 0].text(i+1, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ RMSE –ø–æ —Ç–∏–∫–µ—Ä–∞–º –∏ —ç—Ç–∞–ø–∞–º\n",
    "    pivot_rmse = results_df.pivot(index='ticker', columns='stage', values='rmse')\n",
    "    im = axes[1, 1].imshow(pivot_rmse.values, cmap='YlOrRd', aspect='auto')\n",
    "    axes[1, 1].set_title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ RMSE')\n",
    "    axes[1, 1].set_xlabel('–≠—Ç–∞–ø')\n",
    "    axes[1, 1].set_ylabel('–¢–∏–∫–µ—Ä')\n",
    "    axes[1, 1].set_xticks(range(len(pivot_rmse.columns)))\n",
    "    axes[1, 1].set_xticklabels(pivot_rmse.columns)\n",
    "    axes[1, 1].set_yticks(range(len(pivot_rmse.index)))\n",
    "    axes[1, 1].set_yticklabels(pivot_rmse.index)\n",
    "    plt.colorbar(im, ax=axes[1, 1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≥—Ä–∞—Ñ–∏–∫\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    plot_file = Path(OUTPUT_PATH) / f\"multivariate_var_comparison_{timestamp}.png\"\n",
    "    plt.savefig(plot_file, dpi=300, bbox_inches='tight')\n",
    "    print(f\"üìä –ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_file}\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def create_summary_table(results_df):\n",
    "    \"\"\"\n",
    "    –°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    \"\"\"\n",
    "    if results_df is None or results_df.empty:\n",
    "        print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–≤–æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã\")\n",
    "        return None\n",
    "    \n",
    "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —ç—Ç–∞–ø–∞–º\n",
    "    summary = results_df.groupby(['stage', 'stage_name']).agg({\n",
    "        'rmse': ['mean', 'std'],\n",
    "        'mape': ['mean', 'std'],\n",
    "        'da': ['mean', 'std'],\n",
    "        'correlation': ['mean', 'std'],\n",
    "        'n_features': 'first'\n",
    "    }).round(4)\n",
    "    \n",
    "    # –£–ø—Ä–æ—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫\n",
    "    summary.columns = ['RMSE_mean', 'RMSE_std', 'MAPE_mean', 'MAPE_std', \n",
    "                      'DA_mean', 'DA_std', 'Corr_mean', 'Corr_std', 'N_features']\n",
    "    \n",
    "    summary = summary.reset_index()\n",
    "    \n",
    "    print(\"üìã –°–≤–æ–¥–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ VAR:\")\n",
    "    print(\"=\" * 80)\n",
    "    display(summary)\n",
    "    \n",
    "    # –ù–∞—Ö–æ–¥–∏–º –ª—É—á—à–∏–π —ç—Ç–∞–ø\n",
    "    best_stage_rmse = summary.loc[summary['RMSE_mean'].idxmin()]\n",
    "    best_stage_da = summary.loc[summary['DA_mean'].idxmax()]\n",
    "    \n",
    "    print(f\"\\nüèÜ –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\")\n",
    "    print(f\"–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π RMSE: –≠—Ç–∞–ø {best_stage_rmse['stage']} ({best_stage_rmse['stage_name']}) - {best_stage_rmse['RMSE_mean']:.4f}\")\n",
    "    print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π DA: –≠—Ç–∞–ø {best_stage_da['stage']} ({best_stage_da['stage_name']}) - {best_stage_da['DA_mean']:.2f}%\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "print(\"üìä –§—É–Ω–∫—Ü–∏–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≥–æ—Ç–æ–≤—ã!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –∑–∞–ø—É—Å—Ç–∏–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤ —Å VAR –º–æ–¥–µ–ª—å—é –¥–ª—è –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤ –∏ —ç—Ç–∞–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É –≤ –∫–æ–¥–µ walk_forward_validation\n",
    "def walk_forward_validation_fixed(df, forecaster, target_col='close'):\n",
    "    \"\"\"\n",
    "    –í—ã–ø–æ–ª–Ω—è–µ—Ç walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—é –¥–ª—è VAR –º–æ–¥–µ–ª–∏ (–∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)\n",
    "    \"\"\"\n",
    "    n_samples = len(df)\n",
    "    train_end = n_samples - TEST_SIZE\n",
    "    \n",
    "    if train_end < MIN_TRAIN_SIZE:\n",
    "        print(f\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏: {n_samples} < {MIN_TRAIN_SIZE + TEST_SIZE}\")\n",
    "        return None\n",
    "    \n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    dates = []\n",
    "    \n",
    "    # Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è\n",
    "    for i in range(TEST_SIZE):\n",
    "        current_train_end = train_end + i\n",
    "        \n",
    "        # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "        train_data = df.iloc[:current_train_end]\n",
    "        \n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ü–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –≤ –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
    "        if target_col not in train_data.columns:\n",
    "            print(f\"‚ö†Ô∏è –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –¥–∞–Ω–Ω—ã—Ö\")\n",
    "            return None\n",
    "        \n",
    "        # –î–µ–ª–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω—ã–º–∏\n",
    "        train_stationary, stationarity_info = make_stationary(train_data)\n",
    "        \n",
    "        if train_stationary.empty or len(train_stationary) < 10:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–∏–∑–∞—Ü–∏–∏: {len(train_stationary)}\")\n",
    "            continue\n",
    "        \n",
    "        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "        var_model = VARForecaster(max_lags=min(5, len(train_stationary)//10))\n",
    "        success = var_model.fit(train_stationary)\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å –Ω–∞ —à–∞–≥–µ {i}\")\n",
    "            continue\n",
    "        \n",
    "        # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑\n",
    "        forecast_df = var_model.forecast(steps=FORECAST_HORIZON)\n",
    "        \n",
    "        if forecast_df is None or target_col not in forecast_df.columns:\n",
    "            print(f\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ —à–∞–≥–µ {i}\")\n",
    "            continue\n",
    "        \n",
    "        # –ë–µ—Ä–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π\n",
    "        pred_values = forecast_df[target_col].values\n",
    "        \n",
    "        # –ü–æ–ª—É—á–∞–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
    "        actual_start = current_train_end\n",
    "        actual_end = min(actual_start + FORECAST_HORIZON, len(df))\n",
    "        actual_values = df[target_col].iloc[actual_start:actual_end].values\n",
    "        \n",
    "        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã\n",
    "        min_len = min(len(pred_values), len(actual_values))\n",
    "        if min_len > 0:\n",
    "            predictions.extend(pred_values[:min_len])\n",
    "            actuals.extend(actual_values[:min_len])\n",
    "            dates.extend(df.index[actual_start:actual_start + min_len])\n",
    "    \n",
    "    if len(predictions) == 0:\n",
    "        print(\"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞\")\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'predictions': np.array(predictions),\n",
    "        'actuals': np.array(actuals),\n",
    "        'dates': dates\n",
    "    }\n",
    "\n",
    "# –ó–∞–ø—É—Å–∫–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã\n",
    "print(\"üé¨ –ù–∞—á–∏–Ω–∞–µ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã...\")\n",
    "results_df = run_var_experiments()\n",
    "\n",
    "if results_df is not None:\n",
    "    print(\"‚úÖ –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!\")\n",
    "else:\n",
    "    print(\"‚ùå –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –Ω–µ —É–¥–∞–ª–∏—Å—å\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "if results_df is not None and not results_df.empty:\n",
    "    print(\"üìä –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏...\")\n",
    "    \n",
    "    # –°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫–∏\n",
    "    plot_var_results(results_df)\n",
    "    \n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É\n",
    "    summary_table = create_summary_table(results_df)\n",
    "    \n",
    "    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑\n",
    "    print(\"\\nüîç –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑:\")\n",
    "    print(f\"–õ—É—á—à–∏–π —Ç–∏–∫–µ—Ä –ø–æ RMSE: {results_df.loc[results_df['rmse'].idxmin(), 'ticker']} (RMSE: {results_df['rmse'].min():.4f})\")\n",
    "    print(f\"–•—É–¥—à–∏–π —Ç–∏–∫–µ—Ä –ø–æ RMSE: {results_df.loc[results_df['rmse'].idxmax(), 'ticker']} (RMSE: {results_df['rmse'].max():.4f})\")\n",
    "    print(f\"–õ—É—á—à–∏–π —Ç–∏–∫–µ—Ä –ø–æ DA: {results_df.loc[results_df['da'].idxmax(), 'ticker']} (DA: {results_df['da'].max():.2f}%)\")\n",
    "    \n",
    "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –¥–µ—Ç–∞–ª—å–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
    "    print(\"\\\\nüìã –ü–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
    "    display(results_df.head(10))\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## üéØ –ó–∞–∫–ª—é—á–µ–Ω–∏–µ\n",
    "\n",
    "–ë–ª–æ–∫–Ω–æ—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è VAR (Vector Autoregression) –º–æ–¥–µ–ª–∏ –Ω–∞ –º–Ω–æ–≥–æ–º–µ—Ä–Ω—ã—Ö –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–∞—Ö –≥–æ—Ç–æ–≤!\n",
    "\n",
    "### üîß –ß—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ:\n",
    "\n",
    "1. **–ö–æ–º–ø–ª–µ–∫—Å–Ω–∞—è VAR –º–æ–¥–µ–ª—å** —Å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –ª–∞–≥–æ–≤\n",
    "2. **–ü–æ—ç—Ç–∞–ø–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ** —Å 6 —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤  \n",
    "3. **–°—Ç–∞—Ü–∏–æ–Ω–∞—Ä–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö** —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π ADF —Ç–µ—Å—Ç–æ–º\n",
    "4. **Walk-forward –≤–∞–ª–∏–¥–∞—Ü–∏—è** –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è\n",
    "5. **–ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**: RMSE, MAE, MAPE, DA, –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è\n",
    "6. **–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤** —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏ –∏ —Ç–µ–ø–ª–æ–≤—ã–º–∏ –∫–∞—Ä—Ç–∞–º–∏\n",
    "7. **–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ** —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ CSV –∏ PNG —Ñ–∞–π–ª—ã\n",
    "\n",
    "### üìä –≠—Ç–∞–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\n",
    "\n",
    "- **–≠—Ç–∞–ø 1**: –ë–∞–∑–æ–≤—ã–µ (close + volume)\n",
    "- **–≠—Ç–∞–ø 2**: + –ê–Ω–æ–º–∞–ª–∏–∏  \n",
    "- **–≠—Ç–∞–ø 3**: + –ù–æ–≤–æ—Å—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
    "- **–≠—Ç–∞–ø 4**: + OHLCV –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "- **–≠—Ç–∞–ø 5**: + –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
    "- **–≠—Ç–∞–ø 6**: + –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (TSFresh)\n",
    "\n",
    "### üöÄ –î–ª—è –∑–∞–ø—É—Å–∫–∞:\n",
    "\n",
    "1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –≤—Å–µ —è—á–µ–π–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ\n",
    "2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω—è—Ç—Å—è –≤ –ø–∞–ø–∫–µ `results/multivariate_var/`\n",
    "3. –ë—É–¥—É—Ç —Å–æ–∑–¥–∞–Ω—ã CSV —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏ PNG —Å –≥—Ä–∞—Ñ–∏–∫–∞–º–∏\n",
    "\n",
    "### üìà –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:\n",
    "\n",
    "VAR –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ –ø–æ–∫–∞–∑–∞—Ç—å —Ö–æ—Ä–æ—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–º–µ—Ä–µ–Ω–Ω–æ–π –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–µ–π –º–µ–∂–¥—É –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏. –ú–æ–¥–µ–ª—å –æ—Å–æ–±–µ–Ω–Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞ –¥–ª—è:\n",
    "- –ê–Ω–∞–ª–∏–∑–∞ –≤–∑–∞–∏–º–æ—Å–≤—è–∑–µ–π –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏\n",
    "- –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è (1-10 —à–∞–≥–æ–≤)\n",
    "- –î–∞–Ω–Ω—ã—Ö —Å —Å—Ç–∞–±–∏–ª—å–Ω—ã–º–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–º–∏ —Å–≤–æ–π—Å—Ç–≤–∞–º–∏\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (simple_strategies)",
   "language": "python",
   "name": "simple_strategies"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
