{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "\n",
        "–≠—Ç–æ—Ç –Ω–æ—É—Ç–±—É–∫ –ø—Ä–µ–¥–Ω–∞–∑–Ω–∞—á–µ–Ω –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–æ–≥–æ, –∫–∞–∫ –ø–æ—ç—Ç–∞–ø–Ω–æ–µ –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤.\n",
        "\n",
        "## –≠—Ç–∞–ø—ã –∞–Ω–∞–ª–∏–∑–∞:\n",
        "1. **–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å** - –æ–±—É—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞ —Ä—è–¥—É `close` (univariate)\n",
        "2. **+ –ê–Ω–æ–º–∞–ª–∏–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π\n",
        "3. **+ –ù–æ–≤–æ—Å—Ç–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π  \n",
        "4. **+ –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —è–ø–æ–Ω—Å–∫–∏—Ö —Å–≤–µ—á–µ–π\n",
        "5. **+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤\n",
        "6. **+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö —Å–≤–æ–π—Å—Ç–≤ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤\n",
        "7. **+ PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã** - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ —Å–∂–∞—Ç—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "\n",
        "## –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏:\n",
        "- **RMSE** - —Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞\n",
        "- **MAPE** - —Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞  \n",
        "- **DA** - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (Directional Accuracy)\n",
        "\n",
        "## –ú–æ–¥–µ–ª—å:\n",
        "- **RandomForest** –∏–∑ DARTS —Å –∞–Ω–∞–ª–∏–∑–æ–º –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "- **–ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞**: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 20 –¥–Ω–µ–π\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# DARTS –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n",
        "from darts import TimeSeries\n",
        "from darts.models import RandomForest\n",
        "from darts.metrics import rmse, mape\n",
        "from darts.utils.utils import SeasonalityMode, TrendMode\n",
        "\n",
        "# –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "plt.style.use('seaborn-v0_8')\n",
        "plt.rcParams['figure.figsize'] = (15, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "print(\"–ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ü—É—Ç–∏ –∫ –¥–∞–Ω–Ω—ã–º\n",
        "INPUT_PATH = 'data/multivariate_series/'\n",
        "OUTPUT_PATH = 'results/progressive_analysis/'\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# –°–ø–∏—Å–æ–∫ —Ç–∏–∫–µ—Ä–æ–≤\n",
        "tickers = ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
        "\n",
        "# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞\n",
        "FORECAST_HORIZON = 20  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "TEST_SIZE = 30         # –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ (–±–æ–ª—å—à–µ —á–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞)\n",
        "\n",
        "print(f\"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞:\")\n",
        "print(f\"- –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ: {INPUT_PATH}\")\n",
        "print(f\"- –†–µ–∑—É–ª—å—Ç–∞—Ç—ã: {OUTPUT_PATH}\")\n",
        "print(f\"- –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–æ–≥–Ω–æ–∑–∞: {FORECAST_HORIZON} –¥–Ω–µ–π\")\n",
        "print(f\"- –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {TEST_SIZE} –¥–Ω–µ–π\")\n",
        "print(f\"- –¢–∏–∫–µ—Ä—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {tickers}\")\n",
        "\n",
        "# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
        "data = {}\n",
        "for ticker in tickers:\n",
        "    try:\n",
        "        file_path = f\"{INPUT_PATH}{ticker}_multivariate.csv\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "        df = df.set_index('timestamp').sort_index()\n",
        "        data[ticker] = df\n",
        "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω {ticker}: {df.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {ticker}: {str(e)}\")\n",
        "\n",
        "print(f\"\\n–£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(data)} —Ç–∏–∫–µ—Ä–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_directional_accuracy(actual, predicted):\n",
        "    \"\"\"\n",
        "    –í—ã—á–∏—Å–ª—è–µ—Ç —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (Directional Accuracy)\n",
        "    \n",
        "    Args:\n",
        "        actual: —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "        predicted: –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è\n",
        "    \n",
        "    Returns:\n",
        "        DA: —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–æ—Ç 0 –¥–æ 1)\n",
        "    \"\"\"\n",
        "    if len(actual) <= 1 or len(predicted) <= 1:\n",
        "        return np.nan\n",
        "    \n",
        "    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π\n",
        "    actual_direction = np.diff(actual) > 0\n",
        "    \n",
        "    # –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
        "    predicted_direction = np.diff(predicted) > 0\n",
        "    \n",
        "    # –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è\n",
        "    da = np.mean(actual_direction == predicted_direction)\n",
        "    \n",
        "    return da\n",
        "\n",
        "def prepare_features_for_stage(df, stage):\n",
        "    \"\"\"\n",
        "    –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–≥–æ —ç—Ç–∞–ø–∞ –∞–Ω–∞–ª–∏–∑–∞\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏\n",
        "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-7)\n",
        "    \n",
        "    Returns:\n",
        "        feature_columns: —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è\n",
        "    \"\"\"\n",
        "    \n",
        "    # –ë–∞–∑–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ (–≤—Å–µ–≥–¥–∞ –∏—Å–∫–ª—é—á–∞–µ–º)\n",
        "    base_exclude = ['open', 'high', 'low', 'volume', 'date', 'daily_headlines', 'return']\n",
        "    \n",
        "    if stage == 1:\n",
        "        # –≠—Ç–∞–ø 1: —Ç–æ–ª—å–∫–æ close (univariate)\n",
        "        return ['close']\n",
        "    \n",
        "    elif stage == 2:\n",
        "        # –≠—Ç–∞–ø 2: + –∞–Ω–æ–º–∞–ª–∏–∏\n",
        "        features = ['close', 'anomaly']\n",
        "        return features\n",
        "    \n",
        "    elif stage == 3:\n",
        "        # –≠—Ç–∞–ø 3: + –Ω–æ–≤–æ—Å—Ç–∏\n",
        "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –¥—Ä—É–≥–∏–µ –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å\n",
        "        news_cols = [col for col in df.columns if col in ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg']]\n",
        "        features.extend(news_cols)\n",
        "        return features\n",
        "    \n",
        "    elif stage == 4:\n",
        "        # –≠—Ç–∞–ø 4: + –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π\n",
        "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
        "        news_cols = [col for col in df.columns if col in ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg']]\n",
        "        features.extend(news_cols)\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã\n",
        "        pattern_cols = [col for col in df.columns if 'Pattern_' in col or 'Bullish' in col or 'Bearish' in col or 'Overbought' in col]\n",
        "        features.extend(pattern_cols)\n",
        "        return features\n",
        "    \n",
        "    elif stage == 5:\n",
        "        # –≠—Ç–∞–ø 5: + —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
        "        news_cols = [col for col in df.columns if col in ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg']]\n",
        "        features.extend(news_cols)\n",
        "        \n",
        "        pattern_cols = [col for col in df.columns if 'Pattern_' in col or 'Bullish' in col or 'Bearish' in col or 'Overbought' in col]\n",
        "        features.extend(pattern_cols)\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\n",
        "        tech_indicators = ['SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
        "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
        "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
        "        features.extend(tech_cols)\n",
        "        return features\n",
        "    \n",
        "    elif stage == 6:\n",
        "        # –≠—Ç–∞–ø 6: + TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
        "        news_cols = [col for col in df.columns if col in ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg']]\n",
        "        features.extend(news_cols)\n",
        "        \n",
        "        pattern_cols = [col for col in df.columns if 'Pattern_' in col or 'Bullish' in col or 'Bearish' in col or 'Overbought' in col]\n",
        "        features.extend(pattern_cols)\n",
        "        \n",
        "        tech_indicators = ['SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
        "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
        "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
        "        features.extend(tech_cols)\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
        "        tsfresh_cols = [col for col in df.columns if 'value__' in col]\n",
        "        features.extend(tsfresh_cols)\n",
        "        return features\n",
        "    \n",
        "    elif stage == 7:\n",
        "        # –≠—Ç–∞–ø 7: + PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
        "        features = ['close', 'anomaly', 'weighted_score_with_decay']\n",
        "        news_cols = [col for col in df.columns if col in ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg']]\n",
        "        features.extend(news_cols)\n",
        "        \n",
        "        pattern_cols = [col for col in df.columns if 'Pattern_' in col or 'Bullish' in col or 'Bearish' in col or 'Overbought' in col]\n",
        "        features.extend(pattern_cols)\n",
        "        \n",
        "        tech_indicators = ['SMA_14', 'SMA_50', 'EMA_14', 'EMA_50', 'RSI_14', 'MACD', 'MACD_signal', \n",
        "                          'BB_hband', 'BB_lband', 'ATR_14', 'OBV', 'VWAP']\n",
        "        tech_cols = [col for col in df.columns if any(indicator in col for indicator in tech_indicators)]\n",
        "        features.extend(tech_cols)\n",
        "        \n",
        "        tsfresh_cols = [col for col in df.columns if 'value__' in col]\n",
        "        features.extend(tsfresh_cols)\n",
        "        \n",
        "        # –î–æ–±–∞–≤–ª—è–µ–º PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n",
        "        pca_cols = [col for col in df.columns if col.startswith('PCA_')]\n",
        "        features.extend(pca_cols)\n",
        "        return features\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —ç—Ç–∞–ø: {stage}\")\n",
        "\n",
        "def evaluate_model_for_ticker(df, ticker, stage):\n",
        "    \"\"\"\n",
        "    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç –µ—ë –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–∏–∫–µ—Ä–∞ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º —ç—Ç–∞–ø–µ\n",
        "    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏—é \"—Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\" –¥–ª—è —á–µ—Å—Ç–Ω–æ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame —Å –¥–∞–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞\n",
        "        ticker: –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∏–∫–µ—Ä–∞\n",
        "        stage: –Ω–æ–º–µ—Ä —ç—Ç–∞–ø–∞ (1-7)\n",
        "    \n",
        "    Returns:\n",
        "        results: —Å–ª–æ–≤–∞—Ä—å —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –≤–∞–∂–Ω–æ—Å—Ç—å—é –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    \"\"\"\n",
        "    \n",
        "    try:\n",
        "        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —ç—Ç–∞–ø–∞\n",
        "        feature_columns = prepare_features_for_stage(df, stage)\n",
        "        \n",
        "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –∫–æ–ª–æ–Ω–æ–∫\n",
        "        available_features = [col for col in feature_columns if col in df.columns]\n",
        "        \n",
        "        if len(available_features) == 0:\n",
        "            print(f\"  - –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤—ã–±—Ä–∞–Ω–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–∞—Ö\n",
        "        df_clean = df[available_features].dropna()\n",
        "        \n",
        "        if len(df_clean) < TEST_SIZE + 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "            print(f\"  - –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        # –ì–æ—Ç–æ–≤–∏–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è DARTS\n",
        "        if stage == 1:\n",
        "            # Univariate –º–æ–¥–µ–ª—å\n",
        "            ts = TimeSeries.from_dataframe(\n",
        "                df_clean, \n",
        "                time_col=None, \n",
        "                value_cols='close',\n",
        "                fill_missing_dates=True,\n",
        "                freq='D'\n",
        "            )\n",
        "            \n",
        "            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∫—Ä–æ–º–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö\n",
        "            train_ts = ts[:-TEST_SIZE]\n",
        "            \n",
        "            # –û–±—É—á–∞–µ–º univariate –º–æ–¥–µ–ª—å\n",
        "            model = RandomForest(lags=14, random_state=42)\n",
        "            model.fit(train_ts)\n",
        "            \n",
        "            # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
        "            predictions = []\n",
        "            current_ts = ts[:-TEST_SIZE]  # –ù–∞—á–∏–Ω–∞–µ–º —Å –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö\n",
        "            \n",
        "            for i in range(FORECAST_HORIZON):\n",
        "                # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 —à–∞–≥ –≤–ø–µ—Ä–µ–¥\n",
        "                pred = model.predict(n=1)\n",
        "                predictions.append(pred.values().flatten()[0])\n",
        "                \n",
        "                # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∫ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ –ø—Ä–æ–≥–Ω–æ–∑–∞\n",
        "                actual_value = ts[len(current_ts)].values().flatten()[0]\n",
        "                actual_ts = TimeSeries.from_values(\n",
        "                    [actual_value], \n",
        "                    start=ts[len(current_ts)].start_time(),\n",
        "                    freq='D'\n",
        "                )\n",
        "                current_ts = current_ts.append(actual_ts)\n",
        "            \n",
        "            # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è univariate\n",
        "            feature_importance = {}\n",
        "            \n",
        "        else:\n",
        "            # Multivariate –º–æ–¥–µ–ª—å\n",
        "            target_col = 'close'\n",
        "            past_covariates_cols = [col for col in available_features if col != target_col]\n",
        "            \n",
        "            if len(past_covariates_cols) == 0:\n",
        "                # Fallback –∫ univariate –µ—Å–ª–∏ –Ω–µ—Ç –∫–æ–≤–∞—Ä–∏–∞—Ç\n",
        "                ts = TimeSeries.from_dataframe(\n",
        "                    df_clean, \n",
        "                    time_col=None, \n",
        "                    value_cols=target_col,\n",
        "                    fill_missing_dates=True,\n",
        "                    freq='D'\n",
        "                )\n",
        "                train_ts = ts[:-TEST_SIZE]\n",
        "                \n",
        "                model = RandomForest(lags=14, random_state=42)\n",
        "                model.fit(train_ts)\n",
        "                \n",
        "                # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
        "                predictions = []\n",
        "                current_ts = ts[:-TEST_SIZE]\n",
        "                \n",
        "                for i in range(FORECAST_HORIZON):\n",
        "                    pred = model.predict(n=1)\n",
        "                    predictions.append(pred.values().flatten()[0])\n",
        "                    \n",
        "                    actual_value = ts[len(current_ts)].values().flatten()[0]\n",
        "                    actual_ts = TimeSeries.from_values(\n",
        "                        [actual_value], \n",
        "                        start=ts[len(current_ts)].start_time(),\n",
        "                        freq='D'\n",
        "                    )\n",
        "                    current_ts = current_ts.append(actual_ts)\n",
        "                \n",
        "                feature_importance = {}\n",
        "                \n",
        "            else:\n",
        "                # –°–æ–∑–¥–∞–µ–º TimeSeries –¥–ª—è —Ü–µ–ª–∏ –∏ –∫–æ–≤–∞—Ä–∏–∞—Ç\n",
        "                target_ts = TimeSeries.from_dataframe(\n",
        "                    df_clean, \n",
        "                    time_col=None, \n",
        "                    value_cols=target_col,\n",
        "                    fill_missing_dates=True,\n",
        "                    freq='D'\n",
        "                )\n",
        "                past_covariates_ts = TimeSeries.from_dataframe(\n",
        "                    df_clean, \n",
        "                    time_col=None, \n",
        "                    value_cols=past_covariates_cols,\n",
        "                    fill_missing_dates=True,\n",
        "                    freq='D'\n",
        "                )\n",
        "                \n",
        "                # –û–±—É—á–∞–µ–º –Ω–∞ –¥–∞–Ω–Ω—ã—Ö –¥–æ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–µ—Ä–∏–æ–¥–∞\n",
        "                train_target = target_ts[:-TEST_SIZE]\n",
        "                train_covariates = past_covariates_ts[:-TEST_SIZE]\n",
        "                \n",
        "                # –û–±—É—á–∞–µ–º multivariate –º–æ–¥–µ–ª—å\n",
        "                model = RandomForest(\n",
        "                    lags=14,\n",
        "                    lags_past_covariates=7,\n",
        "                    random_state=42\n",
        "                )\n",
        "                \n",
        "                model.fit(\n",
        "                    series=train_target,\n",
        "                    past_covariates=train_covariates\n",
        "                )\n",
        "                \n",
        "                # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–æ—á–∫–∞ –∑–∞ —Ç–æ—á–∫–æ–π\n",
        "                predictions = []\n",
        "                current_target = target_ts[:-TEST_SIZE]\n",
        "                current_covariates = past_covariates_ts[:-TEST_SIZE]\n",
        "                \n",
        "                for i in range(FORECAST_HORIZON):\n",
        "                    # –î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 1 —à–∞–≥ –≤–ø–µ—Ä–µ–¥\n",
        "                    pred = model.predict(\n",
        "                        n=1,\n",
        "                        past_covariates=current_covariates\n",
        "                    )\n",
        "                    predictions.append(pred.values().flatten()[0])\n",
        "                    \n",
        "                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫ –∏—Å—Ç–æ—Ä–∏–∏\n",
        "                    actual_target_value = target_ts[len(current_target)].values().flatten()[0]\n",
        "                    actual_target_ts = TimeSeries.from_values(\n",
        "                        [actual_target_value], \n",
        "                        start=target_ts[len(current_target)].start_time(),\n",
        "                        freq='D'\n",
        "                    )\n",
        "                    current_target = current_target.append(actual_target_ts)\n",
        "                    \n",
        "                    # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–æ–≤–∞—Ä–∏–∞—Ç\n",
        "                    actual_covariates_values = past_covariates_ts[len(current_covariates)].values()\n",
        "                    actual_covariates_ts = TimeSeries.from_values(\n",
        "                        actual_covariates_values, \n",
        "                        start=past_covariates_ts[len(current_covariates)].start_time(),\n",
        "                        freq='D'\n",
        "                    )\n",
        "                    current_covariates = current_covariates.append(actual_covariates_ts)\n",
        "                \n",
        "                # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "                if hasattr(model.model, 'feature_importances_'):\n",
        "                    importance_values = model.model.feature_importances_\n",
        "                    # –°–æ–∑–¥–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (lags + past_covariates)\n",
        "                    feature_names = []\n",
        "                    for lag in range(1, 15):  # lags=14\n",
        "                        feature_names.append(f'{target_col}_lag_{lag}')\n",
        "                    for lag in range(1, 8):   # lags_past_covariates=7\n",
        "                        for col in past_covariates_cols:\n",
        "                            feature_names.append(f'{col}_lag_{lag}')\n",
        "                    \n",
        "                    feature_importance = dict(zip(feature_names[:len(importance_values)], importance_values))\n",
        "                else:\n",
        "                    feature_importance = {}\n",
        "        \n",
        "        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
        "        actual_values = ts[-TEST_SIZE:-TEST_SIZE+FORECAST_HORIZON].values().flatten()\n",
        "        predicted_values = np.array(predictions)\n",
        "        \n",
        "        # –£–±–µ–∂–¥–∞–µ–º—Å—è —á—Ç–æ —Ä–∞–∑–º–µ—Ä—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç\n",
        "        min_length = min(len(actual_values), len(predicted_values))\n",
        "        actual_values = actual_values[:min_length]\n",
        "        predicted_values = predicted_values[:min_length]\n",
        "        \n",
        "        if min_length == 0:\n",
        "            print(f\"  - –ü—É—Å—Ç—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}\")\n",
        "            return None\n",
        "        \n",
        "        # RMSE\n",
        "        rmse_value = np.sqrt(mean_squared_error(actual_values, predicted_values))\n",
        "        \n",
        "        # MAPE\n",
        "        mape_value = mean_absolute_percentage_error(actual_values, predicted_values) * 100\n",
        "        \n",
        "        # DA (Directional Accuracy)\n",
        "        da_value = calculate_directional_accuracy(actual_values, predicted_values)\n",
        "        \n",
        "        results = {\n",
        "            'ticker': ticker,\n",
        "            'stage': stage,\n",
        "            'rmse': rmse_value,\n",
        "            'mape': mape_value,\n",
        "            'da': da_value,\n",
        "            'feature_count': len(available_features),\n",
        "            'feature_importance': feature_importance\n",
        "        }\n",
        "        \n",
        "        print(f\"  - {ticker}: RMSE={rmse_value:.4f}, MAPE={mape_value:.2f}%, DA={da_value:.3f}, Features={len(available_features)}\")\n",
        "        \n",
        "        return results\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"  - –û—à–∏–±–∫–∞ –¥–ª—è {ticker} –Ω–∞ —ç—Ç–∞–ø–µ {stage}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"–í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ —ç—Ç–∞–ø–∞–º\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —ç—Ç–∞–ø–æ–≤\n",
        "stage_names = {\n",
        "    1: \"–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (close)\",\n",
        "    2: \"+ –ê–Ω–æ–º–∞–ª–∏–∏\", \n",
        "    3: \"+ –ù–æ–≤–æ—Å—Ç–∏\",\n",
        "    4: \"+ –ü–∞—Ç—Ç–µ—Ä–Ω—ã —Å–≤–µ—á–µ–π\",\n",
        "    5: \"+ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã\",\n",
        "    6: \"+ TSFresh –ø—Ä–∏–∑–Ω–∞–∫–∏\", \n",
        "    7: \"+ PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\"\n",
        "}\n",
        "\n",
        "# –ö–æ–Ω—Ç–µ–π–Ω–µ—Ä –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n",
        "all_results = []\n",
        "stage_summaries = []\n",
        "\n",
        "print(\"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\\n\")\n",
        "\n",
        "# –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –≤—Å–µ–º —ç—Ç–∞–ø–∞–º\n",
        "for stage in range(1, 8):\n",
        "    print(f\"üìä –≠–¢–ê–ü {stage}: {stage_names[stage]}\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    stage_results = []\n",
        "    \n",
        "    # –û—Ü–µ–Ω–∏–≤–∞–µ–º –∫–∞–∂–¥—ã–π —Ç–∏–∫–µ—Ä –Ω–∞ —Ç–µ–∫—É—â–µ–º —ç—Ç–∞–ø–µ\n",
        "    for ticker in tickers:\n",
        "        if ticker in data:\n",
        "            result = evaluate_model_for_ticker(data[ticker], ticker, stage)\n",
        "            if result is not None:\n",
        "                all_results.append(result)\n",
        "                stage_results.append(result)\n",
        "    \n",
        "    # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–æ —ç—Ç–∞–ø—É\n",
        "    if stage_results:\n",
        "        avg_rmse = np.mean([r['rmse'] for r in stage_results])\n",
        "        avg_mape = np.mean([r['mape'] for r in stage_results])\n",
        "        avg_da = np.mean([r['da'] for r in stage_results if not np.isnan(r['da'])])\n",
        "        avg_features = np.mean([r['feature_count'] for r in stage_results])\n",
        "        \n",
        "        stage_summary = {\n",
        "            'stage': stage,\n",
        "            'stage_name': stage_names[stage],\n",
        "            'avg_rmse': avg_rmse,\n",
        "            'avg_mape': avg_mape,\n",
        "            'avg_da': avg_da,\n",
        "            'avg_features': avg_features,\n",
        "            'ticker_count': len(stage_results)\n",
        "        }\n",
        "        \n",
        "        stage_summaries.append(stage_summary)\n",
        "        \n",
        "        print(f\"\\nüìà –°—Ä–µ–¥–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —ç—Ç–∞–ø–∞ {stage}:\")\n",
        "        print(f\"   RMSE: {avg_rmse:.4f}\")\n",
        "        print(f\"   MAPE: {avg_mape:.2f}%\")\n",
        "        print(f\"   DA: {avg_da:.3f}\")\n",
        "        print(f\"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {avg_features:.1f}\")\n",
        "        print(f\"   –£—Å–ø–µ—à–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤: {len(stage_results)}/{len(tickers)}\")\n",
        "    else:\n",
        "        print(f\"‚ùå –ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {stage}\")\n",
        "    \n",
        "    print(\"\\\\n\" + \"=\"*50 + \"\\\\n\")\n",
        "\n",
        "print(f\"‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω! –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ {len(stage_summaries)} —ç—Ç–∞–ø–æ–≤\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –ò—Ç–æ–≥–æ–≤–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—É—é —Ç–∞–±–ª–∏—Ü—É\n",
        "results_df = pd.DataFrame(stage_summaries)\n",
        "\n",
        "if not results_df.empty:\n",
        "    print(\"üìä –ò–¢–û–ì–û–í–ê–Ø –¢–ê–ë–õ–ò–¶–ê –†–ï–ó–£–õ–¨–¢–ê–¢–û–í\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—É –¥–ª—è –∫—Ä–∞—Å–∏–≤–æ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    display_df = results_df.copy()\n",
        "    display_df['RMSE'] = display_df['avg_rmse'].apply(lambda x: f\"{x:.4f}\")\n",
        "    display_df['MAPE (%)'] = display_df['avg_mape'].apply(lambda x: f\"{x:.2f}\")\n",
        "    display_df['DA'] = display_df['avg_da'].apply(lambda x: f\"{x:.3f}\")\n",
        "    display_df['–ü—Ä–∏–∑–Ω–∞–∫–æ–≤'] = display_df['avg_features'].apply(lambda x: f\"{x:.0f}\")\n",
        "    display_df['–¢–∏–∫–µ—Ä–æ–≤'] = display_df['ticker_count'].apply(lambda x: f\"{x}\")\n",
        "    \n",
        "    # –í—ã–±–∏—Ä–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "    final_table = display_df[['stage', 'stage_name', 'RMSE', 'MAPE (%)', 'DA', '–ü—Ä–∏–∑–Ω–∞–∫–æ–≤', '–¢–∏–∫–µ—Ä–æ–≤']].copy()\n",
        "    final_table.columns = ['–≠—Ç–∞–ø', '–û–ø–∏—Å–∞–Ω–∏–µ', 'RMSE', 'MAPE (%)', 'DA', '–ü—Ä–∏–∑–Ω–∞–∫–æ–≤', '–¢–∏–∫–µ—Ä–æ–≤']\n",
        "    \n",
        "    print(final_table.to_string(index=False))\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–∞–±–ª–∏—Ü—É\n",
        "    results_df.to_csv(f\"{OUTPUT_PATH}progressive_analysis_summary.csv\", index=False)\n",
        "    final_table.to_csv(f\"{OUTPUT_PATH}progressive_analysis_formatted.csv\", index=False)\n",
        "    \n",
        "    print(f\"\\\\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {OUTPUT_PATH}\")\n",
        "    \n",
        "    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏\n",
        "    if len(results_df) > 1:\n",
        "        base_rmse = results_df.iloc[0]['avg_rmse']\n",
        "        base_mape = results_df.iloc[0]['avg_mape'] \n",
        "        base_da = results_df.iloc[0]['avg_da']\n",
        "        \n",
        "        print(\"\\\\nüìà –ò–ó–ú–ï–ù–ï–ù–ò–Ø –û–¢–ù–û–°–ò–¢–ï–õ–¨–ù–û –ë–ê–ó–û–í–û–ô –ú–û–î–ï–õ–ò:\")\n",
        "        print(\"=\" * 60)\n",
        "        \n",
        "        for i, row in results_df.iterrows():\n",
        "            if i == 0:\n",
        "                continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å\n",
        "                \n",
        "            rmse_change = ((row['avg_rmse'] - base_rmse) / base_rmse) * 100\n",
        "            mape_change = ((row['avg_mape'] - base_mape) / base_mape) * 100\n",
        "            da_change = ((row['avg_da'] - base_da) / base_da) * 100\n",
        "            \n",
        "            print(f\"–≠—Ç–∞–ø {row['stage']} - {row['stage_name']}:\")\n",
        "            print(f\"  RMSE: {rmse_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if rmse_change < 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "            print(f\"  MAPE: {mape_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if mape_change < 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "            print(f\"  DA: {da_change:+.1f}% ({'—É–ª—É—á—à–µ–Ω–∏–µ' if da_change > 0 else '—É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "            print()\n",
        "            \n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏—Ç–æ–≥–æ–≤–æ–π —Ç–∞–±–ª–∏—Ü—ã\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not results_df.empty:\n",
        "    # –°–æ–∑–¥–∞–µ–º –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "    fig.suptitle('–ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è', fontsize=16, fontweight='bold')\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ 1: RMSE\n",
        "    axes[0,0].plot(results_df['stage'], results_df['avg_rmse'], 'o-', linewidth=2, markersize=8, color='red')\n",
        "    axes[0,0].set_title('RMSE (—Å—Ä–µ–¥–Ω–µ–∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–∞—è –æ—à–∏–±–∫–∞)', fontweight='bold')\n",
        "    axes[0,0].set_xlabel('–≠—Ç–∞–ø')\n",
        "    axes[0,0].set_ylabel('RMSE')\n",
        "    axes[0,0].grid(True, alpha=0.3)\n",
        "    axes[0,0].set_xticks(results_df['stage'])\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ 2: MAPE\n",
        "    axes[0,1].plot(results_df['stage'], results_df['avg_mape'], 'o-', linewidth=2, markersize=8, color='orange')\n",
        "    axes[0,1].set_title('MAPE (—Å—Ä–µ–¥–Ω—è—è –∞–±—Å–æ–ª—é—Ç–Ω–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è –æ—à–∏–±–∫–∞)', fontweight='bold')\n",
        "    axes[0,1].set_xlabel('–≠—Ç–∞–ø')\n",
        "    axes[0,1].set_ylabel('MAPE (%)')\n",
        "    axes[0,1].grid(True, alpha=0.3)\n",
        "    axes[0,1].set_xticks(results_df['stage'])\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ 3: DA\n",
        "    axes[1,0].plot(results_df['stage'], results_df['avg_da'], 'o-', linewidth=2, markersize=8, color='green')\n",
        "    axes[1,0].set_title('DA (—Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è)', fontweight='bold')\n",
        "    axes[1,0].set_xlabel('–≠—Ç–∞–ø')\n",
        "    axes[1,0].set_ylabel('DA')\n",
        "    axes[1,0].grid(True, alpha=0.3)\n",
        "    axes[1,0].set_xticks(results_df['stage'])\n",
        "    axes[1,0].set_ylim(0, 1)\n",
        "    \n",
        "    # –ì—Ä–∞—Ñ–∏–∫ 4: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "    axes[1,1].plot(results_df['stage'], results_df['avg_features'], 'o-', linewidth=2, markersize=8, color='purple')\n",
        "    axes[1,1].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤', fontweight='bold')\n",
        "    axes[1,1].set_xlabel('–≠—Ç–∞–ø')\n",
        "    axes[1,1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤')\n",
        "    axes[1,1].grid(True, alpha=0.3)\n",
        "    axes[1,1].set_xticks(results_df['stage'])\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_PATH}metrics_progression.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º heatmap —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "    \n",
        "    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è heatmap\n",
        "    heatmap_data = results_df[['stage', 'avg_rmse', 'avg_mape', 'avg_da']].copy()\n",
        "    \n",
        "    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ (min-max scaling)\n",
        "    for col in ['avg_rmse', 'avg_mape', 'avg_da']:\n",
        "        min_val = heatmap_data[col].min()\n",
        "        max_val = heatmap_data[col].max()\n",
        "        if max_val > min_val:\n",
        "            heatmap_data[col] = (heatmap_data[col] - min_val) / (max_val - min_val)\n",
        "    \n",
        "    # –î–ª—è RMSE –∏ MAPE - –∏–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º (–º–µ–Ω—å—à–µ = –ª—É—á—à–µ)\n",
        "    heatmap_data['avg_rmse'] = 1 - heatmap_data['avg_rmse']\n",
        "    heatmap_data['avg_mape'] = 1 - heatmap_data['avg_mape']\n",
        "    \n",
        "    # –°–æ–∑–¥–∞–µ–º heatmap\n",
        "    heatmap_matrix = heatmap_data[['avg_rmse', 'avg_mape', 'avg_da']].T\n",
        "    heatmap_matrix.columns = [f\"–≠—Ç–∞–ø {i}\" for i in results_df['stage']]\n",
        "    heatmap_matrix.index = ['RMSE (–Ω–æ—Ä–º.)', 'MAPE (–Ω–æ—Ä–º.)', 'DA']\n",
        "    \n",
        "    sns.heatmap(heatmap_matrix, annot=True, cmap='RdYlGn', center=0.5, \n",
        "                cbar_kws={'label': '–ö–∞—á–µ—Å—Ç–≤–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–æ)'}, ax=ax)\n",
        "    ax.set_title('–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–µ–π –ø–æ —ç—Ç–∞–ø–∞–º\\\\n(–∑–µ–ª–µ–Ω—ã–π = –ª—É—á—à–µ, –∫—Ä–∞—Å–Ω—ã–π = —Ö—É–∂–µ)', \n",
        "                 fontweight='bold', fontsize=14)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_PATH}quality_heatmap.png\", dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —ç—Ç–∞–ø–∞–º\n",
        "print(\"üéØ –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í –ü–û –≠–¢–ê–ü–ê–ú\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "stage_importance = {}\n",
        "\n",
        "for result in all_results:\n",
        "    stage = result['stage']\n",
        "    if stage not in stage_importance:\n",
        "        stage_importance[stage] = {}\n",
        "    \n",
        "    # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ —Ç–∏–∫–µ—Ä–∞–º\n",
        "    for feature, importance in result['feature_importance'].items():\n",
        "        if feature not in stage_importance[stage]:\n",
        "            stage_importance[stage][feature] = []\n",
        "        stage_importance[stage][feature].append(importance)\n",
        "\n",
        "# –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "avg_importance = {}\n",
        "for stage, features in stage_importance.items():\n",
        "    avg_importance[stage] = {}\n",
        "    for feature, values in features.items():\n",
        "        avg_importance[stage][feature] = np.mean(values)\n",
        "\n",
        "# –í—ã–≤–æ–¥–∏–º —Ç–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "for stage in sorted(avg_importance.keys()):\n",
        "    if stage == 1:\n",
        "        print(f\"\\\\nüìä –≠—Ç–∞–ø {stage}: {stage_names[stage]}\")\n",
        "        print(\"   (–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è univariate –º–æ–¥–µ–ª–∏)\")\n",
        "        continue\n",
        "        \n",
        "    print(f\"\\\\nüìä –≠—Ç–∞–ø {stage}: {stage_names[stage]}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –≤–∞–∂–Ω–æ—Å—Ç–∏\n",
        "    sorted_features = sorted(avg_importance[stage].items(), key=lambda x: x[1], reverse=True)\n",
        "    \n",
        "    if sorted_features:\n",
        "        print(\"   –¢–æ–ø-10 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:\")\n",
        "        for i, (feature, importance) in enumerate(sorted_features[:10], 1):\n",
        "            print(f\"   {i:2d}. {feature}: {importance:.4f}\")\n",
        "    else:\n",
        "        print(\"   –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç—Ç–∞–ø–∞\n",
        "if avg_importance:\n",
        "    last_stage = max(avg_importance.keys())\n",
        "    if last_stage in avg_importance and avg_importance[last_stage]:\n",
        "        \n",
        "        print(f\"\\\\nüé® –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —ç—Ç–∞–ø–∞ {last_stage}\")\n",
        "        \n",
        "        # –ë–µ—Ä–µ–º —Ç–æ–ø-20 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏\n",
        "        sorted_features = sorted(avg_importance[last_stage].items(), key=lambda x: x[1], reverse=True)\n",
        "        top_features = sorted_features[:20]\n",
        "        \n",
        "        if top_features:\n",
        "            features_names = [f[0] for f in top_features]\n",
        "            features_importance = [f[1] for f in top_features]\n",
        "            \n",
        "            # –°–æ–∑–¥–∞–µ–º –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(12, 10))\n",
        "            \n",
        "            bars = ax.barh(range(len(features_names)), features_importance, color='skyblue', alpha=0.8)\n",
        "            \n",
        "            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –æ—Å–∏\n",
        "            ax.set_yticks(range(len(features_names)))\n",
        "            ax.set_yticklabels(features_names, fontsize=10)\n",
        "            ax.set_xlabel('–í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∞', fontsize=12)\n",
        "            ax.set_title(f'–¢–æ–ø-20 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ - {stage_names[last_stage]}', fontsize=14, fontweight='bold')\n",
        "            \n",
        "            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –Ω–∞ —Å—Ç–æ–ª–±—Ü—ã\n",
        "            for i, bar in enumerate(bars):\n",
        "                width = bar.get_width()\n",
        "                ax.text(width + width*0.01, bar.get_y() + bar.get_height()/2, \n",
        "                       f'{width:.4f}', ha='left', va='center', fontsize=9)\n",
        "            \n",
        "            # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è\n",
        "            ax.invert_yaxis()\n",
        "            ax.grid(axis='x', alpha=0.3)\n",
        "            \n",
        "            plt.tight_layout()\n",
        "            plt.savefig(f\"{OUTPUT_PATH}feature_importance_stage_{last_stage}.png\", dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º —Å–≤–æ–¥–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "print(\"\\\\nüìã –°–í–û–î–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê –í–ê–ñ–ù–û–°–¢–ò –ü–†–ò–ó–ù–ê–ö–û–í\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
        "importance_data = []\n",
        "for stage in sorted(avg_importance.keys()):\n",
        "    if stage == 1:\n",
        "        continue  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º univariate –º–æ–¥–µ–ª—å\n",
        "    \n",
        "    sorted_features = sorted(avg_importance[stage].items(), key=lambda x: x[1], reverse=True)\n",
        "    for rank, (feature, importance) in enumerate(sorted_features[:5], 1):  # –¢–æ–ø-5 –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞\n",
        "        importance_data.append({\n",
        "            '–≠—Ç–∞–ø': stage,\n",
        "            '–û–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–∞–ø–∞': stage_names[stage],\n",
        "            '–†–∞–Ω–≥': rank,\n",
        "            '–ü—Ä–∏–∑–Ω–∞–∫': feature,\n",
        "            '–í–∞–∂–Ω–æ—Å—Ç—å': importance\n",
        "        })\n",
        "\n",
        "if importance_data:\n",
        "    importance_df = pd.DataFrame(importance_data)\n",
        "    \n",
        "    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —ç—Ç–∞–ø–∞–º –∏ –≤—ã–≤–æ–¥–∏–º\n",
        "    for stage in sorted(importance_df['–≠—Ç–∞–ø'].unique()):\n",
        "        stage_data = importance_df[importance_df['–≠—Ç–∞–ø'] == stage]\n",
        "        print(f\"\\\\n–≠—Ç–∞–ø {stage}: {stage_data.iloc[0]['–û–ø–∏—Å–∞–Ω–∏–µ —ç—Ç–∞–ø–∞']}\")\n",
        "        for _, row in stage_data.iterrows():\n",
        "            print(f\"  {row['–†–∞–Ω–≥']}. {row['–ü—Ä–∏–∑–Ω–∞–∫']}: {row['–í–∞–∂–Ω–æ—Å—Ç—å']:.4f}\")\n",
        "    \n",
        "    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–∞–π–ª\n",
        "    importance_df.to_csv(f\"{OUTPUT_PATH}feature_importance_summary.csv\", index=False)\n",
        "    print(f\"\\\\nüíæ –¢–∞–±–ª–∏—Ü–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {OUTPUT_PATH}feature_importance_summary.csv\")\n",
        "\n",
        "print(\"\\\\n‚úÖ –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## –í—ã–≤–æ–¥—ã –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üéØ –†–ï–ó–Æ–ú–ï –ü–†–û–ì–†–ï–°–°–ò–í–ù–û–ì–û –ê–ù–ê–õ–ò–ó–ê\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "if results_df.empty:\n",
        "    print(\"‚ùå –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\")\n",
        "else:\n",
        "    print(f\"‚úÖ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results_df)} —ç—Ç–∞–ø–æ–≤ –¥–ª—è {len(tickers)} —Ç–∏–∫–µ—Ä–æ–≤\")\n",
        "    print(f\"üìä –°–æ–±—Ä–∞–Ω–æ {len(all_results)} —É—Å–ø–µ—à–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤\")\n",
        "    \n",
        "    # –ù–∞–π–¥–µ–º –ª—É—á—à–∏–π —ç—Ç–∞–ø –ø–æ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–µ\n",
        "    best_rmse_stage = results_df.loc[results_df['avg_rmse'].idxmin()]\n",
        "    best_mape_stage = results_df.loc[results_df['avg_mape'].idxmin()]\n",
        "    best_da_stage = results_df.loc[results_df['avg_da'].idxmax()]\n",
        "    \n",
        "    print(\"\\\\nüèÜ –õ–£–ß–®–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´:\")\n",
        "    print(f\"   –õ—É—á—à–∏–π RMSE: –≠—Ç–∞–ø {best_rmse_stage['stage']} ({best_rmse_stage['stage_name']}) - {best_rmse_stage['avg_rmse']:.4f}\")\n",
        "    print(f\"   –õ—É—á—à–∏–π MAPE: –≠—Ç–∞–ø {best_mape_stage['stage']} ({best_mape_stage['stage_name']}) - {best_mape_stage['avg_mape']:.2f}%\")\n",
        "    print(f\"   –õ—É—á—à–∏–π DA: –≠—Ç–∞–ø {best_da_stage['stage']} ({best_da_stage['stage_name']}) - {best_da_stage['avg_da']:.3f}\")\n",
        "    \n",
        "    # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞ –∫–∞—á–µ—Å—Ç–≤–∞\n",
        "    base_metrics = results_df.iloc[0]\n",
        "    final_metrics = results_df.iloc[-1]\n",
        "    \n",
        "    rmse_change = ((final_metrics['avg_rmse'] - base_metrics['avg_rmse']) / base_metrics['avg_rmse']) * 100\n",
        "    mape_change = ((final_metrics['avg_mape'] - base_metrics['avg_mape']) / base_metrics['avg_mape']) * 100\n",
        "    da_change = ((final_metrics['avg_da'] - base_metrics['avg_da']) / base_metrics['avg_da']) * 100\n",
        "    \n",
        "    print(\"\\\\nüìà –û–ë–©–ï–ï –£–õ–£–ß–®–ï–ù–ò–ï (—Ñ–∏–Ω–∞–ª—å–Ω—ã–π —ç—Ç–∞–ø vs –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å):\")\n",
        "    print(f\"   RMSE: {rmse_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if rmse_change < 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "    print(f\"   MAPE: {mape_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if mape_change < 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "    print(f\"   DA: {da_change:+.1f}% ({'‚úÖ —É–ª—É—á—à–µ–Ω–∏–µ' if da_change > 0 else '‚ùå —É—Ö—É–¥—à–µ–Ω–∏–µ'})\")\n",
        "    \n",
        "    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏\n",
        "    print(\"\\\\nüí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò:\")\n",
        "    \n",
        "    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–∞–∏–±–æ–ª–µ–µ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–µ —ç—Ç–∞–ø—ã\n",
        "    rmse_improvements = []\n",
        "    mape_improvements = []\n",
        "    da_improvements = []\n",
        "    \n",
        "    for i in range(1, len(results_df)):\n",
        "        prev_metrics = results_df.iloc[i-1]\n",
        "        curr_metrics = results_df.iloc[i]\n",
        "        \n",
        "        rmse_change = ((curr_metrics['avg_rmse'] - prev_metrics['avg_rmse']) / prev_metrics['avg_rmse']) * 100\n",
        "        mape_change = ((curr_metrics['avg_mape'] - prev_metrics['avg_mape']) / prev_metrics['avg_mape']) * 100\n",
        "        da_change = ((curr_metrics['avg_da'] - prev_metrics['avg_da']) / prev_metrics['avg_da']) * 100\n",
        "        \n",
        "        rmse_improvements.append((curr_metrics['stage'], rmse_change))\n",
        "        mape_improvements.append((curr_metrics['stage'], mape_change))\n",
        "        da_improvements.append((curr_metrics['stage'], da_change))\n",
        "    \n",
        "    # –ù–∞—Ö–æ–¥–∏–º —ç—Ç–∞–ø—ã —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ —É–ª—É—á—à–µ–Ω–∏—è–º–∏\n",
        "    best_rmse_improvement = min(rmse_improvements, key=lambda x: x[1])\n",
        "    best_mape_improvement = min(mape_improvements, key=lambda x: x[1])\n",
        "    best_da_improvement = max(da_improvements, key=lambda x: x[1])\n",
        "    \n",
        "    print(f\"   1. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ RMSE –¥–∞–ª —ç—Ç–∞–ø {best_rmse_improvement[0]} ({best_rmse_improvement[1]:+.1f}%)\")\n",
        "    print(f\"   2. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ MAPE –¥–∞–ª —ç—Ç–∞–ø {best_mape_improvement[0]} ({best_mape_improvement[1]:+.1f}%)\")\n",
        "    print(f\"   3. –ù–∞–∏–±–æ–ª—å—à–µ–µ —É–ª—É—á—à–µ–Ω–∏–µ DA –¥–∞–ª —ç—Ç–∞–ø {best_da_improvement[0]} ({best_da_improvement[1]:+.1f}%)\")\n",
        "    \n",
        "    # –ê–Ω–∞–ª–∏–∑ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –∫ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏\n",
        "    print(\"\\\\n‚öñÔ∏è –ê–ù–ê–õ–ò–ó –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–ò (–∫–∞—á–µ—Å—Ç–≤–æ vs —Å–ª–æ–∂–Ω–æ—Å—Ç—å):\")\n",
        "    for _, row in results_df.iterrows():\n",
        "        efficiency_score = (1 - row['avg_rmse']/base_metrics['avg_rmse']) / (row['avg_features']/base_metrics['avg_features'])\n",
        "        print(f\"   –≠—Ç–∞–ø {row['stage']}: –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å = {efficiency_score:.3f} (–∫–∞—á–µ—Å—Ç–≤–æ/—Å–ª–æ–∂–Ω–æ—Å—Ç—å)\")\n",
        "\n",
        "print(\"\\\\nüéâ –ü–†–û–ì–†–ï–°–°–ò–í–ù–´–ô –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù!\")\n",
        "print(f\"üìÅ –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {OUTPUT_PATH}\")\n",
        "print(\"\\\\n–§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:\")\n",
        "print(\"   - progressive_analysis_summary.csv - —Å–≤–æ–¥–∫–∞ –ø–æ —ç—Ç–∞–ø–∞–º\")\n",
        "print(\"   - progressive_analysis_formatted.csv - —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞\")\n",
        "print(\"   - feature_importance_summary.csv - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\")\n",
        "print(\"   - metrics_progression.png - –≥—Ä–∞—Ñ–∏–∫–∏ –∏–∑–º–µ–Ω–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫\")\n",
        "print(\"   - quality_heatmap.png - —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∫–∞—á–µ—Å—Ç–≤–∞\")\n",
        "print(\"   - feature_importance_stage_N.png - –≤–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —ç—Ç–∞–ø–∞\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
