{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Интеграция оценок новостей в временные ряды с аномалиями\n",
    "\n",
    "Этот ноутбук предназначен для интеграции оценок новостей из различных моделей в временные ряды с обнаруженными аномалиями.\n",
    "\n",
    "## Задачи:\n",
    "1. Загрузить данные из series_with_anomaly и news\n",
    "2. Добавить 5 новых колонок:\n",
    "   - Средняя оценка за день от KerAI\n",
    "   - Средняя оценка за день от Гигачат\n",
    "   - Средняя оценка за день от DeepSeek\n",
    "   - Текст заголовков за день\n",
    "   - Взвешенная оценка с затухающим сигналом\n",
    "3. Сохранить результаты в папку series_with_news\n",
    "4. Визуализировать цену и новостной фон\n",
    "5. Посчитать корреляцию между оценками и доходностями\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Настройка отображения\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Установка русского языка для графиков\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans', 'Liberation Sans', 'sans-serif']\n",
    "\n",
    "print(\"Библиотеки загружены успешно\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пути к данным\n",
    "SERIES_PATH = 'data/series_with_anomaly/'\n",
    "NEWS_PATH = 'data/news/'\n",
    "OUTPUT_PATH = 'data/series_with_news/'\n",
    "\n",
    "# Создаем выходную папку если её нет\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Список тикеров\n",
    "tickers = ['AFLT', 'LKOH', 'MOEX', 'NVTK', 'PIKK', 'SBER', 'VKCO', 'VTBR', 'X5', 'YDEX']\n",
    "\n",
    "print(f\"Рабочие директории:\")\n",
    "print(f\"- Временные ряды с аномалиями: {SERIES_PATH}\")\n",
    "print(f\"- Новости: {NEWS_PATH}\")\n",
    "print(f\"- Выходные данные: {OUTPUT_PATH}\")\n",
    "print(f\"\\nТикеры для обработки: {tickers}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Функции для обработки данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_series_data(ticker):\n",
    "    \"\"\"Загружает данные временного ряда для тикера\"\"\"\n",
    "    file_path = f\"{SERIES_PATH}{ticker}_with_anomalies.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    return df\n",
    "\n",
    "def load_news_data(ticker):\n",
    "    \"\"\"Загружает новостные данные для тикера\"\"\"\n",
    "    file_path = f\"{NEWS_PATH}{ticker}_evaluated_news.csv\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['published'] = pd.to_datetime(df['published'])\n",
    "    df['date'] = df['published'].dt.date\n",
    "    return df\n",
    "\n",
    "def safe_convert_to_numeric(value, default=0):\n",
    "    \"\"\"Безопасно конвертирует значение в число\"\"\"\n",
    "    if pd.isna(value) or value == 'Error_Evaluation_Gigachat':\n",
    "        return default\n",
    "    try:\n",
    "        return float(value)\n",
    "    except:\n",
    "        return default\n",
    "\n",
    "def process_daily_news_scores(news_df):\n",
    "    \"\"\"Обрабатывает новостные данные и вычисляет дневные оценки\"\"\"\n",
    "    # Создаем копию для обработки\n",
    "    df = news_df.copy()\n",
    "    \n",
    "    # Конвертируем оценки в числовые значения\n",
    "    df['kernai_score'] = df['KernAI_stock_news_distilbert'].apply(lambda x: safe_convert_to_numeric(x, 0))\n",
    "    df['gigachat_score'] = df['gigachat_score_only'].apply(lambda x: safe_convert_to_numeric(x, 0))\n",
    "    df['deepseek_score'] = df['deepseek_score'].apply(lambda x: safe_convert_to_numeric(x, 0))\n",
    "    \n",
    "    # Нормализуем оценки к диапазону [-1, 1]\n",
    "    df['kernai_norm'] = df['kernai_score'] / 100.0  # KernAI уже в диапазоне [-100, 100]\n",
    "    df['gigachat_norm'] = df['gigachat_score'] / 100.0  # Gigachat в диапазоне [-100, 100]\n",
    "    df['deepseek_norm'] = df['deepseek_score'] / 100.0  # DeepSeek в диапазоне [-100, 100]\n",
    "    \n",
    "    # Сортируем новости по времени (от вечера к утру следующего дня)\n",
    "    df = df.sort_values('published')\n",
    "    \n",
    "    # Группируем по дням и вычисляем агрегированные показатели\n",
    "    daily_scores = df.groupby('date').agg({\n",
    "        'kernai_norm': 'mean',\n",
    "        'gigachat_norm': 'mean', \n",
    "        'deepseek_norm': 'mean',\n",
    "        'title': lambda x: ' | '.join([f\"{row['published'].strftime('%H:%M')}: {row['title']}\" \n",
    "                                      for _, row in df[df['date'] == x.name].iterrows()])\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Переименовываем колонки\n",
    "    daily_scores.columns = ['date', 'kernai_daily_avg', 'gigachat_daily_avg', \n",
    "                           'deepseek_daily_avg', 'daily_headlines']\n",
    "    \n",
    "    return daily_scores\n",
    "\n",
    "print(\"Функции загрузки и обработки данных определены\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_score_with_decay(daily_scores):\n",
    "    \"\"\"Вычисляет взвешенную оценку с затухающим сигналом\"\"\"\n",
    "    df = daily_scores.copy()\n",
    "    \n",
    "    # Веса для моделей\n",
    "    weights = {\n",
    "        'kernai': 0.2,\n",
    "        'gigachat': 0.3,\n",
    "        'deepseek': 0.5\n",
    "    }\n",
    "    \n",
    "    # Вычисляем базовую взвешенную оценку\n",
    "    df['weighted_score_base'] = (\n",
    "        weights['kernai'] * df['kernai_daily_avg'] +\n",
    "        weights['gigachat'] * df['gigachat_daily_avg'] +\n",
    "        weights['deepseek'] * df['deepseek_daily_avg']\n",
    "    )\n",
    "    \n",
    "    # Инициализируем итоговую взвешенную оценку\n",
    "    df['weighted_score_with_decay'] = df['weighted_score_base'].copy()\n",
    "    \n",
    "    # Применяем затухающий сигнал\n",
    "    for i in range(1, len(df)):\n",
    "        # Затухание предыдущего дня (деление на 2)\n",
    "        decay_from_prev = df.iloc[i-1]['weighted_score_with_decay'] / 2\n",
    "        \n",
    "        # Если есть данные за 2 дня назад, добавляем затухание (деление на 4)\n",
    "        if i >= 2:\n",
    "            decay_from_prev2 = df.iloc[i-2]['weighted_score_with_decay'] / 4\n",
    "        else:\n",
    "            decay_from_prev2 = 0\n",
    "            \n",
    "        # Если есть данные за 3 дня назад, влияние практически исчезает (деление на 8)\n",
    "        if i >= 3:\n",
    "            decay_from_prev3 = df.iloc[i-3]['weighted_score_with_decay'] / 8\n",
    "        else:\n",
    "            decay_from_prev3 = 0\n",
    "        \n",
    "        # Итоговый сигнал = текущий день + затухающие сигналы прошлых дней\n",
    "        # Ограничиваем влияние прошлых дней, чтобы не было слишком сильного накопления\n",
    "        total_decay = decay_from_prev + decay_from_prev2 * 0.5 + decay_from_prev3 * 0.25\n",
    "        \n",
    "        df.iloc[i, df.columns.get_loc('weighted_score_with_decay')] = (\n",
    "            df.iloc[i]['weighted_score_base'] + total_decay\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"Функция расчета взвешенной оценки с затуханием определена\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Обработка данных для всех тикеров\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь для хранения обработанных данных\n",
    "integrated_data = {}\n",
    "correlation_results = {}\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"\\nОбработка тикера: {ticker}\")\n",
    "    \n",
    "    try:\n",
    "        # Загружаем данные\n",
    "        series_df = load_series_data(ticker)\n",
    "        news_df = load_news_data(ticker)\n",
    "        \n",
    "        print(f\"  - Загружено {len(series_df)} записей временного ряда\")\n",
    "        print(f\"  - Загружено {len(news_df)} новостей\")\n",
    "        \n",
    "        # Обрабатываем новостные оценки\n",
    "        daily_news = process_daily_news_scores(news_df)\n",
    "        \n",
    "        # Вычисляем взвешенную оценку с затуханием\n",
    "        daily_news_with_decay = calculate_weighted_score_with_decay(daily_news)\n",
    "        \n",
    "        print(f\"  - Обработано {len(daily_news_with_decay)} дневных новостных оценок\")\n",
    "        \n",
    "        # Объединяем данные временного ряда с новостными оценками\n",
    "        integrated_df = series_df.merge(\n",
    "            daily_news_with_decay, \n",
    "            on='date', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Заполняем пропуски нулями для новостных данных\n",
    "        news_columns = ['kernai_daily_avg', 'gigachat_daily_avg', 'deepseek_daily_avg', \n",
    "                       'weighted_score_base', 'weighted_score_with_decay']\n",
    "        for col in news_columns:\n",
    "            integrated_df[col] = integrated_df[col].fillna(0)\n",
    "        \n",
    "        # Заполняем пропуски в заголовках\n",
    "        integrated_df['daily_headlines'] = integrated_df['daily_headlines'].fillna('Новости отсутствуют')\n",
    "        \n",
    "        # Вычисляем доходности\n",
    "        integrated_df['return'] = integrated_df['close'].pct_change()\n",
    "        \n",
    "        # Сохраняем результат\n",
    "        output_file = f\"{OUTPUT_PATH}{ticker}_with_news.csv\"\n",
    "        integrated_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Сохраняем для дальнейшего анализа\n",
    "        integrated_data[ticker] = integrated_df\n",
    "        \n",
    "        print(f\"  - Сохранено в {output_file}\")\n",
    "        print(f\"  - Итоговые размеры: {integrated_df.shape}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  - Ошибка при обработке {ticker}: {str(e)}\")\n",
    "\n",
    "print(f\"\\nОбработка завершена. Обработано {len(integrated_data)} тикеров.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Визуализация результатов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_price_vs_news_sentiment(ticker_data, ticker_name):\n",
    "    \"\"\"Создает график цены и новостного фона\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 12), sharex=True)\n",
    "    \n",
    "    # График цены\n",
    "    ax1.plot(ticker_data['timestamp'], ticker_data['close'], \n",
    "             color='blue', linewidth=1.5, label='Цена закрытия')\n",
    "    \n",
    "    # Выделяем аномалии\n",
    "    anomaly_data = ticker_data[ticker_data['anomaly'] == 1.0]\n",
    "    if len(anomaly_data) > 0:\n",
    "        ax1.scatter(anomaly_data['timestamp'], anomaly_data['close'], \n",
    "                   color='red', s=30, alpha=0.7, label='Аномалии', zorder=5)\n",
    "    \n",
    "    ax1.set_ylabel('Цена, руб.', fontsize=12)\n",
    "    ax1.set_title(f'{ticker_name}: Цена и новостной фон', fontsize=14, fontweight='bold')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # График новостного фона\n",
    "    ax2.plot(ticker_data['timestamp'], ticker_data['weighted_score_with_decay'], \n",
    "             color='green', linewidth=2, label='Взвешенная оценка с затуханием')\n",
    "    ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Заливка положительных и отрицательных значений\n",
    "    ax2.fill_between(ticker_data['timestamp'], ticker_data['weighted_score_with_decay'], 0,\n",
    "                     where=(ticker_data['weighted_score_with_decay'] >= 0),\n",
    "                     color='green', alpha=0.3, label='Позитивный фон')\n",
    "    ax2.fill_between(ticker_data['timestamp'], ticker_data['weighted_score_with_decay'], 0,\n",
    "                     where=(ticker_data['weighted_score_with_decay'] < 0),\n",
    "                     color='red', alpha=0.3, label='Негативный фон')\n",
    "    \n",
    "    ax2.set_xlabel('Дата', fontsize=12)\n",
    "    ax2.set_ylabel('Новостной фон', fontsize=12)\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "\n",
    "# Создаем графики для каждого тикера\n",
    "for ticker, data in integrated_data.items():\n",
    "    print(f\"\\nГрафик для {ticker}:\")\n",
    "    plot_price_vs_news_sentiment(data, ticker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Анализ корреляций\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_correlations(ticker_data):\n",
    "    \"\"\"Вычисляет корреляции между новостным фоном и доходностями\"\"\"\n",
    "    # Убираем первую строку где доходность NaN\n",
    "    data_clean = ticker_data.dropna(subset=['return'])\n",
    "    \n",
    "    if len(data_clean) == 0:\n",
    "        return {}\n",
    "    \n",
    "    correlations = {}\n",
    "    \n",
    "    # Корреляция с текущими доходностями\n",
    "    correlations['current_return'] = data_clean['weighted_score_with_decay'].corr(data_clean['return'])\n",
    "    \n",
    "    # Корреляция с будущими доходностями (лаг 1 день)\n",
    "    if len(data_clean) > 1:\n",
    "        future_returns = data_clean['return'].shift(-1)\n",
    "        correlations['future_return_1d'] = data_clean['weighted_score_with_decay'].corr(future_returns)\n",
    "    \n",
    "    # Корреляция с будущими доходностями (лаг 3 дня)\n",
    "    if len(data_clean) > 3:\n",
    "        future_returns_3d = data_clean['return'].shift(-3)\n",
    "        correlations['future_return_3d'] = data_clean['weighted_score_with_decay'].corr(future_returns_3d)\n",
    "    \n",
    "    # Корреляция с будущими доходностями (лаг 7 дней)\n",
    "    if len(data_clean) > 7:\n",
    "        future_returns_7d = data_clean['return'].shift(-7)\n",
    "        correlations['future_return_7d'] = data_clean['weighted_score_with_decay'].corr(future_returns_7d)\n",
    "    \n",
    "    return correlations\n",
    "\n",
    "# Вычисляем корреляции для всех тикеров\n",
    "correlation_summary = []\n",
    "\n",
    "for ticker, data in integrated_data.items():\n",
    "    correlations = calculate_correlations(data)\n",
    "    \n",
    "    if correlations:\n",
    "        row = {'ticker': ticker}\n",
    "        row.update(correlations)\n",
    "        correlation_summary.append(row)\n",
    "        \n",
    "        print(f\"\\nКорреляции для {ticker}:\")\n",
    "        for key, value in correlations.items():\n",
    "            if not pd.isna(value):\n",
    "                print(f\"  {key}: {value:.4f}\")\n",
    "\n",
    "# Создаем сводную таблицу корреляций\n",
    "correlation_df = pd.DataFrame(correlation_summary)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"СВОДНАЯ ТАБЛИЦА КОРРЕЛЯЦИЙ\")\n",
    "print(\"=\"*60)\n",
    "print(correlation_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Визуализация корреляций\n",
    "if len(correlation_df) > 0:\n",
    "    # Подготавливаем данные для тепловой карты\n",
    "    corr_matrix = correlation_df.set_index('ticker')\n",
    "    \n",
    "    # Создаем тепловую карту\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0, \n",
    "                fmt='.3f', square=True, linewidths=0.5)\n",
    "    plt.title('Корреляции между новостным фоном и доходностями акций', \n",
    "              fontsize=14, fontweight='bold', pad=20)\n",
    "    plt.xlabel('Тип корреляции', fontsize=12)\n",
    "    plt.ylabel('Тикер', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Статистика по корреляциям\n",
    "    print(\"\\nСТАТИСТИКА КОРРЕЛЯЦИЙ:\")\n",
    "    print(\"-\" * 40)\n",
    "    for col in corr_matrix.columns:\n",
    "        values = corr_matrix[col].dropna()\n",
    "        if len(values) > 0:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"  Среднее: {values.mean():.4f}\")\n",
    "            print(f\"  Медиана: {values.median():.4f}\")\n",
    "            print(f\"  Стд. отклонение: {values.std():.4f}\")\n",
    "            print(f\"  Мин: {values.min():.4f}\")\n",
    "            print(f\"  Макс: {values.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Сохранение сводных результатов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем сводную таблицу корреляций\n",
    "correlation_df.to_csv(f\"{OUTPUT_PATH}correlation_summary.csv\", index=False)\n",
    "print(f\"Сводная таблица корреляций сохранена в {OUTPUT_PATH}correlation_summary.csv\")\n",
    "\n",
    "# Создаем сводку по всем обработанным файлам\n",
    "summary_info = []\n",
    "for ticker in tickers:\n",
    "    if ticker in integrated_data:\n",
    "        data = integrated_data[ticker]\n",
    "        summary_info.append({\n",
    "            'ticker': ticker,\n",
    "            'total_records': len(data),\n",
    "            'date_start': data['timestamp'].min(),\n",
    "            'date_end': data['timestamp'].max(),\n",
    "            'anomalies_count': data['anomaly'].sum(),\n",
    "            'news_days': (data['kernai_daily_avg'] != 0).sum(),\n",
    "            'avg_news_sentiment': data['weighted_score_with_decay'].mean()\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_info)\n",
    "summary_df.to_csv(f\"{OUTPUT_PATH}processing_summary.csv\", index=False)\n",
    "\n",
    "print(f\"\\nСводка по обработке сохранена в {OUTPUT_PATH}processing_summary.csv\")\n",
    "print(\"\\nОБЗОР ОБРАБОТАННЫХ ДАННЫХ:\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Выводы\n",
    "\n",
    "В данном ноутбуке была выполнена интеграция оценок новостей в временные ряды с аномалиями:\n",
    "\n",
    "1. **Добавлены 5 новых колонок:**\n",
    "   - `kernai_daily_avg` - средняя оценка за день от KerAI\n",
    "   - `gigachat_daily_avg` - средняя оценка за день от Гигачат \n",
    "   - `deepseek_daily_avg` - средняя оценка за день от DeepSeek\n",
    "   - `daily_headlines` - текст заголовков за день\n",
    "   - `weighted_score_with_decay` - взвешенная оценка с затухающим сигналом\n",
    "\n",
    "2. **Реализован затухающий сигнал:**\n",
    "   - Влияние новостей уменьшается в 2 раза каждый день\n",
    "   - За 3 дня влияние практически сводится к нулю\n",
    "\n",
    "3. **Выполнена визуализация** цен и новостного фона для всех тикеров\n",
    "\n",
    "4. **Рассчитаны корреляции** между новостным фоном и доходностями акций\n",
    "\n",
    "5. **Результаты сохранены** в папку `data/series_with_news/`\n",
    "\n",
    "Анализ корреляций позволяет оценить влияние новостного фона на динамику цен акций и выявить тикеры, наиболее чувствительные к новостным событиям.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (news)",
   "language": "python",
   "name": "news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
